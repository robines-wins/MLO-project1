\documentclass[10pt,conference,compsocconf]{IEEEtran}

\usepackage{hyperref}
\usepackage{graphicx}	% For figure environment


\begin{document}
\title{Finding the Higgs boson}

\author{
  Robin Solignac,
  \textit{Department of Computer Science, EPFL Lausanne, Switzerland}\\
  Adrian Pace,
  \textit{Department of Computer Science, EPFL Lausanne, Switzerland}\\
  Yannick Schaeffer,
  \textit{Department of Computer Science, EPFL Lausanne, Switzerland}
}

\maketitle

\begin{abstract}
 The Higgs boson is an elementary particle in the Standard Model of physics, which explains why other particles have mass. The Higgs boson decays rapidly into other particles and we observe the resulting signature to determine if it was a Higgs boson or some other particle / process. By using binary classification techniques, we can estimate the likelihood that a given signature was the result of a Higgs boson. This will be helpful to narrow the region where the Higgs is expected to be from the overwhelming number of noise in the dataset.
\end{abstract}

\section{Introduction}
We use binary classification techniques to estimate the likelihood that a Higgs boson is in a given snapshot of a particle collision, this data will then be used to distinguish the interesting samples that have a high probability to contain a Higgs boson from the samples where it's very likely to be only background information, mostly in the form of already known processes.

We have tried various machine learning approaches to obtain the final weights and discuss them in section II.

In section III we compare how effective each approach we tried were, and discuss the methodology we used to determine the best choice of parameters.

We then discuss the strength and weaknesses of our approach and conclude this paper with the implication of our method.

\section{Models and Methods}
\subsection{Background, Basic methods}
%\subsubsection{Basic methods}
\subsubsection{general\_gradient\_descent}
the general gradient descent is a function who performs the gradient descent algorithm for \emph{any} gradient and loss formula. It's simply a loop of a certain length, at each iteration we compute the gradient and move a little bit our weight vector along this gradient. Its arguments are:
\paragraph{y,tx,w} namely, the output vector, the input feature matrix and
the inital weight vector from where we'll start the gradient descent
loop
\paragraph{max\_iter} the number of gradient descent iteration we'll do
\paragraph{gamma} the coefficient by which will add the gradient to our current
vector \(w^{t+1}=w^{t}+gamma*\nabla L(w^{t})\)
\paragraph{grad\_function} the function who will return the gradient at point
w given (y,tx,w)
\paragraph{cost\_function} the function who will return the cost at w given
(y,tx,w)


\subsubsection{general\_stochastic\_gradient\_descent}
is the same as the same spirit as the function general\_gradient\_descent
but it perform a stochastic gradient descend loop, this mean only compute correction on part of the sample. all argument are the same as above, with same meaning execept grad\_function is gone and we have two new ones.

\paragraph{batch\_size} the size of the subset on which we'll compute the
``partial gradient'' (the g correction term in the course notes)
\paragraph{stock\_grad\_function} the function who will compute the stochastic
gradient given a subset of y, a corresponding subset of tx and the
current weigth vector w.
\\selection of the subset is done inside the function with
the help of the batch\_iter function from the labs exercises


\subsubsection{least\_squares\_GD}
perform a gradient descent with cost function MSE and gradient function the gradient of the MSE function. this is the definition of least square GD. argument have same meaning as their homonyms described above

\subsubsection{least\_squares\_SGD}
perform a stochastic gradient descent with cost function MSE and gradient function the gradient of the MSE function. this is the definition of least square GD. argument have same meaning as their homonyms described above

\subsubsection{least\_squares}
solve directly the least square problem using linear algebra, we use numpy linear system solve function to solve the system presented in the course this way as a better accuracy and less rounding error than if we directly use the solution formula who need to compute an inverse

\subsubsection{ridge regression}

same as the one before expect that we introduce a regularization term in the equation. again we solve the linear system derived in course.

\subsubsection{logistic regression}

perform logistic regression which is a gradient descent with special cost function and associate gradient function. that what we do we call general\_gradient\_descent with the new cost and gradient function 

\subsubsection{reg logistic regression}
same as the previous one but with a regularization term in the cost function. (and so also in gradient function). As general\_gradient\_descent only call the cost and grad function with the argument y,tx and w we need to fix the lambda\_ parameter before passing the cost and gradient function to general\_gradient\_descent. this is done using the partial function of python

\subsection{Parameters}
There are multiple parameters in the project which can be tuned. 
\paragraph{Gamma} is the parameter describing the step of the gradient descent and of the stochastic gradient descent. At each iteration, the new weight will be calculated by adding to the current weights the derivative of the weights times gamma. It mustn’t be too high or we will go in the direction of the good weights but miss them and go further. And at each iteration, we will further ourselves from the weights. If it is too small, we will eventually reach the best weights, but it will take a lot of time. Thus, we searched iteratively for the smallest gamma for which we got a reasonable computing time.
Least square GD and Least square SGD : gamma=0.000003.
Logistic regression and regularized logistic regression : gamma=0.00003
\paragraph{Max iterations} this parameter describes how many steps will we take before stopping the algorithm. The higher, the more finely tuned the weights will get, and the smaller, the shorter the algorithme will be. 
Least square GD and Least square SGD : max iterations=1000.
Logistic regression and regularized logistic regression :  max iterations=1000
\paragraph{Lambda} is the parameter used to avoid overfitting. For this, we did a cross-validation (with 4 folds) and check the test error. We tried it for 30 different lambdas and we saw that there was no lambdas for which the test error was extremely different from the train error. We can then conclude that we don’t have overfitting. This is also because we use linear models. So lambda=0
\paragraph{Which function we will use to estimate the weights} We decided to use the least square method. We chose this before the least square GD and SGD since it yielded a better mse (0.3423 against 0.3616). We chose Least square over logistic regression, since when we applied the weights to the input data, classified it, and compared it to the original y, we saw that Least square was more effective than logistic regression (64695 misclassifications against 68921).
\paragraph{tX itself} we've tried different preproceesing on the feature before performing machine learning on them. We tried to normalize outliers (feature outside of its 95\%  interval for each feature. TO COMPLETE\\
we also tried to rescale the data. This mean that all feature will be in mapped \( [0,1] \) but with conservation of the relative distance between them. for each feature set we took the max an min and define new value x' from old value x by = \( x' = \frac{x-\min}{\max - \min }\). this is know to improve convergence and efficacy of gradient descend algorithm. But even with this the direct least square computation was still the better way.
\subsection{Final implementation}

TODO: When finalized describe here the final implementation with the reasoning behind it.

\section{Results}
TODO: Present our results.

\section{Discussion}
TODO: Explain our results strength and weaknesses.
eg: did not consider ridge with a polynom

\section{Summary}
TODO: Conclusion on how tis will be helpfull in this research area.

\bibliographystyle{IEEEtran}
\bibliography{literature}

\end{document}
