{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_outliers(tX,mean_x,std_x):\n",
    "    tX2=tX.copy()\n",
    "    for sample in range(tX.shape[0]):\n",
    "        for dim in range(tX.shape[1]):\n",
    "            if(tX2[sample,dim]>mean_x[dim]+2*std_x[dim]):\n",
    "                tX2[sample,dim]=mean_x[dim]\n",
    "            if(tX2[sample,dim]<mean_x[dim]-2*std_x[dim]):\n",
    "                tX2[sample,dim]=mean_x[dim]\n",
    "            if(tX2[sample,dim]==-999):\n",
    "                tX2[sample,dim]=0\n",
    "    return tX2\n",
    "def modify_y(y):\n",
    "    y2 = y.copy()\n",
    "    y2[np.where(y==-1)] = 0\n",
    "    return y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 138.47    51.655   97.827 ...,    1.24    -2.475  113.497]\n",
      " [ 160.937   68.768  103.235 ..., -999.    -999.      46.226]\n",
      " [-999.     162.172  125.953 ..., -999.    -999.      44.251]\n",
      " ..., \n",
      " [ 105.457   60.526   75.839 ..., -999.    -999.      41.992]\n",
      " [  94.951   19.362   68.812 ..., -999.    -999.       0.   ]\n",
      " [-999.      72.756   70.831 ..., -999.    -999.       0.   ]]\n"
     ]
    }
   ],
   "source": [
    "mean_x = np.mean(tX, axis=0)\n",
    "std_x = np.std(tX, axis=0)\n",
    "tX_norm=remove_outliers(tX,mean_x,std_x)\n",
    "y0=modify_y(y)\n",
    "print(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., -1., -1., ...,  1., -1., -1.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement ML functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(y, tx, w):\n",
    "    return compute_cost_MSE(y,tx,w)\n",
    "    \n",
    "def compute_cost_MSE(y,tx,w):\n",
    "    e=y-(tx @ w)\n",
    "    return (1/(2*y.shape[0]))*(e.T @ e)\n",
    "def compute_cost_MAE(y,tx,w):\n",
    "    e=y-(tx @ w)\n",
    "    return (1/y.shape[0])*np.absolute(e).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_gradient_MSE(y,tx,w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    e=y-(tx @ w)\n",
    "    return -1/y.shape[0]*(tx.T @ e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def general_gradient_descent(y, tx, initial_w, max_iters, gamma, grad_function, cost_function):\n",
    "    \"\"\"Gradient descent algorithm who work with arbitrary gradient and cost function\n",
    "    grad and cost function should take y,tw and w as parameter and return resÃªctivly the gradient vector and the scalar error\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w.ravel()]\n",
    "    losses = []\n",
    "    w = initial_w.ravel()\n",
    "    for n_iter in range(max_iters):\n",
    "        #compute gradient and loss\n",
    "        gradient=grad_function(y,tx,w)\n",
    "        loss=cost_function(y,tx,w)\n",
    "        if n_iter % 1 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=n_iter, l=loss))\n",
    "        #update w by gradient\n",
    "        w=w-gamma*gradient\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "        #print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "        #     bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses[-1], ws[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def general_stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma, stock_grad_function, cost_function):\n",
    "    \"\"\"Gradient descent algorithm who work with arbitrary gradient and cost function\n",
    "    grad and cost function should take y,tw and w as parameter and return respectivly the gradient vector and the scalar error\"\"\"\n",
    "    \n",
    "    # implement stochastic gradient descent.\n",
    "    ws = [initial_w.ravel()]\n",
    "    losses = []\n",
    "    w = initial_w.ravel()\n",
    "    \n",
    "    minibatchs = batch_iter(y, tx, batch_size, num_batches=math.floor(y.shape[0]/batch_size))\n",
    "    for n_iter in range(0,max_epochs):\n",
    "        \n",
    "        # compute gradient and loss\n",
    "        minibatch=minibatchs.__next__()\n",
    "        gradient=stock_grad_function(minibatch[0],minibatch[1],w)\n",
    "        loss=cost_function(y,tx,w)\n",
    "        \n",
    "        # update w by gradient\n",
    "        w=w-gamma*gradient\n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses[-1], ws[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, gamma, max_iters):\n",
    "    return general_gradient_descent(y,tx,np.zeros((tx.shape[1], 1)).ravel(),max_iters,gamma,compute_gradient_MSE,compute_cost_MSE)\n",
    "\n",
    "#general_gradient_descent(y,tX,np.zeros((tX.shape[1], 1)),20,0.000003,compute_gradient_MSE,compute_cost_MSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, gamma, max_iters):\n",
    "    batch_size = y.shape[0]//2\n",
    "    return general_stochastic_gradient_descent(y,tx,np.zeros((tx.shape[1], 1)),batch_size,max_iters,gamma,compute_gradient_MSE,compute_cost_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    w = np.linalg.solve(tx.T.dot(tx),tx.T.dot(y)) #return best weight\n",
    "    return compute_cost_MSE(y, tx, w), w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lamb):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    \n",
    "    lambp = lamb*(2*tx.shape[0])\n",
    "    #return np.linalg.inv(tx.T.dot(tx)+lambp*np.eye(tx.shape[1])).dot(tx.T).dot(y)\n",
    "    w =  np.linalg.solve(tx.T.dot(tx)+lambp*np.eye(tx.shape[1]),tx.T.dot(y))\n",
    "    return compute_cost_MSE(y, tx, w), w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    z = np.exp(t)\n",
    "    return z/(1+z)\n",
    "\n",
    "def compute_loss_logistic(y, tx, w, lambda_=0):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    clip = np.clip(tx @ w, -700, 700)\n",
    "    if lambda_ == 0:\n",
    "        return (1/w.shape[0])*np.sum(np.log(1+np.exp(clip))-y*(tx @ w))\n",
    "    else:\n",
    "        return (1/w.shape[0])*np.sum(np.log(1+np.exp(clip))-y*(tx @ w)) + lambda_*np.sum(w*w) #or + lambda_*w.T*w\n",
    "    #return -np.sum(np.log(1+np.exp(tx @ w))-y*(tx @ w))\n",
    "    \n",
    "def compute_gradient_sigmoid(y, tx, w, lambda_=0):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    clip = np.clip(tx @ w, -700, 700)\n",
    "    if lambda_ == 0:\n",
    "        return (1/w.shape[0])*tx.T.dot(sigmoid(clip)-y)\n",
    "    else:\n",
    "        return (1/w.shape[0])*tx.T.dot(sigmoid(clip)-y) + lambda_*2*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, gamma ,max_iters):\n",
    "    print(\"fuck\")\n",
    "    ok = general_gradient_descent(y,tx,np.zeros((tx.shape[1], 1)),max_iters,gamma,compute_gradient_sigmoid,compute_loss_logistic)\n",
    "    return ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reg_logistic_regression(y, tx, lambda_ , gamma, max_iters):\n",
    "    gradf = partial(compute_gradient_sigmoid,lambda_ = lambda_)\n",
    "    costf = partial(compute_loss_logistic, lambda_ = lambda_)\n",
    "    return general_gradient_descent(y,tx,np.zeros((tx.shape[1], 1)),max_iters,gamma,gradf,costf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation(y, tx, function_to_test,k_fold, lambda_,seed,cost_function):\n",
    "    \"\"\"return the loss of train values, loss of test values and weights\"\"\"\n",
    "    k_indices=build_k_indices(y,k_fold,seed)\n",
    "    loss_tr=[]\n",
    "    loss_te=[]\n",
    "    weights=[]\n",
    "    for k in range(k_indices.shape[0]):\n",
    "        # get k'th subgroup in test, others in train:\n",
    "        train_indices=k_indices[[i for i in range(len(k_indices)) if i != k]]\n",
    "        train_tx,train_y=tx[np.ravel(train_indices)],y[np.ravel(train_indices)]\n",
    "        test_tx,test_y=tx[k_indices[k]],y[k_indices[k]]\n",
    "\n",
    "        loss_tr_k, weight_k = function_to_test(train_y, train_tx,lambda_)\n",
    "        loss_tr.append(loss_tr_k)\n",
    "        weights.append(weight_k)\n",
    "        loss_te.append(cost_function(test_y,test_tx,weight_k))\n",
    "    return np.mean(loss_tr), np.mean(loss_te),np.mean(weights,axis=0)\n",
    "\n",
    "def finding_lambda(y, tx, function_to_test,k_fold,seed,lambdas,cost_function):\n",
    "    \n",
    "    # define lists to store the loss of training data and test data\n",
    "    loss_tr = []\n",
    "    loss_te = []\n",
    "    for lambda_ in lambdas:\n",
    "        print(\"lambda : \", lambda_)\n",
    "        \n",
    "        loss_tr_lamb,loss_te_lamb,weight_lamb=cross_validation(y,tx,function_to_test,k_fold,lambda_,seed,cost_function)\n",
    "        loss_tr.append(loss_tr_lamb)\n",
    "        loss_te.append(loss_te_lamb)\n",
    "    return loss_tr,loss_te\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reg_logistic_regression_with_mse(y, tx, lambda_ , gamma, max_iters):\n",
    "    gradf = partial(compute_gradient_sigmoid,lambda_ = lambda_)\n",
    "    return general_gradient_descent(y,tx,np.zeros((tx.shape[1], 1)),max_iters,gamma,gradf,compute_cost)\n",
    "def logistic_regression_with_mse(y, tx, gamma ,max_iters):\n",
    "    ok = general_gradient_descent(y,tx,np.zeros((tx.shape[1], 1)),max_iters,gamma,compute_gradient_sigmoid,compute_cost)\n",
    "    return ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding a good lambda for ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEdCAYAAAAvj0GNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bbce004be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "initial_w=np.zeros(tX.shape[1])\n",
    "function_to_test=ridge_regression\n",
    "k_fold=4\n",
    "lambdas = np.logspace(-4, 2, 30)\n",
    "loss_tr,loss_te=finding_lambda(y,tX_norm,function_to_test,k_fold,1,lambdas,compute_cost)\n",
    "from helpers import *\n",
    "cross_validation_visualization(lambdas, loss_tr, loss_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.342334045238817,\n",
       " 0.34233404523881727,\n",
       " array([  2.01408328e-04,  -9.37679313e-03,   3.08279700e-03,\n",
       "         -5.53801577e-04,   1.56763588e-02,  -3.20654044e-04,\n",
       "         -2.34149841e-02,  -1.79819855e-02,  -3.84894764e-03,\n",
       "         -4.68513987e-04,  -1.97319796e-01,   1.03865892e-01,\n",
       "          3.42139596e-01,   5.66352463e-03,  -4.08214643e-05,\n",
       "         -1.15707948e-03,   2.74322661e-03,  -1.33201704e-03,\n",
       "          1.02510089e-03,   7.22615826e-04,   3.49079745e-04,\n",
       "         -1.74267650e-04,  -1.66379987e-02,  -4.26939012e-04,\n",
       "         -2.96151899e-05,   9.58107528e-05,  -5.92882391e-04,\n",
       "          1.17377665e-03,  -1.22375626e-03,   5.91560308e-04]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_tr,loss_te,w=cross_validation(y, tX_norm, ridge_regression,4, 0,1,compute_cost)\n",
    "loss_tr,loss_te,w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Finding a good lambda for penalized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda :  0.0001\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.21827510302727585\n",
      "Current iteration=3, the loss=0.23999712053328412\n",
      "Current iteration=4, the loss=0.2600640931943646\n",
      "Current iteration=5, the loss=0.2782968472806928\n",
      "Current iteration=6, the loss=0.29466670582839893\n",
      "Current iteration=7, the loss=0.30923558831073533\n",
      "Current iteration=8, the loss=0.32211663175885624\n",
      "Current iteration=9, the loss=0.33344871025544653\n",
      "Current iteration=10, the loss=0.3433802558318946\n",
      "Current iteration=11, the loss=0.3520592516877934\n",
      "Current iteration=12, the loss=0.35962729051657466\n",
      "Current iteration=13, the loss=0.3662162871080823\n",
      "Current iteration=14, the loss=0.37194690379863093\n",
      "Current iteration=15, the loss=0.3769280621453965\n",
      "Current iteration=16, the loss=0.38125712497335373\n",
      "Current iteration=17, the loss=0.3850204740663238\n",
      "Current iteration=18, the loss=0.3882943033262864\n",
      "Current iteration=19, the loss=0.3911455106035119\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.21815371064425354\n",
      "Current iteration=3, the loss=0.24036340906922746\n",
      "Current iteration=4, the loss=0.26091545703310814\n",
      "Current iteration=5, the loss=0.279613722254689\n",
      "Current iteration=6, the loss=0.2964200873428892\n",
      "Current iteration=7, the loss=0.31139185792334995\n",
      "Current iteration=8, the loss=0.3246406040624022\n",
      "Current iteration=9, the loss=0.33630551363881317\n",
      "Current iteration=10, the loss=0.3465364534337546\n",
      "Current iteration=11, the loss=0.3554834696781814\n",
      "Current iteration=12, the loss=0.36329052895714653\n",
      "Current iteration=13, the loss=0.3700920284162139\n",
      "Current iteration=14, the loss=0.3760110944188469\n",
      "Current iteration=15, the loss=0.3811590171730916\n",
      "Current iteration=16, the loss=0.3856353884452388\n",
      "Current iteration=17, the loss=0.389528656360607\n",
      "Current iteration=18, the loss=0.39291690961871967\n",
      "Current iteration=19, the loss=0.3958687693177627\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.21807149621984512\n",
      "Current iteration=3, the loss=0.23997419201892203\n",
      "Current iteration=4, the loss=0.2602238811281652\n",
      "Current iteration=5, the loss=0.2786350593918364\n",
      "Current iteration=6, the loss=0.2951751172051331\n",
      "Current iteration=7, the loss=0.3099036815262742\n",
      "Current iteration=8, the loss=0.32293271059819384\n",
      "Current iteration=9, the loss=0.334400649274548\n",
      "Current iteration=10, the loss=0.3444559979353593\n",
      "Current iteration=11, the loss=0.3532471310063832\n",
      "Current iteration=12, the loss=0.3609162339395952\n",
      "Current iteration=13, the loss=0.36759593151085285\n",
      "Current iteration=14, the loss=0.3734076548592372\n",
      "Current iteration=15, the loss=0.37846111298244883\n",
      "Current iteration=16, the loss=0.3828544475226488\n",
      "Current iteration=17, the loss=0.3866747923925919\n",
      "Current iteration=18, the loss=0.3899990554330756\n",
      "Current iteration=19, the loss=0.3928948034233681\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.21808122468556435\n",
      "Current iteration=3, the loss=0.24016392563413186\n",
      "Current iteration=4, the loss=0.2605868380275188\n",
      "Current iteration=5, the loss=0.27915886513142124\n",
      "Current iteration=6, the loss=0.29584491650616723\n",
      "Current iteration=7, the loss=0.3107039898917065\n",
      "Current iteration=8, the loss=0.3238484703122352\n",
      "Current iteration=9, the loss=0.3354177917571311\n",
      "Current iteration=10, the loss=0.3455617051940073\n",
      "Current iteration=11, the loss=0.35442991776381544\n",
      "Current iteration=12, the loss=0.3621659264357224\n",
      "Current iteration=13, the loss=0.368903590056654\n",
      "Current iteration=14, the loss=0.3747654689656999\n",
      "Current iteration=15, the loss=0.3798622864073412\n",
      "Current iteration=16, the loss=0.3842930833838917\n",
      "Current iteration=17, the loss=0.38814578401264677\n",
      "Current iteration=18, the loss=0.3914979858072225\n",
      "Current iteration=19, the loss=0.39441785451574546\n",
      "lambda :  0.000161026202756\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.21827510302727576\n",
      "Current iteration=3, the loss=0.23999712053328373\n",
      "Current iteration=4, the loss=0.26006409319436397\n",
      "Current iteration=5, the loss=0.27829684728069176\n",
      "Current iteration=6, the loss=0.2946667058283975\n",
      "Current iteration=7, the loss=0.30923558831073356\n",
      "Current iteration=8, the loss=0.32211663175885397\n",
      "Current iteration=9, the loss=0.3334487102554437\n",
      "Current iteration=10, the loss=0.34338025583189136\n",
      "Current iteration=11, the loss=0.35205925168778984\n",
      "Current iteration=12, the loss=0.35962729051657055\n",
      "Current iteration=13, the loss=0.3662162871080775\n",
      "Current iteration=14, the loss=0.37194690379862605\n",
      "Current iteration=15, the loss=0.37692806214539126\n",
      "Current iteration=16, the loss=0.3812571249733482\n",
      "Current iteration=17, the loss=0.3850204740663179\n",
      "Current iteration=18, the loss=0.3882943033262802\n",
      "Current iteration=19, the loss=0.3911455106035055\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.21815371064425343\n",
      "Current iteration=3, the loss=0.24036340906922715\n",
      "Current iteration=4, the loss=0.26091545703310753\n",
      "Current iteration=5, the loss=0.279613722254688\n",
      "Current iteration=6, the loss=0.29642008734288783\n",
      "Current iteration=7, the loss=0.311391857923348\n",
      "Current iteration=8, the loss=0.3246406040623998\n",
      "Current iteration=9, the loss=0.33630551363881034\n",
      "Current iteration=10, the loss=0.3465364534337513\n",
      "Current iteration=11, the loss=0.3554834696781776\n",
      "Current iteration=12, the loss=0.36329052895714237\n",
      "Current iteration=13, the loss=0.37009202841620925\n",
      "Current iteration=14, the loss=0.3760110944188419\n",
      "Current iteration=15, the loss=0.38115901717308615\n",
      "Current iteration=16, the loss=0.38563538844523326\n",
      "Current iteration=17, the loss=0.38952865636060097\n",
      "Current iteration=18, the loss=0.3929169096187133\n",
      "Current iteration=19, the loss=0.3958687693177561\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.218071496219845\n",
      "Current iteration=3, the loss=0.2399741920189218\n",
      "Current iteration=4, the loss=0.2602238811281645\n",
      "Current iteration=5, the loss=0.2786350593918354\n",
      "Current iteration=6, the loss=0.29517511720513173\n",
      "Current iteration=7, the loss=0.3099036815262723\n",
      "Current iteration=8, the loss=0.3229327105981914\n",
      "Current iteration=9, the loss=0.3344006492745452\n",
      "Current iteration=10, the loss=0.34445599793535614\n",
      "Current iteration=11, the loss=0.3532471310063796\n",
      "Current iteration=12, the loss=0.3609162339395912\n",
      "Current iteration=13, the loss=0.3675959315108482\n",
      "Current iteration=14, the loss=0.3734076548592323\n",
      "Current iteration=15, the loss=0.37846111298244356\n",
      "Current iteration=16, the loss=0.38285444752264336\n",
      "Current iteration=17, the loss=0.3866747923925859\n",
      "Current iteration=18, the loss=0.3899990554330696\n",
      "Current iteration=19, the loss=0.3928948034233616\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.21808122468556418\n",
      "Current iteration=3, the loss=0.24016392563413155\n",
      "Current iteration=4, the loss=0.2605868380275182\n",
      "Current iteration=5, the loss=0.2791588651314203\n",
      "Current iteration=6, the loss=0.29584491650616573\n",
      "Current iteration=7, the loss=0.31070398989170467\n",
      "Current iteration=8, the loss=0.3238484703122329\n",
      "Current iteration=9, the loss=0.3354177917571283\n",
      "Current iteration=10, the loss=0.3455617051940041\n",
      "Current iteration=11, the loss=0.3544299177638116\n",
      "Current iteration=12, the loss=0.3621659264357183\n",
      "Current iteration=13, the loss=0.3689035900566494\n",
      "Current iteration=14, the loss=0.37476546896569485\n",
      "Current iteration=15, the loss=0.37986228640733577\n",
      "Current iteration=16, the loss=0.3842930833838857\n",
      "Current iteration=17, the loss=0.3881457840126408\n",
      "Current iteration=18, the loss=0.3914979858072161\n",
      "Current iteration=19, the loss=0.394417854515739\n",
      "lambda :  0.00025929437974\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.21827510302727562\n",
      "Current iteration=3, the loss=0.23999712053328326\n",
      "Current iteration=4, the loss=0.260064093194363\n",
      "Current iteration=5, the loss=0.2782968472806901\n",
      "Current iteration=6, the loss=0.2946667058283954\n",
      "Current iteration=7, the loss=0.3092355883107306\n",
      "Current iteration=8, the loss=0.3221166317588503\n",
      "Current iteration=9, the loss=0.3334487102554392\n",
      "Current iteration=10, the loss=0.34338025583188625\n",
      "Current iteration=11, the loss=0.352059251687784\n",
      "Current iteration=12, the loss=0.359627290516564\n",
      "Current iteration=13, the loss=0.3662162871080703\n",
      "Current iteration=14, the loss=0.3719469037986183\n",
      "Current iteration=15, the loss=0.3769280621453828\n",
      "Current iteration=16, the loss=0.3812571249733392\n",
      "Current iteration=17, the loss=0.38502047406630835\n",
      "Current iteration=18, the loss=0.38829430332627024\n",
      "Current iteration=19, the loss=0.3911455106034953\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.21815371064425326\n",
      "Current iteration=3, the loss=0.24036340906922665\n",
      "Current iteration=4, the loss=0.26091545703310653\n",
      "Current iteration=5, the loss=0.27961372225468645\n",
      "Current iteration=6, the loss=0.29642008734288566\n",
      "Current iteration=7, the loss=0.311391857923345\n",
      "Current iteration=8, the loss=0.32464060406239614\n",
      "Current iteration=9, the loss=0.3363055136388058\n",
      "Current iteration=10, the loss=0.346536453433746\n",
      "Current iteration=11, the loss=0.3554834696781716\n",
      "Current iteration=12, the loss=0.36329052895713543\n",
      "Current iteration=13, the loss=0.3700920284162018\n",
      "Current iteration=14, the loss=0.37601109441883385\n",
      "Current iteration=15, the loss=0.3811590171730776\n",
      "Current iteration=16, the loss=0.38563538844522394\n",
      "Current iteration=17, the loss=0.3895286563605913\n",
      "Current iteration=18, the loss=0.3929169096187029\n",
      "Current iteration=19, the loss=0.3958687693177454\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.2180714962198448\n",
      "Current iteration=3, the loss=0.23997419201892126\n",
      "Current iteration=4, the loss=0.2602238811281635\n",
      "Current iteration=5, the loss=0.27863505939183386\n",
      "Current iteration=6, the loss=0.29517511720512957\n",
      "Current iteration=7, the loss=0.3099036815262693\n",
      "Current iteration=8, the loss=0.32293271059818796\n",
      "Current iteration=9, the loss=0.33440064927454083\n",
      "Current iteration=10, the loss=0.3444559979353508\n",
      "Current iteration=11, the loss=0.35324713100637367\n",
      "Current iteration=12, the loss=0.36091623393958444\n",
      "Current iteration=13, the loss=0.36759593151084075\n",
      "Current iteration=14, the loss=0.3734076548592242\n",
      "Current iteration=15, the loss=0.378461112982435\n",
      "Current iteration=16, the loss=0.3828544475226342\n",
      "Current iteration=17, the loss=0.3866747923925761\n",
      "Current iteration=18, the loss=0.3899990554330595\n",
      "Current iteration=19, the loss=0.39289480342335054\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.21808122468556404\n",
      "Current iteration=3, the loss=0.24016392563413105\n",
      "Current iteration=4, the loss=0.26058683802751725\n",
      "Current iteration=5, the loss=0.2791588651314187\n",
      "Current iteration=6, the loss=0.2958449165061635\n",
      "Current iteration=7, the loss=0.3107039898917017\n",
      "Current iteration=8, the loss=0.3238484703122291\n",
      "Current iteration=9, the loss=0.3354177917571238\n",
      "Current iteration=10, the loss=0.3455617051939988\n",
      "Current iteration=11, the loss=0.35442991776380556\n",
      "Current iteration=12, the loss=0.3621659264357116\n",
      "Current iteration=13, the loss=0.368903590056642\n",
      "Current iteration=14, the loss=0.37476546896568685\n",
      "Current iteration=15, the loss=0.37986228640732717\n",
      "Current iteration=16, the loss=0.3842930833838767\n",
      "Current iteration=17, the loss=0.38814578401263106\n",
      "Current iteration=18, the loss=0.3914979858072059\n",
      "Current iteration=19, the loss=0.3944178545157284\n",
      "lambda :  0.000417531893656\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.2182751030272754\n",
      "Current iteration=3, the loss=0.23999712053328243\n",
      "Current iteration=4, the loss=0.26006409319436147\n",
      "Current iteration=5, the loss=0.2782968472806876\n",
      "Current iteration=6, the loss=0.29466670582839183\n",
      "Current iteration=7, the loss=0.30923558831072584\n",
      "Current iteration=8, the loss=0.3221166317588444\n",
      "Current iteration=9, the loss=0.333448710255432\n",
      "Current iteration=10, the loss=0.34338025583187787\n",
      "Current iteration=11, the loss=0.3520592516877745\n",
      "Current iteration=12, the loss=0.3596272905165533\n",
      "Current iteration=13, the loss=0.36621628710805876\n",
      "Current iteration=14, the loss=0.37194690379860557\n",
      "Current iteration=15, the loss=0.3769280621453693\n",
      "Current iteration=16, the loss=0.3812571249733247\n",
      "Current iteration=17, the loss=0.38502047406629303\n",
      "Current iteration=18, the loss=0.3882943033262541\n",
      "Current iteration=19, the loss=0.3911455106034789\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.218153710644253\n",
      "Current iteration=3, the loss=0.24036340906922582\n",
      "Current iteration=4, the loss=0.260915457033105\n",
      "Current iteration=5, the loss=0.2796137222546839\n",
      "Current iteration=6, the loss=0.296420087342882\n",
      "Current iteration=7, the loss=0.3113918579233402\n",
      "Current iteration=8, the loss=0.32464060406239\n",
      "Current iteration=9, the loss=0.3363055136387984\n",
      "Current iteration=10, the loss=0.34653645343373735\n",
      "Current iteration=11, the loss=0.3554834696781619\n",
      "Current iteration=12, the loss=0.3632905289571245\n",
      "Current iteration=13, the loss=0.3700920284161897\n",
      "Current iteration=14, the loss=0.376011094418821\n",
      "Current iteration=15, the loss=0.38115901717306355\n",
      "Current iteration=16, the loss=0.38563538844520906\n",
      "Current iteration=17, the loss=0.38952865636057554\n",
      "Current iteration=18, the loss=0.3929169096186864\n",
      "Current iteration=19, the loss=0.3958687693177282\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.21807149621984456\n",
      "Current iteration=3, the loss=0.23997419201892042\n",
      "Current iteration=4, the loss=0.2602238811281619\n",
      "Current iteration=5, the loss=0.2786350593918313\n",
      "Current iteration=6, the loss=0.2951751172051259\n",
      "Current iteration=7, the loss=0.3099036815262644\n",
      "Current iteration=8, the loss=0.32293271059818196\n",
      "Current iteration=9, the loss=0.33440064927453356\n",
      "Current iteration=10, the loss=0.3444559979353426\n",
      "Current iteration=11, the loss=0.3532471310063641\n",
      "Current iteration=12, the loss=0.36091623393957384\n",
      "Current iteration=13, the loss=0.36759593151082887\n",
      "Current iteration=14, the loss=0.37340765485921157\n",
      "Current iteration=15, the loss=0.3784611129824209\n",
      "Current iteration=16, the loss=0.3828544475226198\n",
      "Current iteration=17, the loss=0.38667479239256064\n",
      "Current iteration=18, the loss=0.38999905543304336\n",
      "Current iteration=19, the loss=0.3928948034233336\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.21808122468556376\n",
      "Current iteration=3, the loss=0.24016392563413022\n",
      "Current iteration=4, the loss=0.26058683802751564\n",
      "Current iteration=5, the loss=0.2791588651314161\n",
      "Current iteration=6, the loss=0.29584491650615985\n",
      "Current iteration=7, the loss=0.31070398989169684\n",
      "Current iteration=8, the loss=0.3238484703122231\n",
      "Current iteration=9, the loss=0.33541779175711656\n",
      "Current iteration=10, the loss=0.34556170519399043\n",
      "Current iteration=11, the loss=0.3544299177637958\n",
      "Current iteration=12, the loss=0.3621659264357006\n",
      "Current iteration=13, the loss=0.36890359005662987\n",
      "Current iteration=14, the loss=0.37476546896567353\n",
      "Current iteration=15, the loss=0.37986228640731334\n",
      "Current iteration=16, the loss=0.3842930833838616\n",
      "Current iteration=17, the loss=0.3881457840126154\n",
      "Current iteration=18, the loss=0.39149798580718953\n",
      "Current iteration=19, the loss=0.39441785451571104\n",
      "lambda :  0.00067233575365\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.21827510302727493\n",
      "Current iteration=3, the loss=0.23999712053328115\n",
      "Current iteration=4, the loss=0.2600640931943589\n",
      "Current iteration=5, the loss=0.27829684728068366\n",
      "Current iteration=6, the loss=0.29466670582838606\n",
      "Current iteration=7, the loss=0.3092355883107182\n",
      "Current iteration=8, the loss=0.3221166317588347\n",
      "Current iteration=9, the loss=0.33344871025542067\n",
      "Current iteration=10, the loss=0.3433802558318643\n",
      "Current iteration=11, the loss=0.3520592516877591\n",
      "Current iteration=12, the loss=0.35962729051653625\n",
      "Current iteration=13, the loss=0.36621628710803966\n",
      "Current iteration=14, the loss=0.37194690379858525\n",
      "Current iteration=15, the loss=0.37692806214534713\n",
      "Current iteration=16, the loss=0.3812571249733015\n",
      "Current iteration=17, the loss=0.38502047406626816\n",
      "Current iteration=18, the loss=0.3882943033262281\n",
      "Current iteration=19, the loss=0.39114551060345226\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.21815371064425257\n",
      "Current iteration=3, the loss=0.24036340906922446\n",
      "Current iteration=4, the loss=0.2609154570331023\n",
      "Current iteration=5, the loss=0.27961372225467973\n",
      "Current iteration=6, the loss=0.296420087342876\n",
      "Current iteration=7, the loss=0.31139185792333235\n",
      "Current iteration=8, the loss=0.32464060406238\n",
      "Current iteration=9, the loss=0.3363055136387866\n",
      "Current iteration=10, the loss=0.34653645343372347\n",
      "Current iteration=11, the loss=0.35548346967814626\n",
      "Current iteration=12, the loss=0.36329052895710706\n",
      "Current iteration=13, the loss=0.3700920284161702\n",
      "Current iteration=14, the loss=0.3760110944187998\n",
      "Current iteration=15, the loss=0.3811590171730409\n",
      "Current iteration=16, the loss=0.3856353884451849\n",
      "Current iteration=17, the loss=0.38952865636055\n",
      "Current iteration=18, the loss=0.3929169096186596\n",
      "Current iteration=19, the loss=0.3958687693177003\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.21807149621984415\n",
      "Current iteration=3, the loss=0.2399741920189191\n",
      "Current iteration=4, the loss=0.26022388112815936\n",
      "Current iteration=5, the loss=0.2786350593918272\n",
      "Current iteration=6, the loss=0.2951751172051202\n",
      "Current iteration=7, the loss=0.3099036815262568\n",
      "Current iteration=8, the loss=0.3229327105981724\n",
      "Current iteration=9, the loss=0.33440064927452173\n",
      "Current iteration=10, the loss=0.34445599793532894\n",
      "Current iteration=11, the loss=0.35324713100634875\n",
      "Current iteration=12, the loss=0.3609162339395564\n",
      "Current iteration=13, the loss=0.3675959315108099\n",
      "Current iteration=14, the loss=0.3734076548591909\n",
      "Current iteration=15, the loss=0.37846111298239854\n",
      "Current iteration=16, the loss=0.38285444752259606\n",
      "Current iteration=17, the loss=0.3866747923925359\n",
      "Current iteration=18, the loss=0.3899990554330173\n",
      "Current iteration=19, the loss=0.3928948034233063\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.2180812246855633\n",
      "Current iteration=3, the loss=0.24016392563412892\n",
      "Current iteration=4, the loss=0.26058683802751303\n",
      "Current iteration=5, the loss=0.27915886513141197\n",
      "Current iteration=6, the loss=0.2958449165061542\n",
      "Current iteration=7, the loss=0.31070398989168924\n",
      "Current iteration=8, the loss=0.32384847031221364\n",
      "Current iteration=9, the loss=0.3354177917571048\n",
      "Current iteration=10, the loss=0.34556170519397683\n",
      "Current iteration=11, the loss=0.3544299177637799\n",
      "Current iteration=12, the loss=0.36216592643568324\n",
      "Current iteration=13, the loss=0.36890359005661055\n",
      "Current iteration=14, the loss=0.37476546896565266\n",
      "Current iteration=15, the loss=0.3798622864072909\n",
      "Current iteration=16, the loss=0.3842930833838379\n",
      "Current iteration=17, the loss=0.3881457840125905\n",
      "Current iteration=18, the loss=0.39149798580716333\n",
      "Current iteration=19, the loss=0.39441785451568284\n",
      "lambda :  0.00108263673387\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.21827510302727424\n",
      "Current iteration=3, the loss=0.23999712053327907\n",
      "Current iteration=4, the loss=0.2600640931943548\n",
      "Current iteration=5, the loss=0.27829684728067705\n",
      "Current iteration=6, the loss=0.29466670582837684\n",
      "Current iteration=7, the loss=0.3092355883107057\n",
      "Current iteration=8, the loss=0.3221166317588195\n",
      "Current iteration=9, the loss=0.3334487102554024\n",
      "Current iteration=10, the loss=0.34338025583184273\n",
      "Current iteration=11, the loss=0.3520592516877348\n",
      "Current iteration=12, the loss=0.35962729051650877\n",
      "Current iteration=13, the loss=0.36621628710800924\n",
      "Current iteration=14, the loss=0.37194690379855233\n",
      "Current iteration=15, the loss=0.37692806214531205\n",
      "Current iteration=16, the loss=0.38125712497326375\n",
      "Current iteration=17, the loss=0.38502047406622864\n",
      "Current iteration=18, the loss=0.3882943033261863\n",
      "Current iteration=19, the loss=0.39114551060340835\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.21815371064425187\n",
      "Current iteration=3, the loss=0.24036340906922232\n",
      "Current iteration=4, the loss=0.26091545703309815\n",
      "Current iteration=5, the loss=0.279613722254673\n",
      "Current iteration=6, the loss=0.29642008734286635\n",
      "Current iteration=7, the loss=0.3113918579233198\n",
      "Current iteration=8, the loss=0.32464060406236417\n",
      "Current iteration=9, the loss=0.33630551363876754\n",
      "Current iteration=10, the loss=0.3465364534337012\n",
      "Current iteration=11, the loss=0.3554834696781209\n",
      "Current iteration=12, the loss=0.36329052895707864\n",
      "Current iteration=13, the loss=0.3700920284161389\n",
      "Current iteration=14, the loss=0.3760110944187655\n",
      "Current iteration=15, the loss=0.3811590171730045\n",
      "Current iteration=16, the loss=0.3856353884451456\n",
      "Current iteration=17, the loss=0.38952865636050876\n",
      "Current iteration=18, the loss=0.3929169096186163\n",
      "Current iteration=19, the loss=0.39586876931765524\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.2180714962198434\n",
      "Current iteration=3, the loss=0.23997419201891707\n",
      "Current iteration=4, the loss=0.26022388112815525\n",
      "Current iteration=5, the loss=0.27863505939182065\n",
      "Current iteration=6, the loss=0.2951751172051111\n",
      "Current iteration=7, the loss=0.3099036815262444\n",
      "Current iteration=8, the loss=0.3229327105981567\n",
      "Current iteration=9, the loss=0.3344006492745031\n",
      "Current iteration=10, the loss=0.3444559979353072\n",
      "Current iteration=11, the loss=0.35324713100632343\n",
      "Current iteration=12, the loss=0.36091623393952854\n",
      "Current iteration=13, the loss=0.3675959315107794\n",
      "Current iteration=14, the loss=0.3734076548591576\n",
      "Current iteration=15, the loss=0.37846111298236285\n",
      "Current iteration=16, the loss=0.38285444752255804\n",
      "Current iteration=17, the loss=0.38667479239249614\n",
      "Current iteration=18, the loss=0.3899990554329754\n",
      "Current iteration=19, the loss=0.39289480342326255\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.21808122468556254\n",
      "Current iteration=3, the loss=0.24016392563412672\n",
      "Current iteration=4, the loss=0.2605868380275089\n",
      "Current iteration=5, the loss=0.27915886513140514\n",
      "Current iteration=6, the loss=0.29584491650614464\n",
      "Current iteration=7, the loss=0.3107039898916766\n",
      "Current iteration=8, the loss=0.32384847031219793\n",
      "Current iteration=9, the loss=0.3354177917570859\n",
      "Current iteration=10, the loss=0.3455617051939548\n",
      "Current iteration=11, the loss=0.354429917763755\n",
      "Current iteration=12, the loss=0.3621659264356553\n",
      "Current iteration=13, the loss=0.36890359005657974\n",
      "Current iteration=14, the loss=0.3747654689656192\n",
      "Current iteration=15, the loss=0.37986228640725467\n",
      "Current iteration=16, the loss=0.3842930833837993\n",
      "Current iteration=17, the loss=0.3881457840125493\n",
      "Current iteration=18, the loss=0.39149798580712036\n",
      "Current iteration=19, the loss=0.3944178545156382\n",
      "lambda :  0.0017433288222\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.21827510302727307\n",
      "Current iteration=3, the loss=0.23999712053327574\n",
      "Current iteration=4, the loss=0.2600640931943482\n",
      "Current iteration=5, the loss=0.2782968472806666\n",
      "Current iteration=6, the loss=0.29466670582836196\n",
      "Current iteration=7, the loss=0.309235588310686\n",
      "Current iteration=8, the loss=0.32211663175879485\n",
      "Current iteration=9, the loss=0.3334487102553726\n",
      "Current iteration=10, the loss=0.3433802558318079\n",
      "Current iteration=11, the loss=0.3520592516876952\n",
      "Current iteration=12, the loss=0.3596272905164645\n",
      "Current iteration=13, the loss=0.36621628710796045\n",
      "Current iteration=14, the loss=0.371946903798499\n",
      "Current iteration=15, the loss=0.37692806214525465\n",
      "Current iteration=16, the loss=0.3812571249732029\n",
      "Current iteration=17, the loss=0.38502047406616413\n",
      "Current iteration=18, the loss=0.3882943033261191\n",
      "Current iteration=19, the loss=0.39114551060333796\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.21815371064425068\n",
      "Current iteration=3, the loss=0.24036340906921877\n",
      "Current iteration=4, the loss=0.2609154570330915\n",
      "Current iteration=5, the loss=0.27961372225466224\n",
      "Current iteration=6, the loss=0.29642008734285086\n",
      "Current iteration=7, the loss=0.3113918579232993\n",
      "Current iteration=8, the loss=0.3246406040623385\n",
      "Current iteration=9, the loss=0.33630551363873673\n",
      "Current iteration=10, the loss=0.34653645343366607\n",
      "Current iteration=11, the loss=0.35548346967808003\n",
      "Current iteration=12, the loss=0.363290528957033\n",
      "Current iteration=13, the loss=0.37009202841608896\n",
      "Current iteration=14, the loss=0.3760110944187106\n",
      "Current iteration=15, the loss=0.3811590171729459\n",
      "Current iteration=16, the loss=0.3856353884450833\n",
      "Current iteration=17, the loss=0.3895286563604425\n",
      "Current iteration=18, the loss=0.3929169096185466\n",
      "Current iteration=19, the loss=0.3958687693175829\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.21807149621984223\n",
      "Current iteration=3, the loss=0.23997419201891373\n",
      "Current iteration=4, the loss=0.2602238811281486\n",
      "Current iteration=5, the loss=0.27863505939180994\n",
      "Current iteration=6, the loss=0.29517511720509587\n",
      "Current iteration=7, the loss=0.30990368152622416\n",
      "Current iteration=8, the loss=0.32293271059813156\n",
      "Current iteration=9, the loss=0.33440064927447294\n",
      "Current iteration=10, the loss=0.34445599793527193\n",
      "Current iteration=11, the loss=0.3532471310062832\n",
      "Current iteration=12, the loss=0.3609162339394841\n",
      "Current iteration=13, the loss=0.36759593151073044\n",
      "Current iteration=14, the loss=0.37340765485910404\n",
      "Current iteration=15, the loss=0.37846111298230556\n",
      "Current iteration=16, the loss=0.38285444752249637\n",
      "Current iteration=17, the loss=0.386674792392431\n",
      "Current iteration=18, the loss=0.389999055432907\n",
      "Current iteration=19, the loss=0.3928948034231912\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.21808122468556143\n",
      "Current iteration=3, the loss=0.2401639256341233\n",
      "Current iteration=4, the loss=0.26058683802750227\n",
      "Current iteration=5, the loss=0.27915886513139443\n",
      "Current iteration=6, the loss=0.2958449165061293\n",
      "Current iteration=7, the loss=0.3107039898916565\n",
      "Current iteration=8, the loss=0.3238484703121726\n",
      "Current iteration=9, the loss=0.33541779175705533\n",
      "Current iteration=10, the loss=0.3455617051939196\n",
      "Current iteration=11, the loss=0.3544299177637145\n",
      "Current iteration=12, the loss=0.36216592643560996\n",
      "Current iteration=13, the loss=0.36890359005653023\n",
      "Current iteration=14, the loss=0.374765468965565\n",
      "Current iteration=15, the loss=0.37986228640719677\n",
      "Current iteration=16, the loss=0.3842930833837369\n",
      "Current iteration=17, the loss=0.3881457840124839\n",
      "Current iteration=18, the loss=0.39149798580705175\n",
      "Current iteration=19, the loss=0.39441785451556727\n",
      "lambda :  0.00280721620394\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.21827510302727124\n",
      "Current iteration=3, the loss=0.23999712053327027\n",
      "Current iteration=4, the loss=0.2600640931943378\n",
      "Current iteration=5, the loss=0.27829684728064985\n",
      "Current iteration=6, the loss=0.29466670582833804\n",
      "Current iteration=7, the loss=0.309235588310654\n",
      "Current iteration=8, the loss=0.32211663175875455\n",
      "Current iteration=9, the loss=0.3334487102553244\n",
      "Current iteration=10, the loss=0.3433802558317521\n",
      "Current iteration=11, the loss=0.3520592516876316\n",
      "Current iteration=12, the loss=0.359627290516393\n",
      "Current iteration=13, the loss=0.3662162871078819\n",
      "Current iteration=14, the loss=0.3719469037984137\n",
      "Current iteration=15, the loss=0.37692806214516356\n",
      "Current iteration=16, the loss=0.38125712497310554\n",
      "Current iteration=17, the loss=0.385020474066061\n",
      "Current iteration=18, the loss=0.3882943033260109\n",
      "Current iteration=19, the loss=0.39114551060322517\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.21815371064424882\n",
      "Current iteration=3, the loss=0.24036340906921333\n",
      "Current iteration=4, the loss=0.2609154570330806\n",
      "Current iteration=5, the loss=0.27961372225464504\n",
      "Current iteration=6, the loss=0.2964200873428264\n",
      "Current iteration=7, the loss=0.31139185792326673\n",
      "Current iteration=8, the loss=0.3246406040622979\n",
      "Current iteration=9, the loss=0.3363055136386877\n",
      "Current iteration=10, the loss=0.3465364534336089\n",
      "Current iteration=11, the loss=0.35548346967801425\n",
      "Current iteration=12, the loss=0.36329052895695935\n",
      "Current iteration=13, the loss=0.3700920284160081\n",
      "Current iteration=14, the loss=0.37601109441862335\n",
      "Current iteration=15, the loss=0.38115901717285056\n",
      "Current iteration=16, the loss=0.38563538844498335\n",
      "Current iteration=17, the loss=0.3895286563603367\n",
      "Current iteration=18, the loss=0.39291690961843545\n",
      "Current iteration=19, the loss=0.39586876931746623\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.21807149621984037\n",
      "Current iteration=3, the loss=0.23997419201890818\n",
      "Current iteration=4, the loss=0.26022388112813805\n",
      "Current iteration=5, the loss=0.2786350593917928\n",
      "Current iteration=6, the loss=0.29517511720507156\n",
      "Current iteration=7, the loss=0.30990368152619224\n",
      "Current iteration=8, the loss=0.32293271059809137\n",
      "Current iteration=9, the loss=0.33440064927442426\n",
      "Current iteration=10, the loss=0.34445599793521536\n",
      "Current iteration=11, the loss=0.35324713100621913\n",
      "Current iteration=12, the loss=0.3609162339394116\n",
      "Current iteration=13, the loss=0.3675959315106509\n",
      "Current iteration=14, the loss=0.3734076548590176\n",
      "Current iteration=15, the loss=0.3784611129822131\n",
      "Current iteration=16, the loss=0.38285444752239733\n",
      "Current iteration=17, the loss=0.3866747923923254\n",
      "Current iteration=18, the loss=0.3899990554327975\n",
      "Current iteration=19, the loss=0.39289480342307703\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.21808122468555946\n",
      "Current iteration=3, the loss=0.24016392563411781\n",
      "Current iteration=4, the loss=0.2605868380274915\n",
      "Current iteration=5, the loss=0.27915886513137733\n",
      "Current iteration=6, the loss=0.2958449165061049\n",
      "Current iteration=7, the loss=0.31070398989162407\n",
      "Current iteration=8, the loss=0.32384847031213176\n",
      "Current iteration=9, the loss=0.3354177917570066\n",
      "Current iteration=10, the loss=0.3455617051938621\n",
      "Current iteration=11, the loss=0.3544299177636495\n",
      "Current iteration=12, the loss=0.36216592643553686\n",
      "Current iteration=13, the loss=0.3689035900564497\n",
      "Current iteration=14, the loss=0.3747654689654778\n",
      "Current iteration=15, the loss=0.379862286407103\n",
      "Current iteration=16, the loss=0.38429308338363727\n",
      "Current iteration=17, the loss=0.388145784012379\n",
      "Current iteration=18, the loss=0.39149798580694106\n",
      "Current iteration=19, the loss=0.39441785451545164\n",
      "lambda :  0.00452035365636\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.21827510302726824\n",
      "Current iteration=3, the loss=0.23999712053326142\n",
      "Current iteration=4, the loss=0.2600640931943206\n",
      "Current iteration=5, the loss=0.2782968472806224\n",
      "Current iteration=6, the loss=0.29466670582829896\n",
      "Current iteration=7, the loss=0.30923558831060266\n",
      "Current iteration=8, the loss=0.3221166317586904\n",
      "Current iteration=9, the loss=0.33344871025524736\n",
      "Current iteration=10, the loss=0.3433802558316615\n",
      "Current iteration=11, the loss=0.3520592516875289\n",
      "Current iteration=12, the loss=0.3596272905162783\n",
      "Current iteration=13, the loss=0.3662162871077562\n",
      "Current iteration=14, the loss=0.37194690379827633\n",
      "Current iteration=15, the loss=0.37692806214501556\n",
      "Current iteration=16, the loss=0.3812571249729481\n",
      "Current iteration=17, the loss=0.3850204740658949\n",
      "Current iteration=18, the loss=0.3882943033258366\n",
      "Current iteration=19, the loss=0.391145510603043\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.21815371064424585\n",
      "Current iteration=3, the loss=0.24036340906920442\n",
      "Current iteration=4, the loss=0.26091545703306285\n",
      "Current iteration=5, the loss=0.27961372225461695\n",
      "Current iteration=6, the loss=0.2964200873427862\n",
      "Current iteration=7, the loss=0.3113918579232138\n",
      "Current iteration=8, the loss=0.3246406040622317\n",
      "Current iteration=9, the loss=0.3363055136386081\n",
      "Current iteration=10, the loss=0.3465364534335159\n",
      "Current iteration=11, the loss=0.35548346967790867\n",
      "Current iteration=12, the loss=0.36329052895684094\n",
      "Current iteration=13, the loss=0.3700920284158777\n",
      "Current iteration=14, the loss=0.3760110944184818\n",
      "Current iteration=15, the loss=0.3811590171726984\n",
      "Current iteration=16, the loss=0.38563538844482154\n",
      "Current iteration=17, the loss=0.38952865636016476\n",
      "Current iteration=18, the loss=0.39291690961825526\n",
      "Current iteration=19, the loss=0.3958687693172786\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.21807149621983743\n",
      "Current iteration=3, the loss=0.2399741920188993\n",
      "Current iteration=4, the loss=0.2602238811281207\n",
      "Current iteration=5, the loss=0.2786350593917654\n",
      "Current iteration=6, the loss=0.2951751172050322\n",
      "Current iteration=7, the loss=0.3099036815261406\n",
      "Current iteration=8, the loss=0.32293271059802675\n",
      "Current iteration=9, the loss=0.3344006492743462\n",
      "Current iteration=10, the loss=0.344455997935124\n",
      "Current iteration=11, the loss=0.35324713100611516\n",
      "Current iteration=12, the loss=0.36091623393929506\n",
      "Current iteration=13, the loss=0.367595931510523\n",
      "Current iteration=14, the loss=0.37340765485887883\n",
      "Current iteration=15, the loss=0.37846111298206386\n",
      "Current iteration=16, the loss=0.3828544475222387\n",
      "Current iteration=17, the loss=0.38667479239215846\n",
      "Current iteration=18, the loss=0.38999905543262103\n",
      "Current iteration=19, the loss=0.39289480342289296\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.21808122468555635\n",
      "Current iteration=3, the loss=0.24016392563410882\n",
      "Current iteration=4, the loss=0.260586838027474\n",
      "Current iteration=5, the loss=0.27915886513134974\n",
      "Current iteration=6, the loss=0.29584491650606537\n",
      "Current iteration=7, the loss=0.3107039898915719\n",
      "Current iteration=8, the loss=0.32384847031206637\n",
      "Current iteration=9, the loss=0.3354177917569276\n",
      "Current iteration=10, the loss=0.3455617051937698\n",
      "Current iteration=11, the loss=0.35442991776354443\n",
      "Current iteration=12, the loss=0.36216592643541967\n",
      "Current iteration=13, the loss=0.36890359005632045\n",
      "Current iteration=14, the loss=0.37476546896533747\n",
      "Current iteration=15, the loss=0.3798622864069526\n",
      "Current iteration=16, the loss=0.3842930833834768\n",
      "Current iteration=17, the loss=0.38814578401220856\n",
      "Current iteration=18, the loss=0.39149798580676287\n",
      "Current iteration=19, the loss=0.3944178545152649\n",
      "lambda :  0.00727895384398\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.21827510302726336\n",
      "Current iteration=3, the loss=0.23999712053324726\n",
      "Current iteration=4, the loss=0.2600640931942932\n",
      "Current iteration=5, the loss=0.2782968472805783\n",
      "Current iteration=6, the loss=0.29466670582823684\n",
      "Current iteration=7, the loss=0.3092355883105203\n",
      "Current iteration=8, the loss=0.3221166317585872\n",
      "Current iteration=9, the loss=0.3334487102551231\n",
      "Current iteration=10, the loss=0.3433802558315166\n",
      "Current iteration=11, the loss=0.35205925168736313\n",
      "Current iteration=12, the loss=0.35962729051609355\n",
      "Current iteration=13, the loss=0.36621628710755194\n",
      "Current iteration=14, the loss=0.37194690379805523\n",
      "Current iteration=15, the loss=0.37692806214477886\n",
      "Current iteration=16, the loss=0.38125712497269426\n",
      "Current iteration=17, the loss=0.38502047406562784\n",
      "Current iteration=18, the loss=0.38829430332555587\n",
      "Current iteration=19, the loss=0.39114551060274977\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.21815371064424086\n",
      "Current iteration=3, the loss=0.2403634090691899\n",
      "Current iteration=4, the loss=0.2609154570330347\n",
      "Current iteration=5, the loss=0.2796137222545719\n",
      "Current iteration=6, the loss=0.2964200873427227\n",
      "Current iteration=7, the loss=0.31139185792312946\n",
      "Current iteration=8, the loss=0.3246406040621256\n",
      "Current iteration=9, the loss=0.33630551363848077\n",
      "Current iteration=10, the loss=0.34653645343336636\n",
      "Current iteration=11, the loss=0.35548346967773825\n",
      "Current iteration=12, the loss=0.36329052895665076\n",
      "Current iteration=13, the loss=0.3700920284156683\n",
      "Current iteration=14, the loss=0.37601109441825387\n",
      "Current iteration=15, the loss=0.3811590171724533\n",
      "Current iteration=16, the loss=0.3856353884445606\n",
      "Current iteration=17, the loss=0.38952865635988926\n",
      "Current iteration=18, the loss=0.39291690961796594\n",
      "Current iteration=19, the loss=0.3958687693169766\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.21807149621983263\n",
      "Current iteration=3, the loss=0.23997419201888504\n",
      "Current iteration=4, the loss=0.2602238811280931\n",
      "Current iteration=5, the loss=0.2786350593917212\n",
      "Current iteration=6, the loss=0.29517511720496925\n",
      "Current iteration=7, the loss=0.30990368152605674\n",
      "Current iteration=8, the loss=0.32293271059792217\n",
      "Current iteration=9, the loss=0.3344006492742204\n",
      "Current iteration=10, the loss=0.3444559979349778\n",
      "Current iteration=11, the loss=0.3532471310059477\n",
      "Current iteration=12, the loss=0.3609162339391083\n",
      "Current iteration=13, the loss=0.3675959315103167\n",
      "Current iteration=14, the loss=0.373407654858655\n",
      "Current iteration=15, the loss=0.3784611129818238\n",
      "Current iteration=16, the loss=0.3828544475219832\n",
      "Current iteration=17, the loss=0.3866747923918879\n",
      "Current iteration=18, the loss=0.38999905543233737\n",
      "Current iteration=19, the loss=0.39289480342259586\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.2180812246855516\n",
      "Current iteration=3, the loss=0.24016392563409436\n",
      "Current iteration=4, the loss=0.26058683802744576\n",
      "Current iteration=5, the loss=0.2791588651313048\n",
      "Current iteration=6, the loss=0.2958449165060018\n",
      "Current iteration=7, the loss=0.31070398989148784\n",
      "Current iteration=8, the loss=0.3238484703119613\n",
      "Current iteration=9, the loss=0.3354177917568007\n",
      "Current iteration=10, the loss=0.3455617051936216\n",
      "Current iteration=11, the loss=0.3544299177633754\n",
      "Current iteration=12, the loss=0.36216592643523077\n",
      "Current iteration=13, the loss=0.3689035900561127\n",
      "Current iteration=14, the loss=0.37476546896511226\n",
      "Current iteration=15, the loss=0.3798622864067098\n",
      "Current iteration=16, the loss=0.3842930833832187\n",
      "Current iteration=17, the loss=0.3881457840119353\n",
      "Current iteration=18, the loss=0.3914979858064765\n",
      "Current iteration=19, the loss=0.3944178545149655\n",
      "lambda :  0.0117210229753\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.21827510302725583\n",
      "Current iteration=3, the loss=0.23999712053322456\n",
      "Current iteration=4, the loss=0.26006409319424906\n",
      "Current iteration=5, the loss=0.278296847280508\n",
      "Current iteration=6, the loss=0.29466670582813614\n",
      "Current iteration=7, the loss=0.3092355883103877\n",
      "Current iteration=8, the loss=0.3221166317584208\n",
      "Current iteration=9, the loss=0.33344871025492273\n",
      "Current iteration=10, the loss=0.34338025583128284\n",
      "Current iteration=11, the loss=0.35205925168709645\n",
      "Current iteration=12, the loss=0.3596272905157951\n",
      "Current iteration=13, the loss=0.3662162871072243\n",
      "Current iteration=14, the loss=0.3719469037976993\n",
      "Current iteration=15, the loss=0.3769280621443953\n",
      "Current iteration=16, the loss=0.38125712497228736\n",
      "Current iteration=17, the loss=0.3850204740651972\n",
      "Current iteration=18, the loss=0.3882943033251038\n",
      "Current iteration=19, the loss=0.39114551060227787\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.21815371064423314\n",
      "Current iteration=3, the loss=0.2403634090691663\n",
      "Current iteration=4, the loss=0.26091545703298935\n",
      "Current iteration=5, the loss=0.27961372225449965\n",
      "Current iteration=6, the loss=0.29642008734261893\n",
      "Current iteration=7, the loss=0.3113918579229928\n",
      "Current iteration=8, the loss=0.3246406040619543\n",
      "Current iteration=9, the loss=0.3363055136382742\n",
      "Current iteration=10, the loss=0.3465364534331258\n",
      "Current iteration=11, the loss=0.35548346967746375\n",
      "Current iteration=12, the loss=0.36329052895634345\n",
      "Current iteration=13, the loss=0.37009202841533034\n",
      "Current iteration=14, the loss=0.37601109441788727\n",
      "Current iteration=15, the loss=0.3811590171720595\n",
      "Current iteration=16, the loss=0.38563538844414025\n",
      "Current iteration=17, the loss=0.38952865635944534\n",
      "Current iteration=18, the loss=0.3929169096174995\n",
      "Current iteration=19, the loss=0.3958687693164895\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.21807149621982477\n",
      "Current iteration=3, the loss=0.23997419201886194\n",
      "Current iteration=4, the loss=0.2602238811280483\n",
      "Current iteration=5, the loss=0.2786350593916502\n",
      "Current iteration=6, the loss=0.29517511720486767\n",
      "Current iteration=7, the loss=0.3099036815259226\n",
      "Current iteration=8, the loss=0.322932710597754\n",
      "Current iteration=9, the loss=0.33440064927401875\n",
      "Current iteration=10, the loss=0.34445599793474174\n",
      "Current iteration=11, the loss=0.3532471310056787\n",
      "Current iteration=12, the loss=0.36091623393880734\n",
      "Current iteration=13, the loss=0.36759593150998543\n",
      "Current iteration=14, the loss=0.3734076548582951\n",
      "Current iteration=15, the loss=0.3784611129814363\n",
      "Current iteration=16, the loss=0.3828544475215704\n",
      "Current iteration=17, the loss=0.38667479239145236\n",
      "Current iteration=18, the loss=0.3899990554318795\n",
      "Current iteration=19, the loss=0.39289480342211874\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.21808122468554386\n",
      "Current iteration=3, the loss=0.24016392563407116\n",
      "Current iteration=4, the loss=0.26058683802740107\n",
      "Current iteration=5, the loss=0.2791588651312328\n",
      "Current iteration=6, the loss=0.29584491650589934\n",
      "Current iteration=7, the loss=0.3107039898913525\n",
      "Current iteration=8, the loss=0.3238484703117911\n",
      "Current iteration=9, the loss=0.3354177917565968\n",
      "Current iteration=10, the loss=0.3455617051933831\n",
      "Current iteration=11, the loss=0.3544299177631033\n",
      "Current iteration=12, the loss=0.36216592643492657\n",
      "Current iteration=13, the loss=0.36890359005577766\n",
      "Current iteration=14, the loss=0.37476546896474777\n",
      "Current iteration=15, the loss=0.37986228640631814\n",
      "Current iteration=16, the loss=0.38429308338280194\n",
      "Current iteration=17, the loss=0.3881457840114953\n",
      "Current iteration=18, the loss=0.3914979858060138\n",
      "Current iteration=19, the loss=0.3944178545144825\n",
      "lambda :  0.0188739182214\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.2182751030272434\n",
      "Current iteration=3, the loss=0.2399971205331879\n",
      "Current iteration=4, the loss=0.26006409319417784\n",
      "Current iteration=5, the loss=0.27829684728039433\n",
      "Current iteration=6, the loss=0.2946667058279742\n",
      "Current iteration=7, the loss=0.3092355883101735\n",
      "Current iteration=8, the loss=0.3221166317581526\n",
      "Current iteration=9, the loss=0.33344871025460043\n",
      "Current iteration=10, the loss=0.34338025583090637\n",
      "Current iteration=11, the loss=0.3520592516866676\n",
      "Current iteration=12, the loss=0.3596272905153153\n",
      "Current iteration=13, the loss=0.366216287106697\n",
      "Current iteration=14, the loss=0.3719469037971263\n",
      "Current iteration=15, the loss=0.37692806214377955\n",
      "Current iteration=16, the loss=0.38125712497163156\n",
      "Current iteration=17, the loss=0.3850204740645039\n",
      "Current iteration=18, the loss=0.38829430332437626\n",
      "Current iteration=19, the loss=0.39114551060151853\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.2181537106442204\n",
      "Current iteration=3, the loss=0.24036340906912873\n",
      "Current iteration=4, the loss=0.26091545703291613\n",
      "Current iteration=5, the loss=0.27961372225438313\n",
      "Current iteration=6, the loss=0.29642008734245223\n",
      "Current iteration=7, the loss=0.3113918579227729\n",
      "Current iteration=8, the loss=0.3246406040616789\n",
      "Current iteration=9, the loss=0.33630551363794253\n",
      "Current iteration=10, the loss=0.3465364534327383\n",
      "Current iteration=11, the loss=0.3554834696770224\n",
      "Current iteration=12, the loss=0.3632905289558498\n",
      "Current iteration=13, the loss=0.37009202841478633\n",
      "Current iteration=14, the loss=0.37601109441729597\n",
      "Current iteration=15, the loss=0.38115901717142375\n",
      "Current iteration=16, the loss=0.38563538844346334\n",
      "Current iteration=17, the loss=0.38952865635872946\n",
      "Current iteration=18, the loss=0.3929169096167488\n",
      "Current iteration=19, the loss=0.3958687693157048\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.21807149621981237\n",
      "Current iteration=3, the loss=0.2399741920188249\n",
      "Current iteration=4, the loss=0.2602238811279766\n",
      "Current iteration=5, the loss=0.27863505939153527\n",
      "Current iteration=6, the loss=0.2951751172047039\n",
      "Current iteration=7, the loss=0.3099036815257064\n",
      "Current iteration=8, the loss=0.3229327105974829\n",
      "Current iteration=9, the loss=0.33440064927369256\n",
      "Current iteration=10, the loss=0.34445599793436055\n",
      "Current iteration=11, the loss=0.3532471310052446\n",
      "Current iteration=12, the loss=0.36091623393832195\n",
      "Current iteration=13, the loss=0.3675959315094518\n",
      "Current iteration=14, the loss=0.37340765485771493\n",
      "Current iteration=15, the loss=0.3784611129808128\n",
      "Current iteration=16, the loss=0.38285444752090697\n",
      "Current iteration=17, the loss=0.38667479239075053\n",
      "Current iteration=18, the loss=0.38999905543114327\n",
      "Current iteration=19, the loss=0.39289480342135025\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.21808122468553134\n",
      "Current iteration=3, the loss=0.2401639256340339\n",
      "Current iteration=4, the loss=0.2605868380273288\n",
      "Current iteration=5, the loss=0.27915886513111665\n",
      "Current iteration=6, the loss=0.2958449165057337\n",
      "Current iteration=7, the loss=0.31070398989113407\n",
      "Current iteration=8, the loss=0.32384847031151776\n",
      "Current iteration=9, the loss=0.33541779175626724\n",
      "Current iteration=10, the loss=0.3455617051929986\n",
      "Current iteration=11, the loss=0.35442991776266575\n",
      "Current iteration=12, the loss=0.36216592643443624\n",
      "Current iteration=13, the loss=0.3689035900552383\n",
      "Current iteration=14, the loss=0.3747654689641621\n",
      "Current iteration=15, the loss=0.3798622864056882\n",
      "Current iteration=16, the loss=0.3842930833821314\n",
      "Current iteration=17, the loss=0.3881457840107866\n",
      "Current iteration=18, the loss=0.39149798580526957\n",
      "Current iteration=19, the loss=0.39441785451370676\n",
      "lambda :  0.0303919538231\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.21827510302722355\n",
      "Current iteration=3, the loss=0.2399971205331288\n",
      "Current iteration=4, the loss=0.2600640931940631\n",
      "Current iteration=5, the loss=0.2782968472802113\n",
      "Current iteration=6, the loss=0.29466670582771354\n",
      "Current iteration=7, the loss=0.309235588309829\n",
      "Current iteration=8, the loss=0.32211663175772154\n",
      "Current iteration=9, the loss=0.3334487102540813\n",
      "Current iteration=10, the loss=0.34338025583030024\n",
      "Current iteration=11, the loss=0.3520592516859769\n",
      "Current iteration=12, the loss=0.3596272905145435\n",
      "Current iteration=13, the loss=0.366216287105847\n",
      "Current iteration=14, the loss=0.37194690379620315\n",
      "Current iteration=15, the loss=0.37692806214278735\n",
      "Current iteration=16, the loss=0.38125712497057535\n",
      "Current iteration=17, the loss=0.3850204740633878\n",
      "Current iteration=18, the loss=0.38829430332320514\n",
      "Current iteration=19, the loss=0.3911455106002951\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.21815371064420014\n",
      "Current iteration=3, the loss=0.2403634090690683\n",
      "Current iteration=4, the loss=0.26091545703279895\n",
      "Current iteration=5, the loss=0.2796137222541953\n",
      "Current iteration=6, the loss=0.29642008734218456\n",
      "Current iteration=7, the loss=0.3113918579224195\n",
      "Current iteration=8, the loss=0.32464060406123546\n",
      "Current iteration=9, the loss=0.33630551363740835\n",
      "Current iteration=10, the loss=0.34653645343211487\n",
      "Current iteration=11, the loss=0.35548346967631117\n",
      "Current iteration=12, the loss=0.36329052895505487\n",
      "Current iteration=13, the loss=0.3700920284139104\n",
      "Current iteration=14, the loss=0.3760110944163444\n",
      "Current iteration=15, the loss=0.38115901717040085\n",
      "Current iteration=16, the loss=0.38563538844237394\n",
      "Current iteration=17, the loss=0.3895286563575779\n",
      "Current iteration=18, the loss=0.39291690961553943\n",
      "Current iteration=19, the loss=0.39586876931444354\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.21807149621979222\n",
      "Current iteration=3, the loss=0.23997419201876555\n",
      "Current iteration=4, the loss=0.2602238811278611\n",
      "Current iteration=5, the loss=0.2786350593913502\n",
      "Current iteration=6, the loss=0.29517511720444084\n",
      "Current iteration=7, the loss=0.3099036815253586\n",
      "Current iteration=8, the loss=0.3229327105970469\n",
      "Current iteration=9, the loss=0.3344006492731681\n",
      "Current iteration=10, the loss=0.3444559979337479\n",
      "Current iteration=11, the loss=0.35324713100454624\n",
      "Current iteration=12, the loss=0.36091623393754135\n",
      "Current iteration=13, the loss=0.3675959315085917\n",
      "Current iteration=14, the loss=0.3734076548567809\n",
      "Current iteration=15, the loss=0.37846111297980917\n",
      "Current iteration=16, the loss=0.38285444751983727\n",
      "Current iteration=17, the loss=0.3866747923896214\n",
      "Current iteration=18, the loss=0.38999905542995666\n",
      "Current iteration=19, the loss=0.39289480342011146\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.21808122468551086\n",
      "Current iteration=3, the loss=0.24016392563397346\n",
      "Current iteration=4, the loss=0.2605868380272113\n",
      "Current iteration=5, the loss=0.27915886513092997\n",
      "Current iteration=6, the loss=0.2958449165054675\n",
      "Current iteration=7, the loss=0.3107039898907821\n",
      "Current iteration=8, the loss=0.32384847031107755\n",
      "Current iteration=9, the loss=0.33541779175573694\n",
      "Current iteration=10, the loss=0.3455617051923796\n",
      "Current iteration=11, the loss=0.35442991776195937\n",
      "Current iteration=12, the loss=0.36216592643364715\n",
      "Current iteration=13, the loss=0.36890359005436985\n",
      "Current iteration=14, the loss=0.3747654689632181\n",
      "Current iteration=15, the loss=0.37986228640467457\n",
      "Current iteration=16, the loss=0.38429308338105167\n",
      "Current iteration=17, the loss=0.38814578400964556\n",
      "Current iteration=18, the loss=0.3914979858040713\n",
      "Current iteration=19, the loss=0.3944178545124554\n",
      "lambda :  0.0489390091848\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.21827510302719141\n",
      "Current iteration=3, the loss=0.23999712053303382\n",
      "Current iteration=4, the loss=0.2600640931938788\n",
      "Current iteration=5, the loss=0.27829684727991616\n",
      "Current iteration=6, the loss=0.29466670582729354\n",
      "Current iteration=7, the loss=0.30923558830927445\n",
      "Current iteration=8, the loss=0.32211663175702676\n",
      "Current iteration=9, the loss=0.3334487102532452\n",
      "Current iteration=10, the loss=0.3433802558293243\n",
      "Current iteration=11, the loss=0.3520592516848639\n",
      "Current iteration=12, the loss=0.3596272905132997\n",
      "Current iteration=13, the loss=0.3662162871044781\n",
      "Current iteration=14, the loss=0.37194690379471596\n",
      "Current iteration=15, the loss=0.37692806214118985\n",
      "Current iteration=16, the loss=0.38125712496887343\n",
      "Current iteration=17, the loss=0.38502047406159\n",
      "Current iteration=18, the loss=0.3882943033213174\n",
      "Current iteration=19, the loss=0.39114551059832514\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.21815371064416708\n",
      "Current iteration=3, the loss=0.24036340906897102\n",
      "Current iteration=4, the loss=0.2609154570326095\n",
      "Current iteration=5, the loss=0.2796137222538925\n",
      "Current iteration=6, the loss=0.2964200873417532\n",
      "Current iteration=7, the loss=0.31139185792184904\n",
      "Current iteration=8, the loss=0.32464060406052114\n",
      "Current iteration=9, the loss=0.336305513636549\n",
      "Current iteration=10, the loss=0.3465364534311098\n",
      "Current iteration=11, the loss=0.35548346967516575\n",
      "Current iteration=12, the loss=0.36329052895377356\n",
      "Current iteration=13, the loss=0.3700920284125001\n",
      "Current iteration=14, the loss=0.3760110944148116\n",
      "Current iteration=15, the loss=0.3811590171687534\n",
      "Current iteration=16, the loss=0.3856353884406192\n",
      "Current iteration=17, the loss=0.3895286563557237\n",
      "Current iteration=18, the loss=0.39291690961359255\n",
      "Current iteration=19, the loss=0.3958687693124099\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.21807149621975994\n",
      "Current iteration=3, the loss=0.2399741920186695\n",
      "Current iteration=4, the loss=0.26022388112767464\n",
      "Current iteration=5, the loss=0.27863505939105265\n",
      "Current iteration=6, the loss=0.2951751172040166\n",
      "Current iteration=7, the loss=0.30990368152479875\n",
      "Current iteration=8, the loss=0.3229327105963455\n",
      "Current iteration=9, the loss=0.33440064927232277\n",
      "Current iteration=10, the loss=0.3444559979327616\n",
      "Current iteration=11, the loss=0.3532471310034219\n",
      "Current iteration=12, the loss=0.36091623393628375\n",
      "Current iteration=13, the loss=0.36759593150720826\n",
      "Current iteration=14, the loss=0.3734076548552771\n",
      "Current iteration=15, the loss=0.378461112978193\n",
      "Current iteration=16, the loss=0.38285444751811626\n",
      "Current iteration=17, the loss=0.3866747923878014\n",
      "Current iteration=18, the loss=0.3899990554280475\n",
      "Current iteration=19, the loss=0.3928948034181178\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.2180812246854785\n",
      "Current iteration=3, the loss=0.24016392563387645\n",
      "Current iteration=4, the loss=0.2605868380270237\n",
      "Current iteration=5, the loss=0.2791588651306295\n",
      "Current iteration=6, the loss=0.29584491650503963\n",
      "Current iteration=7, the loss=0.31070398989021664\n",
      "Current iteration=8, the loss=0.3238484703103689\n",
      "Current iteration=9, the loss=0.33541779175488357\n",
      "Current iteration=10, the loss=0.3455617051913832\n",
      "Current iteration=11, the loss=0.35442991776082355\n",
      "Current iteration=12, the loss=0.3621659264323769\n",
      "Current iteration=13, the loss=0.36890359005297146\n",
      "Current iteration=14, the loss=0.3747654689616995\n",
      "Current iteration=15, the loss=0.37986228640304154\n",
      "Current iteration=16, the loss=0.3842930833793127\n",
      "Current iteration=17, the loss=0.3881457840078073\n",
      "Current iteration=18, the loss=0.3914979858021421\n",
      "Current iteration=19, the loss=0.394417854510441\n",
      "lambda :  0.0788046281567\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.21827510302714012\n",
      "Current iteration=3, the loss=0.23999712053288064\n",
      "Current iteration=4, the loss=0.26006409319358104\n",
      "Current iteration=5, the loss=0.2782968472794415\n",
      "Current iteration=6, the loss=0.2946667058266168\n",
      "Current iteration=7, the loss=0.3092355883083809\n",
      "Current iteration=8, the loss=0.3221166317559077\n",
      "Current iteration=9, the loss=0.3334487102518988\n",
      "Current iteration=10, the loss=0.34338025582775256\n",
      "Current iteration=11, the loss=0.352059251683073\n",
      "Current iteration=12, the loss=0.3596272905112979\n",
      "Current iteration=13, the loss=0.36621628710227455\n",
      "Current iteration=14, the loss=0.37194690379232176\n",
      "Current iteration=15, the loss=0.37692806213861735\n",
      "Current iteration=16, the loss=0.38125712496613434\n",
      "Current iteration=17, the loss=0.3850204740586961\n",
      "Current iteration=18, the loss=0.3882943033182792\n",
      "Current iteration=19, the loss=0.39114551059515396\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.2181537106441147\n",
      "Current iteration=3, the loss=0.2403634090688143\n",
      "Current iteration=4, the loss=0.2609154570323047\n",
      "Current iteration=5, the loss=0.27961372225340536\n",
      "Current iteration=6, the loss=0.2964200873410586\n",
      "Current iteration=7, the loss=0.3113918579209306\n",
      "Current iteration=8, the loss=0.3246406040593706\n",
      "Current iteration=9, the loss=0.3363055136351636\n",
      "Current iteration=10, the loss=0.3465364534294923\n",
      "Current iteration=11, the loss=0.3554834696733213\n",
      "Current iteration=12, the loss=0.36329052895171043\n",
      "Current iteration=13, the loss=0.3700920284102293\n",
      "Current iteration=14, the loss=0.3760110944123444\n",
      "Current iteration=15, the loss=0.38115901716610046\n",
      "Current iteration=16, the loss=0.38563538843779377\n",
      "Current iteration=17, the loss=0.38952865635273765\n",
      "Current iteration=18, the loss=0.39291690961045667\n",
      "Current iteration=19, the loss=0.3958687693091364\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.21807149621970814\n",
      "Current iteration=3, the loss=0.23997419201851497\n",
      "Current iteration=4, the loss=0.26022388112737427\n",
      "Current iteration=5, the loss=0.2786350593905737\n",
      "Current iteration=6, the loss=0.29517511720333367\n",
      "Current iteration=7, the loss=0.309903681523896\n",
      "Current iteration=8, the loss=0.32293271059521467\n",
      "Current iteration=9, the loss=0.33440064927096247\n",
      "Current iteration=10, the loss=0.34445599793117293\n",
      "Current iteration=11, the loss=0.3532471310016103\n",
      "Current iteration=12, the loss=0.3609162339342584\n",
      "Current iteration=13, the loss=0.36759593150497927\n",
      "Current iteration=14, the loss=0.3734076548528554\n",
      "Current iteration=15, the loss=0.3784611129755906\n",
      "Current iteration=16, the loss=0.382854447515344\n",
      "Current iteration=17, the loss=0.38667479238487257\n",
      "Current iteration=18, the loss=0.38999905542497215\n",
      "Current iteration=19, the loss=0.39289480341490757\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.21808122468542612\n",
      "Current iteration=3, the loss=0.24016392563372127\n",
      "Current iteration=4, the loss=0.26058683802672095\n",
      "Current iteration=5, the loss=0.27915886513014576\n",
      "Current iteration=6, the loss=0.29584491650435013\n",
      "Current iteration=7, the loss=0.31070398988930503\n",
      "Current iteration=8, the loss=0.3238484703092274\n",
      "Current iteration=9, the loss=0.3354177917535097\n",
      "Current iteration=10, the loss=0.3455617051897788\n",
      "Current iteration=11, the loss=0.35442991775899396\n",
      "Current iteration=12, the loss=0.36216592643033163\n",
      "Current iteration=13, the loss=0.36890359005072\n",
      "Current iteration=14, the loss=0.3747654689592529\n",
      "Current iteration=15, the loss=0.37986228640041236\n",
      "Current iteration=16, the loss=0.38429308337651197\n",
      "Current iteration=17, the loss=0.38814578400484845\n",
      "Current iteration=18, the loss=0.39149798579903555\n",
      "Current iteration=19, the loss=0.3944178545071979\n",
      "lambda :  0.126896100317\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.2182751030270567\n",
      "Current iteration=3, the loss=0.23999712053263364\n",
      "Current iteration=4, the loss=0.2600640931931024\n",
      "Current iteration=5, the loss=0.27829684727867665\n",
      "Current iteration=6, the loss=0.2946667058255278\n",
      "Current iteration=7, the loss=0.30923558830694176\n",
      "Current iteration=8, the loss=0.32211663175410615\n",
      "Current iteration=9, the loss=0.3334487102497302\n",
      "Current iteration=10, the loss=0.3433802558252214\n",
      "Current iteration=11, the loss=0.35205925168018837\n",
      "Current iteration=12, the loss=0.35962729050807263\n",
      "Current iteration=13, the loss=0.3662162870987261\n",
      "Current iteration=14, the loss=0.37194690378846673\n",
      "Current iteration=15, the loss=0.3769280621344748\n",
      "Current iteration=16, the loss=0.381257124961723\n",
      "Current iteration=17, the loss=0.38502047405403506\n",
      "Current iteration=18, the loss=0.38829430331338616\n",
      "Current iteration=19, the loss=0.39114551059004676\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.21815371064402972\n",
      "Current iteration=3, the loss=0.24036340906856182\n",
      "Current iteration=4, the loss=0.2609154570318139\n",
      "Current iteration=5, the loss=0.27961372225262127\n",
      "Current iteration=6, the loss=0.29642008733994035\n",
      "Current iteration=7, the loss=0.31139185791945306\n",
      "Current iteration=8, the loss=0.32464060405751943\n",
      "Current iteration=9, the loss=0.3363055136329336\n",
      "Current iteration=10, the loss=0.34653645342688794\n",
      "Current iteration=11, the loss=0.35548346967035177\n",
      "Current iteration=12, the loss=0.36329052894838887\n",
      "Current iteration=13, the loss=0.3700920284065726\n",
      "Current iteration=14, the loss=0.3760110944083709\n",
      "Current iteration=15, the loss=0.3811590171618295\n",
      "Current iteration=16, the loss=0.38563538843324463\n",
      "Current iteration=17, the loss=0.3895286563479289\n",
      "Current iteration=18, the loss=0.3929169096054081\n",
      "Current iteration=19, the loss=0.39586876930386466\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.21807149621962432\n",
      "Current iteration=3, the loss=0.23997419201826586\n",
      "Current iteration=4, the loss=0.2602238811268914\n",
      "Current iteration=5, the loss=0.2786350593898017\n",
      "Current iteration=6, the loss=0.29517511720223377\n",
      "Current iteration=7, the loss=0.30990368152244296\n",
      "Current iteration=8, the loss=0.3229327105933959\n",
      "Current iteration=9, the loss=0.3344006492687716\n",
      "Current iteration=10, the loss=0.3444559979286143\n",
      "Current iteration=11, the loss=0.35324713099869454\n",
      "Current iteration=12, the loss=0.3609162339309976\n",
      "Current iteration=13, the loss=0.36759593150138975\n",
      "Current iteration=14, the loss=0.373407654848956\n",
      "Current iteration=15, the loss=0.3784611129713996\n",
      "Current iteration=16, the loss=0.38285444751088155\n",
      "Current iteration=17, the loss=0.38667479238015623\n",
      "Current iteration=18, the loss=0.38999905542002017\n",
      "Current iteration=19, the loss=0.3928948034097382\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.21808122468534136\n",
      "Current iteration=3, the loss=0.24016392563347047\n",
      "Current iteration=4, the loss=0.26058683802623356\n",
      "Current iteration=5, the loss=0.2791588651293667\n",
      "Current iteration=6, the loss=0.29584491650323963\n",
      "Current iteration=7, the loss=0.3107039898878379\n",
      "Current iteration=8, the loss=0.3238484703073893\n",
      "Current iteration=9, the loss=0.3354177917512965\n",
      "Current iteration=10, the loss=0.34556170518719437\n",
      "Current iteration=11, the loss=0.35442991775604815\n",
      "Current iteration=12, the loss=0.3621659264270375\n",
      "Current iteration=13, the loss=0.36890359004709455\n",
      "Current iteration=14, the loss=0.37476546895531365\n",
      "Current iteration=15, the loss=0.3798622863961786\n",
      "Current iteration=16, the loss=0.38429308337200413\n",
      "Current iteration=17, the loss=0.38814578400008354\n",
      "Current iteration=18, the loss=0.3914979857940333\n",
      "Current iteration=19, the loss=0.39441785450197575\n",
      "lambda :  0.204335971786\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.218275103026923\n",
      "Current iteration=3, the loss=0.23999712053223635\n",
      "Current iteration=4, the loss=0.2600640931923312\n",
      "Current iteration=5, the loss=0.278296847277445\n",
      "Current iteration=6, the loss=0.29466670582377336\n",
      "Current iteration=7, the loss=0.3092355883046249\n",
      "Current iteration=8, the loss=0.32211663175120503\n",
      "Current iteration=9, the loss=0.3334487102462393\n",
      "Current iteration=10, the loss=0.34338025582114556\n",
      "Current iteration=11, the loss=0.35205925167554386\n",
      "Current iteration=12, the loss=0.35962729050287956\n",
      "Current iteration=13, the loss=0.36621628709301046\n",
      "Current iteration=14, the loss=0.37194690378225936\n",
      "Current iteration=15, the loss=0.3769280621278043\n",
      "Current iteration=16, the loss=0.3812571249546207\n",
      "Current iteration=17, the loss=0.3850204740465291\n",
      "Current iteration=18, the loss=0.38829430330550774\n",
      "Current iteration=19, the loss=0.39114551058182156\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.218153710643893\n",
      "Current iteration=3, the loss=0.24036340906815565\n",
      "Current iteration=4, the loss=0.2609154570310243\n",
      "Current iteration=5, the loss=0.27961372225135783\n",
      "Current iteration=6, the loss=0.2964200873381397\n",
      "Current iteration=7, the loss=0.31139185791707313\n",
      "Current iteration=8, the loss=0.324640604054537\n",
      "Current iteration=9, the loss=0.3363055136293427\n",
      "Current iteration=10, the loss=0.3465364534226939\n",
      "Current iteration=11, the loss=0.3554834696655694\n",
      "Current iteration=12, the loss=0.3632905289430406\n",
      "Current iteration=13, the loss=0.3700920284006849\n",
      "Current iteration=14, the loss=0.37601109440197256\n",
      "Current iteration=15, the loss=0.3811590171549516\n",
      "Current iteration=16, the loss=0.38563538842591877\n",
      "Current iteration=17, the loss=0.3895286563401859\n",
      "Current iteration=18, the loss=0.3929169095972774\n",
      "Current iteration=19, the loss=0.39586876929537607\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.2180714962194895\n",
      "Current iteration=3, the loss=0.23997419201786555\n",
      "Current iteration=4, the loss=0.26022388112611367\n",
      "Current iteration=5, the loss=0.27863505938855904\n",
      "Current iteration=6, the loss=0.2951751172004632\n",
      "Current iteration=7, the loss=0.3099036815201033\n",
      "Current iteration=8, the loss=0.3229327105904648\n",
      "Current iteration=9, the loss=0.33440064926524377\n",
      "Current iteration=10, the loss=0.3444559979244952\n",
      "Current iteration=11, the loss=0.3532471309939988\n",
      "Current iteration=12, the loss=0.3609162339257468\n",
      "Current iteration=13, the loss=0.36759593149560954\n",
      "Current iteration=14, the loss=0.37340765484267613\n",
      "Current iteration=15, the loss=0.37846111296465\n",
      "Current iteration=16, the loss=0.38285444750369413\n",
      "Current iteration=17, the loss=0.38667479237256075\n",
      "Current iteration=18, the loss=0.3899990554120463\n",
      "Current iteration=19, the loss=0.39289480340141386\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.21808122468520558\n",
      "Current iteration=3, the loss=0.24016392563306588\n",
      "Current iteration=4, the loss=0.2605868380254488\n",
      "Current iteration=5, the loss=0.2791588651281119\n",
      "Current iteration=6, the loss=0.295844916501452\n",
      "Current iteration=7, the loss=0.3107039898854751\n",
      "Current iteration=8, the loss=0.32384847030442954\n",
      "Current iteration=9, the loss=0.33541779174773334\n",
      "Current iteration=10, the loss=0.3455617051830341\n",
      "Current iteration=11, the loss=0.3544299177513051\n",
      "Current iteration=12, the loss=0.3621659264217341\n",
      "Current iteration=13, the loss=0.3689035900412555\n",
      "Current iteration=14, the loss=0.37476546894897034\n",
      "Current iteration=15, the loss=0.379862286389361\n",
      "Current iteration=16, the loss=0.38429308336474305\n",
      "Current iteration=17, the loss=0.38814578399241106\n",
      "Current iteration=18, the loss=0.3914979857859781\n",
      "Current iteration=19, the loss=0.39441785449356503\n",
      "lambda :  0.329034456231\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.21827510302670722\n",
      "Current iteration=3, the loss=0.23999712053159641\n",
      "Current iteration=4, the loss=0.26006409319108914\n",
      "Current iteration=5, the loss=0.2782968472754624\n",
      "Current iteration=6, the loss=0.29466670582094895\n",
      "Current iteration=7, the loss=0.3092355883008948\n",
      "Current iteration=8, the loss=0.322116631746534\n",
      "Current iteration=9, the loss=0.3334487102406178\n",
      "Current iteration=10, the loss=0.3433802558145838\n",
      "Current iteration=11, the loss=0.35205925166806423\n",
      "Current iteration=12, the loss=0.35962729049451847\n",
      "Current iteration=13, the loss=0.36621628708380916\n",
      "Current iteration=14, the loss=0.371946903772264\n",
      "Current iteration=15, the loss=0.376928062117063\n",
      "Current iteration=16, the loss=0.38125712494318253\n",
      "Current iteration=17, the loss=0.38502047403444334\n",
      "Current iteration=18, the loss=0.38829430329282116\n",
      "Current iteration=19, the loss=0.3911455105685784\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.21815371064367287\n",
      "Current iteration=3, the loss=0.2403634090675004\n",
      "Current iteration=4, the loss=0.2609154570297526\n",
      "Current iteration=5, the loss=0.2796137222493241\n",
      "Current iteration=6, the loss=0.29642008733524106\n",
      "Current iteration=7, the loss=0.3113918579132404\n",
      "Current iteration=8, the loss=0.32464060404973505\n",
      "Current iteration=9, the loss=0.3363055136235603\n",
      "Current iteration=10, the loss=0.3465364534159403\n",
      "Current iteration=11, the loss=0.35548346965786887\n",
      "Current iteration=12, the loss=0.3632905289344284\n",
      "Current iteration=13, the loss=0.3700920283912033\n",
      "Current iteration=14, the loss=0.376011094391669\n",
      "Current iteration=15, the loss=0.38115901714387646\n",
      "Current iteration=16, the loss=0.3856353884141217\n",
      "Current iteration=17, the loss=0.3895286563277178\n",
      "Current iteration=18, the loss=0.3929169095841865\n",
      "Current iteration=19, the loss=0.3958687692817073\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.21807149621927208\n",
      "Current iteration=3, the loss=0.23997419201722123\n",
      "Current iteration=4, the loss=0.2602238811248617\n",
      "Current iteration=5, the loss=0.27863505938655825\n",
      "Current iteration=6, the loss=0.2951751171976123\n",
      "Current iteration=7, the loss=0.3099036815163361\n",
      "Current iteration=8, the loss=0.3229327105857464\n",
      "Current iteration=9, the loss=0.3344006492595633\n",
      "Current iteration=10, the loss=0.34445599791786236\n",
      "Current iteration=11, the loss=0.3532471309864369\n",
      "Current iteration=12, the loss=0.3609162339172918\n",
      "Current iteration=13, the loss=0.3675959314863031\n",
      "Current iteration=14, the loss=0.37340765483256516\n",
      "Current iteration=15, the loss=0.37846111295378343\n",
      "Current iteration=16, the loss=0.3828544474921209\n",
      "Current iteration=17, the loss=0.3866747923603303\n",
      "Current iteration=18, the loss=0.3899990553992068\n",
      "Current iteration=19, the loss=0.3928948033880095\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.2180812246849863\n",
      "Current iteration=3, the loss=0.2401639256324155\n",
      "Current iteration=4, the loss=0.2605868380241842\n",
      "Current iteration=5, the loss=0.279158865126092\n",
      "Current iteration=6, the loss=0.2958449164985729\n",
      "Current iteration=7, the loss=0.3107039898816704\n",
      "Current iteration=8, the loss=0.323848470299664\n",
      "Current iteration=9, the loss=0.33541779174199626\n",
      "Current iteration=10, the loss=0.34556170517633394\n",
      "Current iteration=11, the loss=0.35442991774366783\n",
      "Current iteration=12, the loss=0.36216592641319334\n",
      "Current iteration=13, the loss=0.36890359003185513\n",
      "Current iteration=14, the loss=0.3747654689387566\n",
      "Current iteration=15, the loss=0.37986228637838376\n",
      "Current iteration=16, the loss=0.38429308335305146\n",
      "Current iteration=17, the loss=0.38814578398005556\n",
      "Current iteration=18, the loss=0.3914979857730063\n",
      "Current iteration=19, the loss=0.394417854480024\n",
      "lambda :  0.529831690628\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.21827510302635966\n",
      "Current iteration=3, the loss=0.23999712053056652\n",
      "Current iteration=4, the loss=0.26006409318908935\n",
      "Current iteration=5, the loss=0.2782968472722689\n",
      "Current iteration=6, the loss=0.2946667058164003\n",
      "Current iteration=7, the loss=0.3092355882948867\n",
      "Current iteration=8, the loss=0.3221166317390123\n",
      "Current iteration=9, the loss=0.33344871023156525\n",
      "Current iteration=10, the loss=0.3433802558040156\n",
      "Current iteration=11, the loss=0.35205925165602064\n",
      "Current iteration=12, the loss=0.3596272904810541\n",
      "Current iteration=13, the loss=0.36621628706899184\n",
      "Current iteration=14, the loss=0.37194690375616773\n",
      "Current iteration=15, the loss=0.3769280620997669\n",
      "Current iteration=16, the loss=0.381257124924765\n",
      "Current iteration=17, the loss=0.38502047401498274\n",
      "Current iteration=18, the loss=0.38829430327239245\n",
      "Current iteration=19, the loss=0.39114551054725255\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.21815371064331815\n",
      "Current iteration=3, the loss=0.24036340906644746\n",
      "Current iteration=4, the loss=0.26091545702770386\n",
      "Current iteration=5, the loss=0.27961372224604963\n",
      "Current iteration=6, the loss=0.2964200873305723\n",
      "Current iteration=7, the loss=0.31139185790706947\n",
      "Current iteration=8, the loss=0.3246406040420026\n",
      "Current iteration=9, the loss=0.33630551361424943\n",
      "Current iteration=10, the loss=0.3465364534050661\n",
      "Current iteration=11, the loss=0.35548346964546934\n",
      "Current iteration=12, the loss=0.36329052892056046\n",
      "Current iteration=13, the loss=0.370092028375936\n",
      "Current iteration=14, the loss=0.3760110943750783\n",
      "Current iteration=15, the loss=0.3811590171260429\n",
      "Current iteration=16, the loss=0.3856353883951262\n",
      "Current iteration=17, the loss=0.3895286563076405\n",
      "Current iteration=18, the loss=0.3929169095631059\n",
      "Current iteration=19, the loss=0.39586876925969683\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.21807149621892213\n",
      "Current iteration=3, the loss=0.23997419201618278\n",
      "Current iteration=4, the loss=0.2602238811228447\n",
      "Current iteration=5, the loss=0.2786350593833362\n",
      "Current iteration=6, the loss=0.29517511719302053\n",
      "Current iteration=7, the loss=0.30990368151026965\n",
      "Current iteration=8, the loss=0.3229327105781472\n",
      "Current iteration=9, the loss=0.334400649250416\n",
      "Current iteration=10, the loss=0.3444559979071811\n",
      "Current iteration=11, the loss=0.35324713097426075\n",
      "Current iteration=12, the loss=0.36091623390367666\n",
      "Current iteration=13, the loss=0.3675959314713171\n",
      "Current iteration=14, the loss=0.3734076548162828\n",
      "Current iteration=15, the loss=0.3784611129362845\n",
      "Current iteration=16, the loss=0.3828544474734852\n",
      "Current iteration=17, the loss=0.38667479234063645\n",
      "Current iteration=18, the loss=0.38999905537853097\n",
      "Current iteration=19, the loss=0.39289480336642485\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.21808122468463356\n",
      "Current iteration=3, the loss=0.2401639256313671\n",
      "Current iteration=4, the loss=0.26058683802214855\n",
      "Current iteration=5, the loss=0.27915886512283833\n",
      "Current iteration=6, the loss=0.29584491649393574\n",
      "Current iteration=7, the loss=0.310703989875544\n",
      "Current iteration=8, the loss=0.32384847029198927\n",
      "Current iteration=9, the loss=0.33541779173275676\n",
      "Current iteration=10, the loss=0.3455617051655454\n",
      "Current iteration=11, the loss=0.3544299177313687\n",
      "Current iteration=12, the loss=0.3621659263994401\n",
      "Current iteration=13, the loss=0.36890359001671724\n",
      "Current iteration=14, the loss=0.374765468922309\n",
      "Current iteration=15, the loss=0.3798622863607066\n",
      "Current iteration=16, the loss=0.38429308333422535\n",
      "Current iteration=17, the loss=0.3881457839601602\n",
      "Current iteration=18, the loss=0.3914979857521191\n",
      "Current iteration=19, the loss=0.3944178544582177\n",
      "lambda :  0.853167852417\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.21827510302580047\n",
      "Current iteration=3, the loss=0.23999712052890784\n",
      "Current iteration=4, the loss=0.2600640931858701\n",
      "Current iteration=5, the loss=0.278296847267127\n",
      "Current iteration=6, the loss=0.29466670580907706\n",
      "Current iteration=7, the loss=0.3092355882852136\n",
      "Current iteration=8, the loss=0.3221166317268995\n",
      "Current iteration=9, the loss=0.33344871021698813\n",
      "Current iteration=10, the loss=0.3433802557869993\n",
      "Current iteration=11, the loss=0.35205925163662716\n",
      "Current iteration=12, the loss=0.3596272904593737\n",
      "Current iteration=13, the loss=0.36621628704513254\n",
      "Current iteration=14, the loss=0.3719469037302491\n",
      "Current iteration=15, the loss=0.3769280620719154\n",
      "Current iteration=16, the loss=0.3812571248951077\n",
      "Current iteration=17, the loss=0.38502047398364575\n",
      "Current iteration=18, the loss=0.3882943032394958\n",
      "Current iteration=19, the loss=0.3911455105129136\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.21815371064274675\n",
      "Current iteration=3, the loss=0.2403634090647499\n",
      "Current iteration=4, the loss=0.2609154570244049\n",
      "Current iteration=5, the loss=0.2796137222407754\n",
      "Current iteration=6, the loss=0.2964200873230536\n",
      "Current iteration=7, the loss=0.3113918578971313\n",
      "Current iteration=8, the loss=0.32464060402955186\n",
      "Current iteration=9, the loss=0.3363055135992565\n",
      "Current iteration=10, the loss=0.34653645338755423\n",
      "Current iteration=11, the loss=0.3554834696255025\n",
      "Current iteration=12, the loss=0.36329052889822955\n",
      "Current iteration=13, the loss=0.3700920283513508\n",
      "Current iteration=14, the loss=0.3760110943483622\n",
      "Current iteration=15, the loss=0.3811590170973254\n",
      "Current iteration=16, the loss=0.385635388364538\n",
      "Current iteration=17, the loss=0.3895286562753112\n",
      "Current iteration=18, the loss=0.39291690952916025\n",
      "Current iteration=19, the loss=0.39586876922425446\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.2180714962183592\n",
      "Current iteration=3, the loss=0.23997419201451098\n",
      "Current iteration=4, the loss=0.26022388111959727\n",
      "Current iteration=5, the loss=0.27863505937814725\n",
      "Current iteration=6, the loss=0.2951751171856272\n",
      "Current iteration=7, the loss=0.3099036815005002\n",
      "Current iteration=8, the loss=0.32293271056591133\n",
      "Current iteration=9, the loss=0.33440064923568624\n",
      "Current iteration=10, the loss=0.3444559978899809\n",
      "Current iteration=11, the loss=0.35324713095465377\n",
      "Current iteration=12, the loss=0.3609162338817524\n",
      "Current iteration=13, the loss=0.36759593144718483\n",
      "Current iteration=14, the loss=0.3734076547900649\n",
      "Current iteration=15, the loss=0.3784611129081061\n",
      "Current iteration=16, the loss=0.38285444744347674\n",
      "Current iteration=17, the loss=0.38667479230892504\n",
      "Current iteration=18, the loss=0.38999905534523877\n",
      "Current iteration=19, the loss=0.3928948033316682\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.2180812246840654\n",
      "Current iteration=3, the loss=0.24016392562968028\n",
      "Current iteration=4, the loss=0.26058683801887084\n",
      "Current iteration=5, the loss=0.27915886511760024\n",
      "Current iteration=6, the loss=0.2958449164864705\n",
      "Current iteration=7, the loss=0.3107039898656781\n",
      "Current iteration=8, the loss=0.3238484702796315\n",
      "Current iteration=9, the loss=0.33541779171787967\n",
      "Current iteration=10, the loss=0.34556170514817347\n",
      "Current iteration=11, the loss=0.3544299177115638\n",
      "Current iteration=12, the loss=0.36216592637729467\n",
      "Current iteration=13, the loss=0.36890358999234035\n",
      "Current iteration=14, the loss=0.37476546889582346\n",
      "Current iteration=15, the loss=0.37986228633224095\n",
      "Current iteration=16, the loss=0.3842930833039104\n",
      "Current iteration=17, the loss=0.38814578392812316\n",
      "Current iteration=18, the loss=0.3914979857184853\n",
      "Current iteration=19, the loss=0.39441785442310434\n",
      "lambda :  1.37382379588\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.21827510302490008\n",
      "Current iteration=3, the loss=0.23999712052623703\n",
      "Current iteration=4, the loss=0.26006409318068585\n",
      "Current iteration=5, the loss=0.27829684725884735\n",
      "Current iteration=6, the loss=0.2946667057972841\n",
      "Current iteration=7, the loss=0.309235588269637\n",
      "Current iteration=8, the loss=0.32211663170739546\n",
      "Current iteration=9, the loss=0.3334487101935163\n",
      "Current iteration=10, the loss=0.3433802557595986\n",
      "Current iteration=11, the loss=0.3520592516053986\n",
      "Current iteration=12, the loss=0.3596272904244615\n",
      "Current iteration=13, the loss=0.3662162870067126\n",
      "Current iteration=14, the loss=0.37194690368851346\n",
      "Current iteration=15, the loss=0.3769280620270673\n",
      "Current iteration=16, the loss=0.38125712484735247\n",
      "Current iteration=17, the loss=0.38502047393318467\n",
      "Current iteration=18, the loss=0.38829430318652497\n",
      "Current iteration=19, the loss=0.3911455104576176\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.21815371064182726\n",
      "Current iteration=3, the loss=0.24036340906201742\n",
      "Current iteration=4, the loss=0.26091545701909313\n",
      "Current iteration=5, the loss=0.27961372223228376\n",
      "Current iteration=6, the loss=0.29642008731094777\n",
      "Current iteration=7, the loss=0.31139185788112944\n",
      "Current iteration=8, the loss=0.3246406040095023\n",
      "Current iteration=9, the loss=0.33630551357511385\n",
      "Current iteration=10, the loss=0.34653645335935623\n",
      "Current iteration=11, the loss=0.3554834695933501\n",
      "Current iteration=12, the loss=0.36329052886227026\n",
      "Current iteration=13, the loss=0.3700920283117627\n",
      "Current iteration=14, the loss=0.3760110943053432\n",
      "Current iteration=15, the loss=0.38115901705108257\n",
      "Current iteration=16, the loss=0.38563538831528293\n",
      "Current iteration=17, the loss=0.3895286562232527\n",
      "Current iteration=18, the loss=0.3929169094744991\n",
      "Current iteration=19, the loss=0.39586876916718305\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.21807149621745223\n",
      "Current iteration=3, the loss=0.23997419201181822\n",
      "Current iteration=4, the loss=0.2602238811143681\n",
      "Current iteration=5, the loss=0.2786350593697928\n",
      "Current iteration=6, the loss=0.2951751171737216\n",
      "Current iteration=7, the loss=0.3099036814847702\n",
      "Current iteration=8, the loss=0.3229327105462082\n",
      "Current iteration=9, the loss=0.3344006492119674\n",
      "Current iteration=10, the loss=0.3444559978622856\n",
      "Current iteration=11, the loss=0.35324713092308196\n",
      "Current iteration=12, the loss=0.3609162338464495\n",
      "Current iteration=13, the loss=0.3675959314083274\n",
      "Current iteration=14, the loss=0.37340765474784654\n",
      "Current iteration=15, the loss=0.3784611128627324\n",
      "Current iteration=16, the loss=0.3828544473951544\n",
      "Current iteration=17, the loss=0.38667479225785983\n",
      "Current iteration=18, the loss=0.3899990552916282\n",
      "Current iteration=19, the loss=0.39289480327570114\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.2180812246831508\n",
      "Current iteration=3, the loss=0.24016392562696412\n",
      "Current iteration=4, the loss=0.2605868380135922\n",
      "Current iteration=5, the loss=0.2791588651091644\n",
      "Current iteration=6, the loss=0.2958449164744487\n",
      "Current iteration=7, the loss=0.31070398984979225\n",
      "Current iteration=8, the loss=0.32384847025973257\n",
      "Current iteration=9, the loss=0.335417791693924\n",
      "Current iteration=10, the loss=0.345561705120199\n",
      "Current iteration=11, the loss=0.3544299176796733\n",
      "Current iteration=12, the loss=0.36216592634163475\n",
      "Current iteration=13, the loss=0.36890358995308836\n",
      "Current iteration=14, the loss=0.3747654688531763\n",
      "Current iteration=15, the loss=0.379862286286405\n",
      "Current iteration=16, the loss=0.38429308325509565\n",
      "Current iteration=17, the loss=0.38814578387653675\n",
      "Current iteration=18, the loss=0.3914979856643255\n",
      "Current iteration=19, the loss=0.39441785436656296\n",
      "lambda :  2.21221629107\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.21827510302345013\n",
      "Current iteration=3, the loss=0.23999712052193642\n",
      "Current iteration=4, the loss=0.2600640931723374\n",
      "Current iteration=5, the loss=0.27829684724551523\n",
      "Current iteration=6, the loss=0.2946667057782943\n",
      "Current iteration=7, the loss=0.3092355882445551\n",
      "Current iteration=8, the loss=0.3221166316759887\n",
      "Current iteration=9, the loss=0.3334487101557196\n",
      "Current iteration=10, the loss=0.34338025571547537\n",
      "Current iteration=11, the loss=0.352059251555112\n",
      "Current iteration=12, the loss=0.3596272903682442\n",
      "Current iteration=13, the loss=0.3662162869448458\n",
      "Current iteration=14, the loss=0.37194690362130883\n",
      "Current iteration=15, the loss=0.37692806195484974\n",
      "Current iteration=16, the loss=0.3812571247704531\n",
      "Current iteration=17, the loss=0.38502047385192867\n",
      "Current iteration=18, the loss=0.3882943031012274\n",
      "Current iteration=19, the loss=0.3911455103685766\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.2181537106403467\n",
      "Current iteration=3, the loss=0.24036340905761724\n",
      "Current iteration=4, the loss=0.26091545701054\n",
      "Current iteration=5, the loss=0.2796137222186096\n",
      "Current iteration=6, the loss=0.2964200872914541\n",
      "Current iteration=7, the loss=0.3113918578553624\n",
      "Current iteration=8, the loss=0.3246406039772166\n",
      "Current iteration=9, the loss=0.33630551353623733\n",
      "Current iteration=10, the loss=0.34653645331395044\n",
      "Current iteration=11, the loss=0.35548346954157733\n",
      "Current iteration=12, the loss=0.36329052880436613\n",
      "Current iteration=13, the loss=0.3700920282480161\n",
      "Current iteration=14, the loss=0.3760110942360714\n",
      "Current iteration=15, the loss=0.3811590169766201\n",
      "Current iteration=16, the loss=0.3856353882359707\n",
      "Current iteration=17, the loss=0.38952865613942433\n",
      "Current iteration=18, the loss=0.3929169093864803\n",
      "Current iteration=19, the loss=0.39586876907528257\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.21807149621599176\n",
      "Current iteration=3, the loss=0.23997419200748335\n",
      "Current iteration=4, the loss=0.26022388110594835\n",
      "Current iteration=5, the loss=0.2786350593563387\n",
      "Current iteration=6, the loss=0.29517511715455186\n",
      "Current iteration=7, the loss=0.3099036814594391\n",
      "Current iteration=8, the loss=0.32293271051448075\n",
      "Current iteration=9, the loss=0.33440064917377355\n",
      "Current iteration=10, the loss=0.34445599781768854\n",
      "Current iteration=11, the loss=0.353247130872242\n",
      "Current iteration=12, the loss=0.36091623378960247\n",
      "Current iteration=13, the loss=0.3675959313457547\n",
      "Current iteration=14, the loss=0.37340765467986325\n",
      "Current iteration=15, the loss=0.37846111278966865\n",
      "Current iteration=16, the loss=0.38285444731734386\n",
      "Current iteration=17, the loss=0.3866747921756313\n",
      "Current iteration=18, the loss=0.3899990552053014\n",
      "Current iteration=19, the loss=0.39289480318557923\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.2180812246816777\n",
      "Current iteration=3, the loss=0.24016392562258929\n",
      "Current iteration=4, the loss=0.26058683800509297\n",
      "Current iteration=5, the loss=0.2791588650955818\n",
      "Current iteration=6, the loss=0.29584491645509076\n",
      "Current iteration=7, the loss=0.31070398982421193\n",
      "Current iteration=8, the loss=0.32384847022768937\n",
      "Current iteration=9, the loss=0.33541779165534796\n",
      "Current iteration=10, the loss=0.3455617050751535\n",
      "Current iteration=11, the loss=0.3544299176283214\n",
      "Current iteration=12, the loss=0.3621659262842122\n",
      "Current iteration=13, the loss=0.3689035898898817\n",
      "Current iteration=14, the loss=0.37476546878450256\n",
      "Current iteration=15, the loss=0.37986228621259693\n",
      "Current iteration=16, the loss=0.38429308317649025\n",
      "Current iteration=17, the loss=0.38814578379346726\n",
      "Current iteration=18, the loss=0.3914979855771141\n",
      "Current iteration=19, the loss=0.3944178542755158\n",
      "lambda :  3.56224789026\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.21827510302111555\n",
      "Current iteration=3, the loss=0.23999712051501096\n",
      "Current iteration=4, the loss=0.2600640931588938\n",
      "Current iteration=5, the loss=0.2782968472240466\n",
      "Current iteration=6, the loss=0.29466670574771536\n",
      "Current iteration=7, the loss=0.30923558820416547\n",
      "Current iteration=8, the loss=0.32211663162541576\n",
      "Current iteration=9, the loss=0.3334487100948567\n",
      "Current iteration=10, the loss=0.34338025564442654\n",
      "Current iteration=11, the loss=0.3520592514741374\n",
      "Current iteration=12, the loss=0.35962729027772\n",
      "Current iteration=13, the loss=0.3662162868452234\n",
      "Current iteration=14, the loss=0.3719469035130912\n",
      "Current iteration=15, the loss=0.3769280618385606\n",
      "Current iteration=16, the loss=0.381257124646625\n",
      "Current iteration=17, the loss=0.38502047372108616\n",
      "Current iteration=18, the loss=0.3882943029638759\n",
      "Current iteration=19, the loss=0.3911455102251975\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.21815371063796243\n",
      "Current iteration=3, the loss=0.24036340905053127\n",
      "Current iteration=4, the loss=0.26091545699676727\n",
      "Current iteration=5, the loss=0.27961372219659086\n",
      "Current iteration=6, the loss=0.29642008726006375\n",
      "Current iteration=7, the loss=0.3113918578138705\n",
      "Current iteration=8, the loss=0.3246406039252299\n",
      "Current iteration=9, the loss=0.3363055134736375\n",
      "Current iteration=10, the loss=0.3465364532408352\n",
      "Current iteration=11, the loss=0.35548346945820986\n",
      "Current iteration=12, the loss=0.3632905287111268\n",
      "Current iteration=13, the loss=0.37009202814536546\n",
      "Current iteration=14, the loss=0.37601109412452394\n",
      "Current iteration=15, the loss=0.3811590168567159\n",
      "Current iteration=16, the loss=0.385635388108256\n",
      "Current iteration=17, the loss=0.3895286560044393\n",
      "Current iteration=18, the loss=0.39291690924474765\n",
      "Current iteration=19, the loss=0.3958687689272996\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.21807149621363978\n",
      "Current iteration=3, the loss=0.23997419200050235\n",
      "Current iteration=4, the loss=0.2602238810923889\n",
      "Current iteration=5, the loss=0.2786350593346748\n",
      "Current iteration=6, the loss=0.29517511712368194\n",
      "Current iteration=7, the loss=0.30990368141865154\n",
      "Current iteration=8, the loss=0.3229327104633917\n",
      "Current iteration=9, the loss=0.3344006491122724\n",
      "Current iteration=10, the loss=0.34445599774587504\n",
      "Current iteration=11, the loss=0.353247130790378\n",
      "Current iteration=12, the loss=0.36091623369806286\n",
      "Current iteration=13, the loss=0.3675959312449983\n",
      "Current iteration=14, the loss=0.373407654570393\n",
      "Current iteration=15, the loss=0.37846111267201643\n",
      "Current iteration=16, the loss=0.3828544471920478\n",
      "Current iteration=17, the loss=0.38667479204322325\n",
      "Current iteration=18, the loss=0.38999905506629384\n",
      "Current iteration=19, the loss=0.39289480304045976\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.21808122467930569\n",
      "Current iteration=3, the loss=0.24016392561554475\n",
      "Current iteration=4, the loss=0.260586837991407\n",
      "Current iteration=5, the loss=0.27915886507370963\n",
      "Current iteration=6, the loss=0.2958449164239197\n",
      "Current iteration=7, the loss=0.3107039897830206\n",
      "Current iteration=8, the loss=0.3238484701760921\n",
      "Current iteration=9, the loss=0.3354177915932313\n",
      "Current iteration=10, the loss=0.3455617050026179\n",
      "Current iteration=11, the loss=0.3544299175456313\n",
      "Current iteration=12, the loss=0.3621659261917469\n",
      "Current iteration=13, the loss=0.3689035897881027\n",
      "Current iteration=14, the loss=0.3747654686739197\n",
      "Current iteration=15, the loss=0.3798622860937471\n",
      "Current iteration=16, the loss=0.3842930830499159\n",
      "Current iteration=17, the loss=0.38814578365970437\n",
      "Current iteration=18, the loss=0.39149798543668135\n",
      "Current iteration=19, the loss=0.3944178541289065\n",
      "lambda :  5.73615251045\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.21827510301735617\n",
      "Current iteration=3, the loss=0.2399971205038592\n",
      "Current iteration=4, the loss=0.2600640931372473\n",
      "Current iteration=5, the loss=0.27829684718947656\n",
      "Current iteration=6, the loss=0.2946667056984759\n",
      "Current iteration=7, the loss=0.3092355881391279\n",
      "Current iteration=8, the loss=0.32211663154398096\n",
      "Current iteration=9, the loss=0.33344870999685167\n",
      "Current iteration=10, the loss=0.3433802555300192\n",
      "Current iteration=11, the loss=0.35205925134374777\n",
      "Current iteration=12, the loss=0.3596272901319525\n",
      "Current iteration=13, the loss=0.3662162866848067\n",
      "Current iteration=14, the loss=0.3719469033388326\n",
      "Current iteration=15, the loss=0.37692806165130616\n",
      "Current iteration=16, the loss=0.38125712444722937\n",
      "Current iteration=17, the loss=0.3850204735103948\n",
      "Current iteration=18, the loss=0.3882943027427042\n",
      "Current iteration=19, the loss=0.39114550999432013\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.21815371063412284\n",
      "Current iteration=3, the loss=0.24036340903912207\n",
      "Current iteration=4, the loss=0.26091545697458934\n",
      "Current iteration=5, the loss=0.2796137221611348\n",
      "Current iteration=6, the loss=0.2964200872095169\n",
      "Current iteration=7, the loss=0.3113918577470572\n",
      "Current iteration=8, the loss=0.3246406038415162\n",
      "Current iteration=9, the loss=0.33630551337283365\n",
      "Current iteration=10, the loss=0.34653645312309955\n",
      "Current iteration=11, the loss=0.35548346932396513\n",
      "Current iteration=12, the loss=0.36329052856098615\n",
      "Current iteration=13, the loss=0.3700920279800733\n",
      "Current iteration=14, the loss=0.37601109394490556\n",
      "Current iteration=15, the loss=0.3811590166636396\n",
      "Current iteration=16, the loss=0.38563538790260116\n",
      "Current iteration=17, the loss=0.3895286557870781\n",
      "Current iteration=18, the loss=0.3929169090165209\n",
      "Current iteration=19, the loss=0.39586876868900783\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.2180714962098531\n",
      "Current iteration=3, the loss=0.23997419198926062\n",
      "Current iteration=4, the loss=0.2602238810705554\n",
      "Current iteration=5, the loss=0.2786350592997905\n",
      "Current iteration=6, the loss=0.29517511707397365\n",
      "Current iteration=7, the loss=0.30990368135297136\n",
      "Current iteration=8, the loss=0.32293271038112475\n",
      "Current iteration=9, the loss=0.3344006490132387\n",
      "Current iteration=10, the loss=0.3444559976302357\n",
      "Current iteration=11, the loss=0.3532471306585536\n",
      "Current iteration=12, the loss=0.36091623355066094\n",
      "Current iteration=13, the loss=0.36759593108275146\n",
      "Current iteration=14, the loss=0.3734076543941169\n",
      "Current iteration=15, the loss=0.3784611124825663\n",
      "Current iteration=16, the loss=0.3828544469902889\n",
      "Current iteration=17, the loss=0.38667479183000963\n",
      "Current iteration=18, the loss=0.3899990548424536\n",
      "Current iteration=19, the loss=0.39289480280677913\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.21808122467548685\n",
      "Current iteration=3, the loss=0.24016392560420202\n",
      "Current iteration=4, the loss=0.2605868379693683\n",
      "Current iteration=5, the loss=0.27915886503848886\n",
      "Current iteration=6, the loss=0.2958449163737254\n",
      "Current iteration=7, the loss=0.3107039897166922\n",
      "Current iteration=8, the loss=0.3238484700930062\n",
      "Current iteration=9, the loss=0.33541779149320633\n",
      "Current iteration=10, the loss=0.3455617048858175\n",
      "Current iteration=11, the loss=0.35442991741247765\n",
      "Current iteration=12, the loss=0.36216592604285436\n",
      "Current iteration=13, the loss=0.3689035896242126\n",
      "Current iteration=14, the loss=0.3747654684958525\n",
      "Current iteration=15, the loss=0.37986228590236654\n",
      "Current iteration=16, the loss=0.38429308284609703\n",
      "Current iteration=17, the loss=0.38814578344431055\n",
      "Current iteration=18, the loss=0.3914979852105476\n",
      "Current iteration=19, the loss=0.39441785389282663\n",
      "lambda :  9.23670857187\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.21827510301130196\n",
      "Current iteration=3, the loss=0.2399971204859018\n",
      "Current iteration=4, the loss=0.26006409310239004\n",
      "Current iteration=5, the loss=0.2782968471338096\n",
      "Current iteration=6, the loss=0.29466670561918695\n",
      "Current iteration=7, the loss=0.3092355880344008\n",
      "Current iteration=8, the loss=0.32211663141284785\n",
      "Current iteration=9, the loss=0.33344870983903874\n",
      "Current iteration=10, the loss=0.34338025534579253\n",
      "Current iteration=11, the loss=0.3520592511337855\n",
      "Current iteration=12, the loss=0.3596272898972276\n",
      "Current iteration=13, the loss=0.3662162864264935\n",
      "Current iteration=14, the loss=0.37194690305822864\n",
      "Current iteration=15, the loss=0.3769280613497756\n",
      "Current iteration=16, the loss=0.38125712412615137\n",
      "Current iteration=17, the loss=0.3850204731711272\n",
      "Current iteration=18, the loss=0.38829430238655943\n",
      "Current iteration=19, the loss=0.3911455096225471\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.21815371062794056\n",
      "Current iteration=3, the loss=0.24036340902074987\n",
      "Current iteration=4, the loss=0.26091545693887724\n",
      "Current iteration=5, the loss=0.27961372210404106\n",
      "Current iteration=6, the loss=0.29642008712812423\n",
      "Current iteration=7, the loss=0.3113918576394705\n",
      "Current iteration=8, the loss=0.32464060370671566\n",
      "Current iteration=9, the loss=0.3363055132105136\n",
      "Current iteration=10, the loss=0.34653645293351554\n",
      "Current iteration=11, the loss=0.35548346910779566\n",
      "Current iteration=12, the loss=0.3632905283192201\n",
      "Current iteration=13, the loss=0.370092027713909\n",
      "Current iteration=14, the loss=0.3760110936556713\n",
      "Current iteration=15, the loss=0.381159016352735\n",
      "Current iteration=16, the loss=0.3856353875714443\n",
      "Current iteration=17, the loss=0.3895286554370689\n",
      "Current iteration=18, the loss=0.39291690864901474\n",
      "Current iteration=19, the loss=0.3958687683052951\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.21807149620375466\n",
      "Current iteration=3, the loss=0.23997419197115932\n",
      "Current iteration=4, the loss=0.2602238810353983\n",
      "Current iteration=5, the loss=0.27863505924361726\n",
      "Current iteration=6, the loss=0.2951751169939306\n",
      "Current iteration=7, the loss=0.3099036812472091\n",
      "Current iteration=8, the loss=0.3229327102486536\n",
      "Current iteration=9, the loss=0.33440064885376797\n",
      "Current iteration=10, the loss=0.3444559974440276\n",
      "Current iteration=11, the loss=0.3532471304462827\n",
      "Current iteration=12, the loss=0.360916233313305\n",
      "Current iteration=13, the loss=0.3675959308214938\n",
      "Current iteration=14, the loss=0.3734076541102668\n",
      "Current iteration=15, the loss=0.37846111217750117\n",
      "Current iteration=16, the loss=0.3828544466654044\n",
      "Current iteration=17, the loss=0.38667479148668193\n",
      "Current iteration=18, the loss=0.3899990544820127\n",
      "Current iteration=19, the loss=0.39289480243049174\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.2180812246693369\n",
      "Current iteration=3, the loss=0.2401639255859365\n",
      "Current iteration=4, the loss=0.2605868379338804\n",
      "Current iteration=5, the loss=0.2791588649817759\n",
      "Current iteration=6, the loss=0.2958449162929004\n",
      "Current iteration=7, the loss=0.31070398960988577\n",
      "Current iteration=8, the loss=0.32384846995921773\n",
      "Current iteration=9, the loss=0.33541779133214145\n",
      "Current iteration=10, the loss=0.34556170469773845\n",
      "Current iteration=11, the loss=0.3544299171980666\n",
      "Current iteration=12, the loss=0.3621659258030981\n",
      "Current iteration=13, the loss=0.36890358936030504\n",
      "Current iteration=14, the loss=0.374765468209118\n",
      "Current iteration=15, the loss=0.37986228559419405\n",
      "Current iteration=16, the loss=0.38429308251789623\n",
      "Current iteration=17, the loss=0.3881457830974711\n",
      "Current iteration=18, the loss=0.3914979848464138\n",
      "Current iteration=19, the loss=0.3944178535126768\n",
      "lambda :  14.8735210729\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.2182751030015538\n",
      "Current iteration=3, the loss=0.23999712045698599\n",
      "Current iteration=4, the loss=0.26006409304626016\n",
      "Current iteration=5, the loss=0.2782968470441716\n",
      "Current iteration=6, the loss=0.2946667054915112\n",
      "Current iteration=7, the loss=0.30923558786576183\n",
      "Current iteration=8, the loss=0.32211663120168915\n",
      "Current iteration=9, the loss=0.3334487095849177\n",
      "Current iteration=10, the loss=0.3433802550491399\n",
      "Current iteration=11, the loss=0.35205925079569184\n",
      "Current iteration=12, the loss=0.35962728951925943\n",
      "Current iteration=13, the loss=0.36621628601054135\n",
      "Current iteration=14, the loss=0.37194690260638485\n",
      "Current iteration=15, the loss=0.37692806086423275\n",
      "Current iteration=16, the loss=0.38125712360913\n",
      "Current iteration=17, the loss=0.38502047262481587\n",
      "Current iteration=18, the loss=0.3882943018130731\n",
      "Current iteration=19, the loss=0.39114550902389505\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.21815371061798544\n",
      "Current iteration=3, the loss=0.240363408991165\n",
      "Current iteration=4, the loss=0.26091545688137047\n",
      "Current iteration=5, the loss=0.27961372201210494\n",
      "Current iteration=6, the loss=0.2964200869970597\n",
      "Current iteration=7, the loss=0.31139185746622716\n",
      "Current iteration=8, the loss=0.3246406034896521\n",
      "Current iteration=9, the loss=0.33630551294913663\n",
      "Current iteration=10, the loss=0.3465364526282351\n",
      "Current iteration=11, the loss=0.3554834687597074\n",
      "Current iteration=12, the loss=0.36329052792991423\n",
      "Current iteration=13, the loss=0.3700920272853139\n",
      "Current iteration=14, the loss=0.3760110931899293\n",
      "Current iteration=15, the loss=0.38115901585209727\n",
      "Current iteration=16, the loss=0.3856353870381946\n",
      "Current iteration=17, the loss=0.38952865487346283\n",
      "Current iteration=18, the loss=0.39291690805723456\n",
      "Current iteration=19, the loss=0.3958687676874181\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.2180714961939351\n",
      "Current iteration=3, the loss=0.23997419194201122\n",
      "Current iteration=4, the loss=0.2602238809787854\n",
      "Current iteration=5, the loss=0.2786350591531635\n",
      "Current iteration=6, the loss=0.29517511686503967\n",
      "Current iteration=7, the loss=0.30990368107690447\n",
      "Current iteration=8, the loss=0.32293271003534074\n",
      "Current iteration=9, the loss=0.33440064859698\n",
      "Current iteration=10, the loss=0.34445599714418296\n",
      "Current iteration=11, the loss=0.3532471301044711\n",
      "Current iteration=12, the loss=0.36091623293109915\n",
      "Current iteration=13, the loss=0.3675959304007992\n",
      "Current iteration=14, the loss=0.3734076536531939\n",
      "Current iteration=15, the loss=0.378461111686267\n",
      "Current iteration=16, the loss=0.3828544461422547\n",
      "Current iteration=17, the loss=0.3866747909338332\n",
      "Current iteration=18, the loss=0.3899990539016086\n",
      "Current iteration=19, the loss=0.39289480182457\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.2180812246594338\n",
      "Current iteration=3, the loss=0.24016392555652494\n",
      "Current iteration=4, the loss=0.260586837876736\n",
      "Current iteration=5, the loss=0.27915886489045166\n",
      "Current iteration=6, the loss=0.29584491616275077\n",
      "Current iteration=7, the loss=0.3107039894378992\n",
      "Current iteration=8, the loss=0.3238484697437819\n",
      "Current iteration=9, the loss=0.33541779107278347\n",
      "Current iteration=10, the loss=0.3455617043948809\n",
      "Current iteration=11, the loss=0.35442991685280856\n",
      "Current iteration=12, the loss=0.36216592541702736\n",
      "Current iteration=13, the loss=0.3689035889353444\n",
      "Current iteration=14, the loss=0.37476546774739944\n",
      "Current iteration=15, the loss=0.3798622850979559\n",
      "Current iteration=16, the loss=0.38429308198940587\n",
      "Current iteration=17, the loss=0.38814578253896787\n",
      "Current iteration=18, the loss=0.3914979842600621\n",
      "Current iteration=19, the loss=0.3944178529005362\n",
      "lambda :  23.9502661999\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.21827510298585653\n",
      "Current iteration=3, the loss=0.23999712041042418\n",
      "Current iteration=4, the loss=0.26006409295587757\n",
      "Current iteration=5, the loss=0.2782968468998306\n",
      "Current iteration=6, the loss=0.29466670528591987\n",
      "Current iteration=7, the loss=0.30923558759421044\n",
      "Current iteration=8, the loss=0.32211663086166914\n",
      "Current iteration=9, the loss=0.33344870917571656\n",
      "Current iteration=10, the loss=0.3433802545714517\n",
      "Current iteration=11, the loss=0.35205925025127227\n",
      "Current iteration=12, the loss=0.3596272889106311\n",
      "Current iteration=13, the loss=0.3662162853407505\n",
      "Current iteration=14, the loss=0.37194690187879687\n",
      "Current iteration=15, the loss=0.37692806008238194\n",
      "Current iteration=16, the loss=0.38125712277659124\n",
      "Current iteration=17, the loss=0.3850204717451124\n",
      "Current iteration=18, the loss=0.3882943008896108\n",
      "Current iteration=19, the loss=0.3911455080599082\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.21815371060195496\n",
      "Current iteration=3, the loss=0.2403634089435267\n",
      "Current iteration=4, the loss=0.2609154567887708\n",
      "Current iteration=5, the loss=0.27961372186406386\n",
      "Current iteration=6, the loss=0.2964200867860116\n",
      "Current iteration=7, the loss=0.31139185718726103\n",
      "Current iteration=8, the loss=0.32464060314012216\n",
      "Current iteration=9, the loss=0.33630551252824964\n",
      "Current iteration=10, the loss=0.3465364521366535\n",
      "Current iteration=11, the loss=0.35548346819919363\n",
      "Current iteration=12, the loss=0.36329052730302863\n",
      "Current iteration=13, the loss=0.37009202659516427\n",
      "Current iteration=14, the loss=0.3760110924399629\n",
      "Current iteration=15, the loss=0.38115901504593935\n",
      "Current iteration=16, the loss=0.3856353861795231\n",
      "Current iteration=17, the loss=0.3895286539659084\n",
      "Current iteration=18, the loss=0.3929169071043123\n",
      "Current iteration=19, the loss=0.3958687666924739\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.218071496178123\n",
      "Current iteration=3, the loss=0.23997419189507513\n",
      "Current iteration=4, the loss=0.260223880887624\n",
      "Current iteration=5, the loss=0.2786350590075093\n",
      "Current iteration=6, the loss=0.2951751166574921\n",
      "Current iteration=7, the loss=0.3099036808026698\n",
      "Current iteration=8, the loss=0.3229327096918504\n",
      "Current iteration=9, the loss=0.33440064818348375\n",
      "Current iteration=10, the loss=0.3444559966613544\n",
      "Current iteration=11, the loss=0.3532471295540648\n",
      "Current iteration=12, the loss=0.3609162323156491\n",
      "Current iteration=13, the loss=0.36759592972337113\n",
      "Current iteration=14, the loss=0.3734076529171871\n",
      "Current iteration=15, the loss=0.37846111089524986\n",
      "Current iteration=16, the loss=0.38285444529984614\n",
      "Current iteration=17, the loss=0.3866747900436029\n",
      "Current iteration=18, the loss=0.3899990529670054\n",
      "Current iteration=19, the loss=0.39289480084887807\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.2180812246434874\n",
      "Current iteration=3, the loss=0.24016392550916393\n",
      "Current iteration=4, the loss=0.260586837784718\n",
      "Current iteration=5, the loss=0.2791588647433961\n",
      "Current iteration=6, the loss=0.2958449159531746\n",
      "Current iteration=7, the loss=0.31070398916095543\n",
      "Current iteration=8, the loss=0.32384846939687323\n",
      "Current iteration=9, the loss=0.3354177906551489\n",
      "Current iteration=10, the loss=0.3455617039072001\n",
      "Current iteration=11, the loss=0.35442991629685167\n",
      "Current iteration=12, the loss=0.36216592479535226\n",
      "Current iteration=13, the loss=0.3689035882510478\n",
      "Current iteration=14, the loss=0.3747654670039111\n",
      "Current iteration=15, the loss=0.37986228429888197\n",
      "Current iteration=16, the loss=0.3842930811383996\n",
      "Current iteration=17, the loss=0.3881457816396317\n",
      "Current iteration=18, the loss=0.3914979833158827\n",
      "Current iteration=19, the loss=0.3944178519148293\n",
      "lambda :  38.5662042116\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.21827510296057925\n",
      "Current iteration=3, the loss=0.23999712033544637\n",
      "Current iteration=4, the loss=0.26006409281033765\n",
      "Current iteration=5, the loss=0.2782968466674042\n",
      "Current iteration=6, the loss=0.29466670495486247\n",
      "Current iteration=7, the loss=0.30923558715694005\n",
      "Current iteration=8, the loss=0.3221166303141477\n",
      "Current iteration=9, the loss=0.33344870851679553\n",
      "Current iteration=10, the loss=0.34338025380224835\n",
      "Current iteration=11, the loss=0.35205924937461397\n",
      "Current iteration=12, the loss=0.35962728793058063\n",
      "Current iteration=13, the loss=0.36621628426221\n",
      "Current iteration=14, the loss=0.37194690070718994\n",
      "Current iteration=15, the loss=0.3769280588233965\n",
      "Current iteration=16, the loss=0.381257121435985\n",
      "Current iteration=17, the loss=0.3850204703285601\n",
      "Current iteration=18, the loss=0.3882942994025935\n",
      "Current iteration=19, the loss=0.39114550650763585\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.21815371057614166\n",
      "Current iteration=3, the loss=0.24036340886681568\n",
      "Current iteration=4, the loss=0.2609154566396609\n",
      "Current iteration=5, the loss=0.27961372162567866\n",
      "Current iteration=6, the loss=0.2964200864461701\n",
      "Current iteration=7, the loss=0.3113918567380516\n",
      "Current iteration=8, the loss=0.32464060257728716\n",
      "Current iteration=9, the loss=0.336305511850512\n",
      "Current iteration=10, the loss=0.34653645134507877\n",
      "Current iteration=11, the loss=0.3554834672966198\n",
      "Current iteration=12, the loss=0.36329052629357983\n",
      "Current iteration=13, the loss=0.3700920254838418\n",
      "Current iteration=14, the loss=0.3760110912323194\n",
      "Current iteration=15, the loss=0.38115901374781447\n",
      "Current iteration=16, the loss=0.3856353847968375\n",
      "Current iteration=17, the loss=0.38952865250450897\n",
      "Current iteration=18, the loss=0.39291690556985825\n",
      "Current iteration=19, the loss=0.3958687650903525\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.21807149615266144\n",
      "Current iteration=3, the loss=0.2399741918194957\n",
      "Current iteration=4, the loss=0.26022388074083036\n",
      "Current iteration=5, the loss=0.27863505877296857\n",
      "Current iteration=6, the loss=0.29517511632328597\n",
      "Current iteration=7, the loss=0.3099036803610797\n",
      "Current iteration=8, the loss=0.3229327091387419\n",
      "Current iteration=9, the loss=0.3344006475176459\n",
      "Current iteration=10, the loss=0.3444559958838745\n",
      "Current iteration=11, the loss=0.3532471286677659\n",
      "Current iteration=12, the loss=0.3609162313246121\n",
      "Current iteration=13, the loss=0.3675959286325346\n",
      "Current iteration=14, the loss=0.3734076517320227\n",
      "Current iteration=15, the loss=0.3784611096215075\n",
      "Current iteration=16, the loss=0.3828544439433489\n",
      "Current iteration=17, the loss=0.3866747886100985\n",
      "Current iteration=18, the loss=0.3899990514620509\n",
      "Current iteration=19, the loss=0.3928947992777586\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.21808122461780946\n",
      "Current iteration=3, the loss=0.2401639254329005\n",
      "Current iteration=4, the loss=0.2605868376365454\n",
      "Current iteration=5, the loss=0.2791588645065986\n",
      "Current iteration=6, the loss=0.2958449156157023\n",
      "Current iteration=7, the loss=0.3107039887150036\n",
      "Current iteration=8, the loss=0.32384846883826074\n",
      "Current iteration=9, the loss=0.33541778998264876\n",
      "Current iteration=10, the loss=0.3455617031219075\n",
      "Current iteration=11, the loss=0.35442991540161584\n",
      "Current iteration=12, the loss=0.36216592379429324\n",
      "Current iteration=13, the loss=0.3689035871491504\n",
      "Current iteration=14, the loss=0.3747654658067017\n",
      "Current iteration=15, the loss=0.3798622830121639\n",
      "Current iteration=16, the loss=0.3842930797680542\n",
      "Current iteration=17, the loss=0.38814578019146445\n",
      "Current iteration=18, the loss=0.3914979817955057\n",
      "Current iteration=19, the loss=0.394417850327582\n",
      "lambda :  62.1016941892\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.218275102919877\n",
      "Current iteration=3, the loss=0.2399971202147137\n",
      "Current iteration=4, the loss=0.26006409257597946\n",
      "Current iteration=5, the loss=0.27829684629313656\n",
      "Current iteration=6, the loss=0.2946667044217755\n",
      "Current iteration=7, the loss=0.3092355864528212\n",
      "Current iteration=8, the loss=0.32211662943249514\n",
      "Current iteration=9, the loss=0.33344870745575955\n",
      "Current iteration=10, the loss=0.34338025256362914\n",
      "Current iteration=11, the loss=0.352059247962965\n",
      "Current iteration=12, the loss=0.35962728635244323\n",
      "Current iteration=13, the loss=0.36621628252547866\n",
      "Current iteration=14, the loss=0.3719468988205955\n",
      "Current iteration=15, the loss=0.37692805679610064\n",
      "Current iteration=16, the loss=0.38125711927725736\n",
      "Current iteration=17, the loss=0.38502046804753925\n",
      "Current iteration=18, the loss=0.38829429700810636\n",
      "Current iteration=19, the loss=0.3911455040080717\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.21815371053457533\n",
      "Current iteration=3, the loss=0.24036340874329148\n",
      "Current iteration=4, the loss=0.2609154563995544\n",
      "Current iteration=5, the loss=0.279613721241817\n",
      "Current iteration=6, the loss=0.296420085898935\n",
      "Current iteration=7, the loss=0.3113918560147066\n",
      "Current iteration=8, the loss=0.32464060167097597\n",
      "Current iteration=9, the loss=0.3363055107591765\n",
      "Current iteration=10, the loss=0.34653645007043504\n",
      "Current iteration=11, the loss=0.35548346584323887\n",
      "Current iteration=12, the loss=0.36329052466810147\n",
      "Current iteration=13, the loss=0.370092023694322\n",
      "Current iteration=14, the loss=0.37601108928769816\n",
      "Current iteration=15, the loss=0.3811590116574927\n",
      "Current iteration=16, the loss=0.38563538257034974\n",
      "Current iteration=17, the loss=0.389528650151273\n",
      "Current iteration=18, the loss=0.3929169030989859\n",
      "Current iteration=19, the loss=0.3958687625105178\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.2180714961116621\n",
      "Current iteration=3, the loss=0.23997419169779285\n",
      "Current iteration=4, the loss=0.26022388050445455\n",
      "Current iteration=5, the loss=0.2786350583952957\n",
      "Current iteration=6, the loss=0.29517511578512684\n",
      "Current iteration=7, the loss=0.30990367965000376\n",
      "Current iteration=8, the loss=0.3229327082480915\n",
      "Current iteration=9, the loss=0.33440064644547196\n",
      "Current iteration=10, the loss=0.3444559946319281\n",
      "Current iteration=11, the loss=0.3532471272405928\n",
      "Current iteration=12, the loss=0.3609162297287826\n",
      "Current iteration=13, the loss=0.3675959268760006\n",
      "Current iteration=14, the loss=0.37340764982359737\n",
      "Current iteration=15, the loss=0.37846110757044704\n",
      "Current iteration=16, the loss=0.38285444175903205\n",
      "Current iteration=17, the loss=0.3866747863017792\n",
      "Current iteration=18, the loss=0.3899990490386786\n",
      "Current iteration=19, the loss=0.3928947967478436\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.21808122457646162\n",
      "Current iteration=3, the loss=0.24016392531009637\n",
      "Current iteration=4, the loss=0.26058683739794813\n",
      "Current iteration=5, the loss=0.2791588641252922\n",
      "Current iteration=6, the loss=0.2958449150722841\n",
      "Current iteration=7, the loss=0.3107039879969052\n",
      "Current iteration=8, the loss=0.3238484679387471\n",
      "Current iteration=9, the loss=0.33541778889974655\n",
      "Current iteration=10, the loss=0.34556170185737994\n",
      "Current iteration=11, the loss=0.35442991396005236\n",
      "Current iteration=12, the loss=0.36216592218232485\n",
      "Current iteration=13, the loss=0.36890358537480694\n",
      "Current iteration=14, the loss=0.3747654638788797\n",
      "Current iteration=15, the loss=0.37986228094021124\n",
      "Current iteration=16, the loss=0.38429307756144104\n",
      "Current iteration=17, the loss=0.3881457778595364\n",
      "Current iteration=18, the loss=0.39149797934730074\n",
      "Current iteration=19, the loss=0.39441784777169764\n",
      "lambda :  100.0\n",
      "Current iteration=0, the loss=0.1719146666666667\n",
      "Current iteration=1, the loss=0.19531734564883885\n",
      "Current iteration=2, the loss=0.21827510285433546\n",
      "Current iteration=3, the loss=0.23999712002030188\n",
      "Current iteration=4, the loss=0.26006409219860216\n",
      "Current iteration=5, the loss=0.2782968456904664\n",
      "Current iteration=6, the loss=0.29466670356336516\n",
      "Current iteration=7, the loss=0.30923558531900414\n",
      "Current iteration=8, the loss=0.3221166280128026\n",
      "Current iteration=9, the loss=0.333448705747214\n",
      "Current iteration=10, the loss=0.3433802505691281\n",
      "Current iteration=11, the loss=0.3520592456898397\n",
      "Current iteration=12, the loss=0.35962728381122705\n",
      "Current iteration=13, the loss=0.3662162797288848\n",
      "Current iteration=14, the loss=0.37194689578268525\n",
      "Current iteration=15, the loss=0.37692805353162395\n",
      "Current iteration=16, the loss=0.38125711580114036\n",
      "Current iteration=17, the loss=0.3850204643744979\n",
      "Current iteration=18, the loss=0.38829429315235486\n",
      "Current iteration=19, the loss=0.3911454999831181\n",
      "Current iteration=0, the loss=0.17094133333333333\n",
      "Current iteration=1, the loss=0.19473414833958835\n",
      "Current iteration=2, the loss=0.21815371046764273\n",
      "Current iteration=3, the loss=0.2403634085443854\n",
      "Current iteration=4, the loss=0.26091545601291966\n",
      "Current iteration=5, the loss=0.27961372062369805\n",
      "Current iteration=6, the loss=0.2964200850177429\n",
      "Current iteration=7, the loss=0.31139185484993287\n",
      "Current iteration=8, the loss=0.32464060021157715\n",
      "Current iteration=9, the loss=0.33630550900184125\n",
      "Current iteration=10, the loss=0.3465364480179259\n",
      "Current iteration=11, the loss=0.35548346350291565\n",
      "Current iteration=12, the loss=0.3632905220506561\n",
      "Current iteration=13, the loss=0.37009202081272646\n",
      "Current iteration=14, the loss=0.3760110861563477\n",
      "Current iteration=15, the loss=0.3811590082915269\n",
      "Current iteration=16, the loss=0.385635378985122\n",
      "Current iteration=17, the loss=0.38952864636194706\n",
      "Current iteration=18, the loss=0.39291689912023414\n",
      "Current iteration=19, the loss=0.39586875835630814\n",
      "Current iteration=0, the loss=0.1714\n",
      "Current iteration=1, the loss=0.1949449036105963\n",
      "Current iteration=2, the loss=0.21807149604564185\n",
      "Current iteration=3, the loss=0.23997419150181967\n",
      "Current iteration=4, the loss=0.2602238801238265\n",
      "Current iteration=5, the loss=0.27863505778714365\n",
      "Current iteration=6, the loss=0.29517511491854964\n",
      "Current iteration=7, the loss=0.3099036785049858\n",
      "Current iteration=8, the loss=0.3229327068139109\n",
      "Current iteration=9, the loss=0.33440064471899184\n",
      "Current iteration=10, the loss=0.34445599261596593\n",
      "Current iteration=11, the loss=0.3532471249424702\n",
      "Current iteration=12, the loss=0.3609162271590792\n",
      "Current iteration=13, the loss=0.36759592404752256\n",
      "Current iteration=14, the loss=0.37340764675053245\n",
      "Current iteration=15, the loss=0.37846110426770113\n",
      "Current iteration=16, the loss=0.3828544382417091\n",
      "Current iteration=17, the loss=0.3866747825847814\n",
      "Current iteration=18, the loss=0.38999904513641465\n",
      "Current iteration=19, the loss=0.39289479267401856\n",
      "Current iteration=0, the loss=0.17108\n",
      "Current iteration=1, the loss=0.1947785024591774\n",
      "Current iteration=2, the loss=0.21808122450987968\n",
      "Current iteration=3, the loss=0.24016392511234977\n",
      "Current iteration=4, the loss=0.26058683701374424\n",
      "Current iteration=5, the loss=0.27915886351128966\n",
      "Current iteration=6, the loss=0.2958449141972385\n",
      "Current iteration=7, the loss=0.31070398684057804\n",
      "Current iteration=8, the loss=0.32384846649029503\n",
      "Current iteration=9, the loss=0.3354177871559909\n",
      "Current iteration=10, the loss=0.34556169982116003\n",
      "Current iteration=11, the loss=0.3544299116387561\n",
      "Current iteration=12, the loss=0.36216591958663447\n",
      "Current iteration=13, the loss=0.3689035825176497\n",
      "Current iteration=14, the loss=0.3747654607745815\n",
      "Current iteration=15, the loss=0.3798622776038242\n",
      "Current iteration=16, the loss=0.38429307400821444\n",
      "Current iteration=17, the loss=0.3881457741045208\n",
      "Current iteration=18, the loss=0.39149797540504944\n",
      "Current iteration=19, the loss=0.3944178436560555\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAEdCAYAAAA4rdFEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlXB+gH4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1818015ef28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "initial_w=np.zeros(tX.shape[1])\n",
    "gamma=0.00000000003\n",
    "max_iters=20\n",
    "function_to_test=partial(reg_logistic_regression_with_mse,gamma=gamma, max_iters=max_iters)\n",
    "k_fold=4\n",
    "lambdas = np.logspace(-4, 2, 30)\n",
    "loss_tr,loss_te=finding_lambda(y0,tX_norm,function_to_test,k_fold,1,lambdas,compute_cost)\n",
    "from helpers import *\n",
    "cross_validation_visualization(lambdas, loss_tr, loss_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuck\n",
      "Current iteration=0, the loss=5776.226504666208\n",
      "Current iteration=1, the loss=5767.788695262626\n",
      "Current iteration=2, the loss=5759.648524389578\n",
      "Current iteration=3, the loss=5751.794141213292\n",
      "Current iteration=4, the loss=5744.214153747757\n",
      "Current iteration=5, the loss=5736.897613393705\n",
      "Current iteration=6, the loss=5729.833999739183\n",
      "Current iteration=7, the loss=5723.0132056469565\n",
      "Current iteration=8, the loss=5716.425522649806\n",
      "Current iteration=9, the loss=5710.061626671352\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_tr,w=logistic_regression(y0,tX_norm,0.000000000003,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20060917479199633"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cost(y0,tX_norm,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuck\n",
      "Current iteration=0, the loss=5776.226504666208\n",
      "Current iteration=1, the loss=5767.788695262626\n",
      "Current iteration=2, the loss=5759.648524389578\n",
      "Current iteration=3, the loss=5751.794141213292\n",
      "Current iteration=4, the loss=5744.214153747757\n",
      "Current iteration=5, the loss=5736.897613393705\n",
      "Current iteration=6, the loss=5729.833999739183\n",
      "Current iteration=7, the loss=5723.0132056469565\n",
      "Current iteration=8, the loss=5716.425522649806\n",
      "Current iteration=9, the loss=5710.061626671352\n",
      "Current iteration=10, the loss=5703.912564086559\n",
      "Current iteration=11, the loss=5697.969738133162\n",
      "Current iteration=12, the loss=5692.224895682556\n",
      "Current iteration=13, the loss=5686.67011437626\n",
      "Current iteration=14, the loss=5681.297790131872\n",
      "Current iteration=15, the loss=5676.1006250205555\n",
      "Current iteration=16, the loss=5671.07161551636\n",
      "Current iteration=17, the loss=5666.204041116199\n",
      "Current iteration=18, the loss=5661.491453327911\n",
      "Current iteration=19, the loss=5656.927665022848\n"
     ]
    }
   ],
   "source": [
    "loss_tr,w=logistic_regression(y0,tX_norm,0.000000000003,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22893432655340387"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cost(y0,tX_norm,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuck\n",
      "Current iteration=0, the loss=173286.79513998624\n",
      "Current iteration=1, the loss=-2725323.636153823\n",
      "Current iteration=2, the loss=-3971997.8492310923\n",
      "Current iteration=3, the loss=-5216589.725502851\n",
      "Current iteration=4, the loss=-6459791.385457025\n",
      "Current iteration=5, the loss=-7702040.284123588\n",
      "Current iteration=6, the loss=-8943607.951261204\n",
      "Current iteration=7, the loss=-10184668.000161486\n",
      "Current iteration=8, the loss=-11425336.560482025\n",
      "Current iteration=9, the loss=-12665694.868688699\n",
      "Current iteration=10, the loss=-13905802.001533171\n",
      "Current iteration=11, the loss=-15145702.310699677\n",
      "Current iteration=12, the loss=-16385429.957667593\n",
      "Current iteration=13, the loss=-17625011.804495722\n",
      "Current iteration=14, the loss=-18864469.332928155\n",
      "Current iteration=15, the loss=-20103819.964558944\n",
      "Current iteration=16, the loss=-21343077.996924095\n",
      "Current iteration=17, the loss=-22582255.28439947\n",
      "Current iteration=18, the loss=-23821361.744191248\n",
      "Current iteration=19, the loss=-25060405.73920478\n"
     ]
    }
   ],
   "source": [
    "w=np.zeros((tX.shape[1], 1))#.ravel()\n",
    "#ok = general_gradient_descent(y,tX,w,50,0.00000000003,compute_gradient_sigmoid,compute_loss_logistic)\n",
    "ok2 = logistic_regression(y,tX,0.00000000003,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.69314718  0.69314718  0.69314718 ...,  0.69314718  0.69314718\n",
      "  0.69314718]\n",
      "Current iteration=0, the loss=0.171334\n",
      "Current iteration=1, the loss=0.1713333932443948\n",
      "Current iteration=2, the loss=0.17133278649535727\n",
      "Current iteration=3, the loss=0.17133217975288226\n",
      "Current iteration=4, the loss=0.1713315730169729\n",
      "Current iteration=5, the loss=0.17133096628762776\n",
      "Current iteration=6, the loss=0.17133035956484663\n",
      "Current iteration=7, the loss=0.17132975284863\n",
      "Current iteration=8, the loss=0.17132914613897757\n",
      "Current iteration=9, the loss=0.17132853943588938\n",
      "Current iteration=10, the loss=0.1713279327393654\n",
      "Current iteration=11, the loss=0.17132732604940526\n",
      "Current iteration=12, the loss=0.17132671936600968\n",
      "Current iteration=13, the loss=0.1713261126891773\n",
      "Current iteration=14, the loss=0.1713255060189094\n",
      "Current iteration=15, the loss=0.171324899355205\n",
      "Current iteration=16, the loss=0.1713242926980641\n",
      "Current iteration=17, the loss=0.17132368604748757\n",
      "Current iteration=18, the loss=0.1713230794034737\n",
      "Current iteration=19, the loss=0.17132247276602394\n"
     ]
    }
   ],
   "source": [
    "w=np.zeros((tX.shape[1], 1)).ravel()\n",
    "y2 = y.copy()\n",
    "y2[np.where(y==-1)] = 0\n",
    "test = np.clip(tX @ w, -700, 700)\n",
    "zbub = compute_loss_logistic(y2,tX,w,0.03)\n",
    "zbub2 = compute_gradient_sigmoid(y2,tX,w,0.03)\n",
    "tX2 = tX.copy()\n",
    "tX2[np.where(tX2==-999)] = 0\n",
    "e=y-(tX @ w)\n",
    "print(np.log(1+np.exp(test))-y*(tX @ w))\n",
    "#ok = general_gradient_descent(y2,tX2,w,50,0.00000000003,compute_gradient_sigmoid,compute_loss_logistic)\n",
    "ok = least_squares_GD(y2,tX2,0.00000000003,20)\n",
    "#ok = general_gradient_descent(y2,tX,w,20,0.0000003,compute_gradient_MSE,compute_cost_MSE)\n",
    "#ok = general_gradient_descent(y2,tX,w,10,-0.003,compute_gradient_MSE,compute_cost_MSE)\n",
    "#print((tX @ w).shape,zbub,zbub2,ok)\n",
    "#print(txs.shape,y.shape[1])\n",
    "#compute_loss_logistic(y,test,w)\n",
    "\n",
    "#a = compute_loss_logistic(y,tX,w)\n",
    "#print(txs,sigmoid(-700))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# _ ,weights = least_squares(y,tX)\n",
    "losss2,weights2 = least_squares(y,tX_norm)\n",
    "losss,weights = least_squares(y,tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.342334045239 [  2.01408328e-04  -9.37679313e-03   3.08279700e-03  -5.53801577e-04\n",
      "   1.56763588e-02  -3.20654044e-04  -2.34149841e-02  -1.79819855e-02\n",
      "  -3.84894764e-03  -4.68513987e-04  -1.97319796e-01   1.03865892e-01\n",
      "   3.42139596e-01   5.66352463e-03  -4.08214643e-05  -1.15707948e-03\n",
      "   2.74322661e-03  -1.33201704e-03   1.02510089e-03   7.22615826e-04\n",
      "   3.49079745e-04  -1.74267650e-04  -1.66379987e-02  -4.26939012e-04\n",
      "  -2.96151899e-05   9.58107528e-05  -5.92882391e-04   1.17377665e-03\n",
      "  -1.22375626e-03   5.91560308e-04]\n"
     ]
    }
   ],
   "source": [
    "print(losss2,weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([    66,     70,    103, ..., 249935, 249946, 249994], dtype=int64),)\n",
      "-1.0\n",
      "1.0\n",
      "0.504049409039\n",
      "-0.023087736707\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = np.dot(tX, weights2)*2-1\n",
    "y_pred2[np.where(y_pred2 <= 0)] = -1\n",
    "y_pred2[np.where(y_pred2 > 0)] = 1\n",
    "y_pred = np.dot(tX, weights)\n",
    "y_pred[np.where(y_pred <= 0)] = -1\n",
    "y_pred[np.where(y_pred > 0)] = 1\n",
    "print (np.where((y_pred2-y_pred!=0)))\n",
    "print(y_pred[66])\n",
    "print(y_pred2[66])\n",
    "print(np.dot(tX, weights2)[66])\n",
    "print(np.dot(tX, weights)[66])   #Very weird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_labels_modified_y(weights, data):\n",
    "    \"\"\"Generates class predictions given weights, and a test data matrix when y was 0 or 1 for the weights\"\"\"\n",
    "    y_pred = np.dot(data, weights)*2-1\n",
    "    y_pred[np.where(y_pred <= 0)] = -1\n",
    "    y_pred[np.where(y_pred > 0)] = 1\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tX_test_norm=remove_outliers(tX_test,mean_x,std_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../submission.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights2, tX_test_norm)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
