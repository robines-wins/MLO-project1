{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from helpers import *\n",
    "from costs import *\n",
    "DATA_TRAIN_PATH = '../train.csv' # TODO: download train data and supply path here \n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stdx, mean_x, std_x=standardize(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., -1., -1., ...,  1., -1., -1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional loss and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return np.exp(t)/(1+np.exp(t))\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    poly=np.ones((x.shape[0],degree+1,x.shape[1]))\n",
    "    for i in range(0,x.shape[0]):\n",
    "        if i%100000==0: print(\"Building polynomial \", i)\n",
    "        for d in range(1, degree+1):\n",
    "            for j in range(0,x.shape[1]):\n",
    "                poly[i][d][j]=np.power(x[i,j],d)\n",
    "    return poly\n",
    "\n",
    "def compute_rmse(y,tx,w):\n",
    "    e=y-(tx @ w)\n",
    "    return math.sqrt(1/y.shape[0]*(e @ e))\n",
    "\n",
    "\n",
    "#def calculate_loss_logistic_regression(y, tx, w):\n",
    "#    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "#    a=0\n",
    "#    for n in range(y.shape[0]):    \n",
    "#        a+=np.log(1+np.exp(tx[n].T @ w))-y[n]*tx[n].T @ w\n",
    "#    return a\n",
    "\n",
    "def build_tx(x):\n",
    "    return np.c_[np.ones((x.shape[0], 1)), x]\n",
    "\n",
    "\n",
    "def compute_loss_poly(train_y,train_tx,weight):\n",
    "    w=weight.T   # We will process by degree not by dim\n",
    "    y_predicted=np.zeros(train_y.shape[0])\n",
    "    y_predicted_classed=np.ones(train_y.shape[0])\n",
    "    for n in range(0,train_tx.shape[0]):\n",
    "        if(n%50000==0):print(\"sample \",n)\n",
    "        for degree in range(0,train_tx.shape[1]):\n",
    "            y_predicted[n]+=w[degree] @ train_tx[n,degree]\n",
    "        if(y_predicted[n]<0):y_predicted_classed[n]=-1\n",
    "    e=train_y-y_predicted_classed\n",
    "    return calculate_mse(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_gradient_MSE(y,tx,w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    e=y-(tx @ w)\n",
    "    return -1/y.shape[0]*tx.T @ e\n",
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient for batch data.\"\"\"\n",
    "    return compute_gradient_MSE(y,tx,w)\n",
    "\n",
    "def compute_gradient_MAE(y,tx,w):\n",
    "    sumX=0\n",
    "    sumY=0\n",
    "    for n in range(0,y.shape[0]):\n",
    "        temp=y[n]-w[0]-w[1]*tx[n,1]\n",
    "        if(temp>0):\n",
    "            sumX=sumX-1\n",
    "            sumY=sumY-tx[n,1]\n",
    "        if(temp<0):\n",
    "            sumX=sumX+1\n",
    "            sumY=sumY+tx[n,1]\n",
    "    return np.array([sumX/y.shape[0],sumY/y.shape[0]])\n",
    "\n",
    "def calculate_gradient_logistic_regression(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return tx.T @ (sigmoid(tx @ w)-y)\n",
    "\n",
    "def calculate_hessian(y, tx, w):\n",
    "    \"\"\"return the hessian of the loss function.\"\"\"\n",
    "    # calculate hessian: \n",
    "    S=np.zeros((y.shape[0],y.shape[0]))\n",
    "    for i in range(0,y.shape[0]):\n",
    "        S[i,i]=sigmoid(tx[i].T @ w)*(1-sigmoid(tx[i].T @ w))\n",
    "    return tx.T @ S @ tx\n",
    "\n",
    "\n",
    "def logistic_regression_newton(y, tx, w):\n",
    "    \"\"\"return the loss, gradient, and hessian.\"\"\"\n",
    "    # return loss, gradient, and hessian:\n",
    "    return calculate_loss_logistic_regression(y,tx,w),calculate_gradient_logistic_regression(y,tx,w),calculate_hessian(y,tx,w)\n",
    "\n",
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss, gradient, and hessian.\"\"\"\n",
    "    # return loss, gradient, and hessian:\n",
    "    loss,gradient,hessian=logistic_regression_newton(y,tx,w)\n",
    "    loss+= lambda_*np.sum(w*w)\n",
    "    gradient+=lambda_*np.sum(2*w)\n",
    "    hessian+=lambda_*2*w.shape[0]\n",
    "    return loss,gradient,hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent (one step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent_logistic_regression(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    # compute the cost:\n",
    "    #loss=calculate_loss_logistic_regression(y,tx,w)\n",
    "    loss=compute_loss(y,tx,w)\n",
    "    # compute the gradient:\n",
    "    gradient=calculate_gradient_logistic_regression(y,tx,w)\n",
    "    # update w:\n",
    "    w=w-gamma*gradient\n",
    "    return loss, w\n",
    "\n",
    "def learning_by_newton_method(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step on Newton's method.\n",
    "    return the loss and updated w.\n",
    "    \"\"\"\n",
    "    # return loss, gradient and hessian:\n",
    "    loss,gradient,hessian = logistic_regression_newton(y,tx,w)\n",
    "    # update w:\n",
    "    w=w-gamma * np.linalg.inv(hessian) @ gradient\n",
    "    return loss, w\n",
    "\n",
    "def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated w.\n",
    "    \"\"\"\n",
    "    # return loss, gradient and hessian:\n",
    "    \n",
    "    loss,gradient,hessian = penalized_logistic_regression(y,tx,w,lambda_)\n",
    "    # update w:\n",
    "    w=w-gamma * np.linalg.inv(hessian) @ gradient\n",
    "    return loss, w\n",
    "\n",
    "\n",
    "# DO GD linear regression with mse\n",
    "def learning_by_GD_mse(y,tx,w,lambda_):\n",
    "    return compute_loss(y,tx,w),w-lambda_*compute_gradient_MSE(y,tx,w)\n",
    "# DO SGD linear regression with mse\n",
    "def learning_by_SGD_mse(y,tx,w,lambda_):\n",
    "    return compute_loss(y,tx,w),w-lambda_*compute_stoch_gradient(y,tx,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    w=np.linalg.inv(tx.T @ tx) @ tx.T @ y\n",
    "    return compute_loss(y,tx,w),w\n",
    "\n",
    "def ridge_regression(y, teta, lamb):\n",
    "    \"\"\"implement ridge regression.\n",
    "    # For each dimension we have the weights\n",
    "    ws=np.zeros((tx.shape[1],tx.shape[2]))\n",
    "    \n",
    "    for dim in range(0,tx.shape[2]):\n",
    "        tx_per_dim=tx[:,dim,:] #one dimension\n",
    "        ws[dim]=(np.linalg.solve((tx_per_dim.T @ tx_per_dim)+lamb*np.identity(tx_per_dim.shape[1]), tx_per_dim.T @ y))\n",
    "    # We calculate the average\n",
    "    return ws\n",
    "    \"\"\"\n",
    "    \n",
    "    teta_t=np.transpose(teta,(2,1,0))\n",
    "    teta_good= np.transpose(teta,(2,0,1))\n",
    "    to_add=(teta_t @ teta_good)\n",
    "    \n",
    "    to_inv=to_add+lamb*2*teta.shape[0]*np.identity(to_add.shape[1])\n",
    "    \n",
    "    #inv=np.linalg.inv(to_inv)\n",
    "    #a=inv @ teta_t\n",
    "    #b=a @ y\n",
    "    b=teta_t @ y\n",
    "    \n",
    "    w=np.linalg.solve(to_inv,b) #/teta.shape[0]\n",
    "    # is of form (Dimension X degree)\n",
    "    return w\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent_mse(y, tx, initial_w, max_iters, gamma): \n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        if n_iter % 1000 == 1:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=n_iter, l=losses[n_iter-1]))\n",
    "        loss,w=learning_by_GD_mse(y,tx,w,gamma)\n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "    return losses, ws\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iter, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    minibatchs = batch_iter(y, tx, batch_size, num_batches=int(y.shape[0]/batch_size))\n",
    "    num_batches=int(y.shape[0]/batch_size)\n",
    "    for n_iter in range(0,int(np.min([max_iter,num_batches]))):\n",
    "        # compute gradient and loss\n",
    "        minibatch=minibatchs.__next__()\n",
    "        loss,w=learning_by_SGD_mse(minibatch[0],minibatch[1],w,gamma)\n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "    return losses, ws\n",
    "\n",
    "def logistic_regression_gradient_descent(y, tx, max_iter,gamma):\n",
    "    # init parameters\n",
    "    losses = []\n",
    "    #tx=build_tx(x)\n",
    "    \n",
    "    w = np.zeros((tx.shape[1]))\n",
    "    ws=[]\n",
    "    ws.append(w)\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent_logistic_regression(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 1000 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criteria\n",
    "        losses.append(loss)\n",
    "        ws.append(w)\n",
    "    return losses,ws\n",
    "\n",
    "def logistic_regression_newton_method(y, tx,max_iter,gamma):\n",
    "    # init parameters\n",
    "    losses = []\n",
    "    #tx=build_tx(x)\n",
    "    \n",
    "    w = np.zeros((tx.shape[1]))\n",
    "    ws=[]\n",
    "    ws.append(w)\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_newton_method(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 20 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criteria\n",
    "        losses.append(loss)\n",
    "        ws.append(w)\n",
    "\n",
    "    print(\"The loss={l}\".format(l=compute_loss(y, tx, w)))\n",
    "    return losses,ws\n",
    "\n",
    "def logistic_regression_penalized_gradient_descent(y, x,max_iters,gamma,lambda_):\n",
    "    # init parameters\n",
    "    losses = []\n",
    "    #tx=build_tx(x)\n",
    "    tx=x\n",
    "    w = np.zeros((tx.shape[1]))\n",
    "    ws=[]\n",
    "    ws.append(w)\n",
    "    \n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iters):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "        # log info\n",
    "        if iter % 20 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criteria\n",
    "        losses.append(loss)\n",
    "        ws.append(w)\n",
    "    print(\"The loss={l}\".format(l=compute_loss(y, tx, w)))\n",
    "    return losses,ws\n",
    "    \n",
    "    \n",
    "def ridge_regression_ml_function(y,x_powered,lambda_):\n",
    "    \"\"\"ridge regression demo.\"\"\"\n",
    "    # form train and test data with polynomial basis function: TODO\n",
    "    # ***************************************************\n",
    "    #train_tx=build_poly(x,degree)\n",
    "    train_tx=x_powered\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # ridge regression with different lambda: TODO\n",
    "    # ***************************************************\n",
    "    weight = ridge_regression(y,train_tx,lambda_)\n",
    "    loss=compute_loss_poly(y,train_tx,weight)\n",
    "    return loss,weight\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation_ridge(y, x_poly, k_indices, k, lambda_):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # get k'th subgroup in test, others in train: TODO\n",
    "    # ***************************************************\n",
    "    train_indices=k_indices[[i for i in range(len(k_indices)) if i != k]]\n",
    "    train_tx,train_y=x_poly[np.ravel(train_indices)],y[np.ravel(train_indices)]\n",
    "    test_tx,test_y=x_poly[k_indices[k]],y[k_indices[k]]\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # ridge regression: TODO\n",
    "    # ***************************************************\n",
    "    weight = ridge_regression(train_y,train_tx,lambda_)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # calculate the loss for train and test data: TODO\n",
    "    # ***************************************************\n",
    "    loss_tr=compute_loss_poly(train_y,train_tx,weight)\n",
    "    loss_te=compute_loss_poly(test_y,test_tx,weight)\n",
    "    return loss_tr, loss_te,weight\n",
    "\n",
    "def cross_validation_mean_ridge(y,x_poly,k_indices,lambda_):\n",
    "    loss_tr=[]\n",
    "    loss_te=[]\n",
    "    weights=[]\n",
    "    for k in np.arange(k_indices.shape[0]):\n",
    "        loss=np.zeros(2)\n",
    "        loss[0],loss[1],weight=cross_validation_ridge(y,x_powered,k_indices,k,lambda_)\n",
    "        loss_tr.append(loss[0])\n",
    "        loss_te.append(loss[1])\n",
    "        weights.append(weight)\n",
    "    rmse_tr=np.mean(loss_tr)\n",
    "    rmse_te=np.mean(loss_te)\n",
    "    return rmse_tr,rmse_te,np.mean(weights,axis=0)\n",
    "        \n",
    "    \n",
    "def split_data_newton(y,tx,ratio,max_iter,gamma,seed):\n",
    "    np.random.seed(seed)\n",
    "    split_size=int(x.shape[0]*ratio)\n",
    "    split_x=[]\n",
    "    split_y=[]\n",
    "    shuffle_indices = np.random.permutation(np.arange(y.shape[0]))\n",
    "    shuffled_y = y[shuffle_indices]\n",
    "    shuffled_tx = x[shuffle_indices]\n",
    "    train_tx,train_y,test_tx,test_y=shuffled_tx[0:split_size],shuffled_y[0:split_size],shuffled_tx[split_size:],shuffled_y[split_size:]\n",
    "    \n",
    "    loss_tr,weight=logistic_regression_newton_method(train_y,train_tx,max_iter,gamma)\n",
    "    \n",
    "    return loss_tr,compute_loss(y,test_tx,weight),weight\n",
    "\n",
    "def split_data_penalized_logistic_regression(y,tx,ratio,max_iter,gamma,lambda_,seed):\n",
    "    np.random.seed(seed)\n",
    "    split_size=int(x.shape[0]*ratio)\n",
    "    split_x=[]\n",
    "    split_y=[]\n",
    "    shuffle_indices = np.random.permutation(np.arange(y.shape[0]))\n",
    "    shuffled_y = y[shuffle_indices]\n",
    "    shuffled_tx = x[shuffle_indices]\n",
    "    train_tx,train_y,test_tx,test_y=shuffled_tx[0:split_size],shuffled_y[0:split_size],shuffled_tx[split_size:],shuffled_y[split_size:]\n",
    "    \n",
    "    loss_tr,weight=logistic_regression_penalized_gradient_descent(train_y,train_tx,max_iter,gamma,lambda_)\n",
    "    \n",
    "    return loss_tr,compute_loss(y,test_tx,weight),weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Testing different methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=1, the loss=25250993.002723936\n",
      "Current iteration=1001, the loss=8415839.40507081\n",
      "Current iteration=2001, the loss=3048030.1081383144\n"
     ]
    }
   ],
   "source": [
    "tx=build_tx(x)\n",
    "initial_w=np.ones(tx.shape[1])\n",
    "max_iters=3000\n",
    "gamma =0.0000000001\n",
    "losses,ws=gradient_descent_mse(y, tx, initial_w, max_iters, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "losses,ws=gradient_descent_mse(y, tx, ws[len(ws)-1], max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_w_GD_mse=ws[len(ws)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1323840.1029201755,\n",
       " array([ 1.00067945,  0.85158762,  1.04080702,  1.05437652,  1.00373909,\n",
       "         0.18560112,  0.12512707,  0.18609239,  1.0018195 ,  1.00728287,\n",
       "         1.03231887,  1.00093662,  0.99940229,  0.18587565,  1.02129294,\n",
       "         0.99998509,  0.99998911,  1.02717495,  0.99997015,  1.00002978,\n",
       "         1.01905993,  0.99998151,  1.06526455,  0.99991037,  0.41223165,\n",
       "         0.41600052,  0.41599817,  0.17695288,  0.18594152,  0.18594123,\n",
       "         0.98385098]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses[len(losses)-1],final_w_GD_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient descent mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP Pace\\Documents\\GitHub\\MLO-project1\\scripts\\helpers.py:50: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n"
     ]
    }
   ],
   "source": [
    "tx=build_tx(x)\n",
    "initial_w=np.ones(tx.shape[1])\n",
    "max_iters=50000\n",
    "gamma =0.0000000001\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP Pace\\Documents\\GitHub\\MLO-project1\\scripts\\helpers.py:50: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n"
     ]
    }
   ],
   "source": [
    "losses,ws=stochastic_gradient_descent(y, tx, initial_w, x.shape[0]/500, max_iters, gamma)\n",
    "for i in range(10):\n",
    "    losses,ws=stochastic_gradient_descent(y, tx, ws[len(ws)-1], x.shape[0]/500, max_iters, gamma)\n",
    "final_SGD_gradient_ws = ws[len(ws)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(546718.60564020462,\n",
       " array([ 1.00070323,  0.79114534,  1.04499269,  1.05518276,  0.990674  ,\n",
       "         0.06459592, -0.0353852 ,  0.0653959 ,  1.0019457 ,  1.00551383,\n",
       "         1.00675727,  1.000952  ,  0.99919358,  0.06505433,  1.0199238 ,\n",
       "         0.9999817 ,  0.99998706,  1.02614731,  0.99996239,  1.0000307 ,\n",
       "         1.01654471,  0.99997623,  1.039604  ,  0.99966885,  0.26621117,\n",
       "         0.28420083,  0.28419932,  0.04974589,  0.06516537,  0.06516498,\n",
       "         0.96068615]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses[len(losses)-1],final_SGD_gradient_ws "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss,final_LS_w=least_squares(y,tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.5,\n",
       "  0.49698797921727417,\n",
       "  0.49409231712290069,\n",
       "  0.49131190284432452,\n",
       "  0.48864563371945402,\n",
       "  0.48609241535115366,\n",
       "  0.4836511616579367,\n",
       "  0.48132079492090701,\n",
       "  0.47910024582701538,\n",
       "  0.47698845350868513,\n",
       "  0.47498436557987472,\n",
       "  0.47308693816863612,\n",
       "  0.47129513594623496,\n",
       "  0.46960793215289992,\n",
       "  0.46802430862026656,\n",
       "  0.46654325579058326,\n",
       "  0.46516377273275039,\n",
       "  0.46388486715525862,\n",
       "  0.46270555541610059,\n",
       "  0.46162486252972212,\n",
       "  0.46064182217108818,\n",
       "  0.45975547667693145,\n",
       "  0.45896487704425798,\n",
       "  0.45826908292617918,\n",
       "  0.45766716262514257,\n",
       "  0.45715819308363426,\n",
       "  0.45674125987242231,\n",
       "  0.45641545717641435,\n",
       "  0.45617988777819746,\n",
       "  0.45603366303933629,\n",
       "  0.45597590287949158,\n",
       "  0.45600573575343589,\n",
       "  0.45612229862603243,\n",
       "  0.45632473694524472,\n",
       "  0.45661220461324498,\n",
       "  0.45698386395569063,\n",
       "  0.4574388856892318,\n",
       "  0.4579764488873172,\n",
       "  0.45859574094436084,\n",
       "  0.45929595753833596,\n",
       "  0.46007630259185711,\n",
       "  0.46093598823180992,\n",
       "  0.46187423474759365,\n",
       "  0.46289027054803344,\n",
       "  0.46398333211701981,\n",
       "  0.46515266396793503,\n",
       "  0.46639751859692191,\n",
       "  0.46771715643505096,\n",
       "  0.46911084579943718,\n",
       "  0.47057786284336495,\n",
       "  0.47211749150546684,\n",
       "  0.47372902345801238,\n",
       "  0.47541175805435243,\n",
       "  0.47716500227557068,\n",
       "  0.47898807067638732,\n",
       "  0.48088028533036276,\n",
       "  0.48284097577444596,\n",
       "  0.48486947895291099,\n",
       "  0.48696513916072409,\n",
       "  0.48912730798638543,\n",
       "  0.4913553442542829,\n",
       "  0.4936486139665992,\n",
       "  0.49600649024481092,\n",
       "  0.49842835327081569,\n",
       "  0.50091359022772364,\n",
       "  0.50346159524035006,\n",
       "  0.50607176931544051,\n",
       "  0.50874352028166414,\n",
       "  0.51147626272940527,\n",
       "  0.51426941795038483,\n",
       "  0.51712241387714164,\n",
       "  0.52003468502240402,\n",
       "  0.52300567241837448,\n",
       "  0.52603482355596298,\n",
       "  0.52912159232398726,\n",
       "  0.53226543894836831,\n",
       "  0.53546582993134784,\n",
       "  0.53872223799074437,\n",
       "  0.54203414199927979,\n",
       "  0.54540102692398928,\n",
       "  0.54882238376574077,\n",
       "  0.55229770949888146,\n",
       "  0.55582650701103387,\n",
       "  0.55940828504305318,\n",
       "  0.56304255812917392,\n",
       "  0.56672884653735123,\n",
       "  0.57046667620982316,\n",
       "  0.57425557870390076,\n",
       "  0.57809509113300983,\n",
       "  0.58198475610799061,\n",
       "  0.58592412167867092,\n",
       "  0.58991274127573068,\n",
       "  0.59395017365286229,\n",
       "  0.59803598282924431,\n",
       "  0.60216973803233642,\n",
       "  0.60635101364100696,\n",
       "  0.61057938912900445,\n",
       "  0.61485444900877761,\n",
       "  0.61917578277566221,\n",
       "  0.62354298485242976,\n",
       "  0.62795565453421787,\n",
       "  0.6324133959338446,\n",
       "  0.6369158179275114,\n",
       "  0.64146253410090481,\n",
       "  0.64605316269570434,\n",
       "  0.65068732655649453,\n",
       "  0.65536465307809522,\n",
       "  0.6600847741533088,\n",
       "  0.66484732612109221,\n",
       "  0.66965194971515507,\n",
       "  0.67449829001299133,\n",
       "  0.67938599638534047,\n",
       "  0.68431472244609237,\n",
       "  0.68928412600262812,\n",
       "  0.69429386900660406,\n",
       "  0.6993436175051807,\n",
       "  0.70443304159269826,\n",
       "  0.70956181536280061,\n",
       "  0.71472961686100489,\n",
       "  0.719936128037726,\n",
       "  0.72518103470174777,\n",
       "  0.73046402647414899,\n",
       "  0.73578479674267727,\n",
       "  0.74114304261657749,\n",
       "  0.74653846488187114,\n",
       "  0.75197076795708495,\n",
       "  0.75743965984943284,\n",
       "  0.76294485211144625,\n",
       "  0.76848605979805429,\n",
       "  0.77406300142410966,\n",
       "  0.77967539892236604,\n",
       "  0.78532297760189584,\n",
       "  0.79100546610695499,\n",
       "  0.79672259637628873,\n",
       "  0.80247410360287874,\n",
       "  0.808259726194129,\n",
       "  0.81407920573248682,\n",
       "  0.81993228693649978,\n",
       "  0.82581871762230441,\n",
       "  0.83173824866554646,\n",
       "  0.83769063396372423,\n",
       "  0.84367563039896432,\n",
       "  0.84969299780121021,\n",
       "  0.85574249891184018,\n",
       "  0.8618238993476951,\n",
       "  0.86793696756552519,\n",
       "  0.87408147482684473,\n",
       "  0.880257195163197,\n",
       "  0.88646390534182329,\n",
       "  0.89270138483173567,\n",
       "  0.89896941577018907,\n",
       "  0.90526778292954557,\n",
       "  0.91159627368453799,\n",
       "  0.9179546779799157,\n",
       "  0.92434278829848515,\n",
       "  0.93076039962952517,\n",
       "  0.9372073094375859,\n",
       "  0.94368331763166646,\n",
       "  0.95018822653476054,\n",
       "  0.95672184085377365,\n",
       "  0.96328396764980817,\n",
       "  0.96987441630881022,\n",
       "  0.97649299851257165,\n",
       "  0.98313952821009643,\n",
       "  0.98981382158930886,\n",
       "  0.99651569704912168,\n",
       "  1.0032449751718384,\n",
       "  1.0100014786959077,\n",
       "  1.0167850324890078,\n",
       "  1.0235954635214715,\n",
       "  1.0304326008400406,\n",
       "  1.0372962755419468,\n",
       "  1.0441863207493196,\n",
       "  1.0511025715839146,\n",
       "  1.0580448651421566,\n",
       "  1.0650130404705012,\n",
       "  1.0720069385411042,\n",
       "  1.0790264022277984,\n",
       "  1.0860712762823754,\n",
       "  1.0931414073111665,\n",
       "  1.1002366437519215,\n",
       "  1.107356835850982,\n",
       "  1.1145018356407408,\n",
       "  1.1216714969173966,\n",
       "  1.1288656752189818,\n",
       "  1.1360842278036807,\n",
       "  1.1433270136284155,\n",
       "  1.1505938933277171,\n",
       "  1.1578847291928545,\n",
       "  1.1651993851512408,\n",
       "  1.1725377267460988,\n",
       "  1.1798996211163901,\n",
       "  1.1872849369770013,\n",
       "  1.1946935445991815,\n",
       "  1.2021253157912397,\n",
       "  1.2095801238794834,\n",
       "  1.2170578436894055,\n",
       "  1.2245583515271146,\n",
       "  1.2320815251610033,\n",
       "  1.2396272438036553,\n",
       "  1.2471953880939846,\n",
       "  1.2547858400796037,\n",
       "  1.262398483199427,\n",
       "  1.2700332022664906,\n",
       "  1.277689883450998,\n",
       "  1.2853684142635859,\n",
       "  1.2930686835388092,\n",
       "  1.3007905814188336,\n",
       "  1.3085339993373419,\n",
       "  1.3162988300036551,\n",
       "  1.324084967387049,\n",
       "  1.3318923067012807,\n",
       "  1.3397207443893158,\n",
       "  1.3475701781082507,\n",
       "  1.3554405067144319,\n",
       "  1.3633316302487695,\n",
       "  1.3712434499222377,\n",
       "  1.3791758681015687,\n",
       "  1.3871287882951273,\n",
       "  1.3951021151389695,\n",
       "  1.4030957543830864,\n",
       "  1.4111096128778213,\n",
       "  1.4191435985604615,\n",
       "  1.4271976204420147,\n",
       "  1.4352715885941409,\n",
       "  1.4433654141362686,\n",
       "  1.4514790092228693,\n",
       "  1.4596122870308963,\n",
       "  1.4677651617473937,\n",
       "  1.4759375485572594,\n",
       "  1.4841293636311661,\n",
       "  1.4923405241136427,\n",
       "  1.5005709481113101,\n",
       "  1.5088205546812632,\n",
       "  1.5170892638196094,\n",
       "  1.525376996450152,\n",
       "  1.5336836744132216,\n",
       "  1.5420092204546503,\n",
       "  1.5503535582148897,\n",
       "  1.5587166122182678,\n",
       "  1.5670983078623832,\n",
       "  1.5754985714076435,\n",
       "  1.5839173299669265,\n",
       "  1.592354511495387,\n",
       "  1.6008100447803832,\n",
       "  1.6092838594315459,\n",
       "  1.617775885870959,\n",
       "  1.6262860553234839,\n",
       "  1.6348142998071935,\n",
       "  1.6433605521239385,\n",
       "  1.6519247458500304,\n",
       "  1.6605068153270435,\n",
       "  1.6691066956527376,\n",
       "  1.6777243226720986,\n",
       "  1.6863596329684847,\n",
       "  1.6950125638548958,\n",
       "  1.7036830533653522,\n",
       "  1.7123710402463759,\n",
       "  1.7210764639485945,\n",
       "  1.7297992646184388,\n",
       "  1.7385393830899534,\n",
       "  1.7472967608767118,\n",
       "  1.7560713401638319,\n",
       "  1.7648630638000904,\n",
       "  1.773671875290147,\n",
       "  1.7824977187868583,\n",
       "  1.7913405390836914,\n",
       "  1.800200281607236,\n",
       "  1.8090768924098115,\n",
       "  1.8179703181621625,\n",
       "  1.8268805061462534,\n",
       "  1.8358074042481514,\n",
       "  1.8447509609509976,\n",
       "  1.8537111253280703,\n",
       "  1.8626878470359309,\n",
       "  1.8716810763076628,\n",
       "  1.8806907639461896,\n",
       "  1.8897168613176802,\n",
       "  1.8987593203450377,\n",
       "  1.9078180935014664,\n",
       "  1.9168931338041213,\n",
       "  1.9259843948078397,\n",
       "  1.9350918305989495,\n",
       "  1.9442153957891515,\n",
       "  1.953355045509485,\n",
       "  1.9625107354043632,\n",
       "  1.9716824216256861,\n",
       "  1.9808700608270238,\n",
       "  1.9900736101578764,\n",
       "  1.999293027258003,\n",
       "  2.0085282702518183,\n",
       "  2.0177792977428668,\n",
       "  2.0270460688083571,\n",
       "  2.036328542993767,\n",
       "  2.0456266803075187,\n",
       "  2.0549404412157157,\n",
       "  2.0642697866369444,\n",
       "  2.0736146779371469,\n",
       "  2.0829750769245452,\n",
       "  2.0923509458446379,\n",
       "  2.1017422473752552,\n",
       "  2.1111489446216769,\n",
       "  2.1205710011118031,\n",
       "  2.1300083807913923,\n",
       "  2.1394610480193559,\n",
       "  2.1489289675631076,\n",
       "  2.1584121045939719,\n",
       "  2.1679104246826486,\n",
       "  2.1774238937947294,\n",
       "  2.1869524782862801,\n",
       "  2.196496144899462,\n",
       "  2.2060548607582118,\n",
       "  2.2156285933639825,\n",
       "  2.2252173105915247,\n",
       "  2.2348209806847241,\n",
       "  2.2444395722524884,\n",
       "  2.2540730542646861,\n",
       "  2.2637213960481337,\n",
       "  2.2733845672826272,\n",
       "  2.2830625379970284,\n",
       "  2.2927552785653904,\n",
       "  2.3024627597031433,\n",
       "  2.3121849524633049,\n",
       "  2.3219218282327576,\n",
       "  2.3316733587285565,\n",
       "  2.3414395159942916,\n",
       "  2.3512202723964837,\n",
       "  2.361015600621029,\n",
       "  2.3708254736696874,\n",
       "  2.380649864856613,\n",
       "  2.3904887478049139,\n",
       "  2.4003420964432789,\n",
       "  2.410209885002613,\n",
       "  2.4200920880127335,\n",
       "  2.4299886802991066,\n",
       "  2.4398996369796082,\n",
       "  2.449824933461334,\n",
       "  2.4597645454374439,\n",
       "  2.4697184488840471,\n",
       "  2.479686620057115,\n",
       "  2.4896690354894457,\n",
       "  2.4996656719876458,\n",
       "  2.5096765066291646,\n",
       "  2.5197015167593526,\n",
       "  2.5297406799885587,\n",
       "  2.5397939741892577,\n",
       "  2.549861377493218,\n",
       "  2.5599428682886987,\n",
       "  2.5700384252176769,\n",
       "  2.580148027173113,\n",
       "  2.5902716532962398,\n",
       "  2.6004092829738954,\n",
       "  2.6105608958358757,\n",
       "  2.6207264717523202,\n",
       "  2.6309059908311379,\n",
       "  2.6410994334154454,\n",
       "  2.6513067800810504,\n",
       "  2.6615280116339575,\n",
       "  2.6717631091079039,\n",
       "  2.6820120537619201,\n",
       "  2.6922748270779264,\n",
       "  2.7025514107583524,\n",
       "  2.7128417867237786,\n",
       "  2.7231459371106208,\n",
       "  2.7334638442688175,\n",
       "  2.7437954907595743,\n",
       "  2.754140859353098,\n",
       "  2.7644999330263933,\n",
       "  2.7748726949610534,\n",
       "  2.7852591285411021,\n",
       "  2.7956592173508388,\n",
       "  2.8060729451727147,\n",
       "  2.8165002959852505,\n",
       "  2.8269412539609475,\n",
       "  2.8373958034642461,\n",
       "  2.8478639290494967,\n",
       "  2.8583456154589566,\n",
       "  2.8688408476208096,\n",
       "  2.8793496106472056,\n",
       "  2.889871889832329,\n",
       "  2.9004076706504769,\n",
       "  2.9109569387541736,\n",
       "  2.9215196799722896,\n",
       "  2.9320958803082036,\n",
       "  2.9426855259379585,\n",
       "  2.9532886032084651,\n",
       "  2.9639050986357041,\n",
       "  2.9745349989029588,\n",
       "  2.9851782908590683,\n",
       "  2.9958349615166928,\n",
       "  3.006504998050612,\n",
       "  3.0171883877960202,\n",
       "  3.0278851182468691,\n",
       "  3.0385951770542006,\n",
       "  3.0493185520245158,\n",
       "  3.0600552311181599,\n",
       "  3.0708052024477195,\n",
       "  3.0815684542764417,\n",
       "  3.0923449750166658,\n",
       "  3.1031347532282831,\n",
       "  3.1139377776172017,\n",
       "  3.1247540370338331,\n",
       "  3.1355835204715969,\n",
       "  3.1464262170654389,\n",
       "  3.1572821160903697,\n",
       "  3.168151206960014,\n",
       "  3.1790334792251786,\n",
       "  3.1899289225724372,\n",
       "  3.2008375268227285,\n",
       "  3.2117592819299721,\n",
       "  3.2226941779796916,\n",
       "  3.2336422051876679,\n",
       "  3.2446033538985923,\n",
       "  3.2555776145847388,\n",
       "  3.2665649778446593,\n",
       "  3.2775654344018763,\n",
       "  3.2885789751036025,\n",
       "  3.2996055909194801,\n",
       "  3.3106452729403077,\n",
       "  3.3216980123768116,\n",
       "  3.3327638005584057,\n",
       "  3.3438426289319865,\n",
       "  3.3549344890607213,\n",
       "  3.3660393726228626,\n",
       "  3.3771572714105678,\n",
       "  3.3882881773287359,\n",
       "  3.3994320823938571,\n",
       "  3.4105889787328718,\n",
       "  3.4217588585820402,\n",
       "  3.4329417142858274,\n",
       "  3.4441375382958026,\n",
       "  3.4553463231695467,\n",
       "  3.4665680615695664,\n",
       "  3.4778027462622325,\n",
       "  3.4890503701167153,\n",
       "  3.5003109261039453,\n",
       "  3.5115844072955698,\n",
       "  3.5228708068629353,\n",
       "  3.5341701180760716,\n",
       "  3.5454823343026893,\n",
       "  3.5568074490071817,\n",
       "  3.5681454557496557,\n",
       "  3.5794963481849447,\n",
       "  3.5908601200616594,\n",
       "  3.6022367652212299,\n",
       "  3.6136262775969712,\n",
       "  3.6250286512131384,\n",
       "  3.6364438801840229,\n",
       "  3.6478719587130235,\n",
       "  3.6593128810917563,\n",
       "  3.6707666416991565,\n",
       "  3.6822332350005884,\n",
       "  3.6937126555469879,\n",
       "  3.7052048979739776,\n",
       "  3.7167099570010178,\n",
       "  3.7282278274305671,\n",
       "  3.7397585041472303,\n",
       "  3.7513019821169351,\n",
       "  3.7628582563861115,\n",
       "  3.7744273220808711,\n",
       "  3.7860091744062139,\n",
       "  3.7976038086452211,\n",
       "  3.8092112201582755,\n",
       "  3.8208314043822735,\n",
       "  3.8324643568298562,\n",
       "  3.8441100730886486,\n",
       "  3.8557685488204974,\n",
       "  3.8674397797607281,\n",
       "  3.8791237617173979,\n",
       "  3.890820490570567,\n",
       "  3.9025299622715703,\n",
       "  3.9142521728423003,\n",
       "  3.9259871183744943,\n",
       "  3.9377347950290327,\n",
       "  3.9494951990352418,\n",
       "  3.9612683266902002,\n",
       "  3.9730541743580607,\n",
       "  3.9848527384693719,\n",
       "  3.9966640155204134,\n",
       "  4.0084880020725242,\n",
       "  4.020324694751463,\n",
       "  4.0321740902467385,\n",
       "  4.0440361853109845,\n",
       "  4.0559109767593222,\n",
       "  4.0677984614687182,\n",
       "  4.0796986363773708,\n",
       "  4.0916114984840979,\n",
       "  4.1035370448477151,\n",
       "  4.1154752725864361,\n",
       "  4.1274261788772808,\n",
       "  4.139389760955468,\n",
       "  4.151366016113843,\n",
       "  4.163354941702293,\n",
       "  4.1753565351271629,\n",
       "  4.1873707938507012,\n",
       "  4.1993977153904858,\n",
       "  4.2114372973188683,\n",
       "  4.2234895372624255,\n",
       "  4.2355544329014085,\n",
       "  4.2476319819692012,\n",
       "  4.2597221822517861,\n",
       "  4.271825031587217,\n",
       "  4.2839405278650808,\n",
       "  4.2960686690259928,\n",
       "  4.3082094530610728,\n",
       "  4.3203628780114389,\n",
       "  4.3325289419676931,\n",
       "  4.3447076430694365,\n",
       "  4.356898979504761,\n",
       "  4.3691029495097675,\n",
       "  4.3813195513680752,\n",
       "  4.3935487834103411,\n",
       "  4.4057906440137886,\n",
       "  4.4180451316017306,\n",
       "  4.4303122446431056,\n",
       "  4.4425919816520176,\n",
       "  4.4548843411872738,\n",
       "  4.4671893218519356,\n",
       "  4.4795069222928658,\n",
       "  4.4918371412002855,\n",
       "  4.5041799773073388,\n",
       "  4.5165354293896529,\n",
       "  4.5289034962649,\n",
       "  4.5412841767923862,\n",
       "  4.5536774698726123,\n",
       "  4.5660833744468627,\n",
       "  4.5785018894967902,\n",
       "  4.5909330140440003,\n",
       "  4.6033767471496567,\n",
       "  4.6158330879140559,\n",
       "  4.6283020354762519,\n",
       "  4.6407835890136413,\n",
       "  4.6532777477415914,\n",
       "  4.6657845109130252,\n",
       "  4.678303877818073,\n",
       "  4.6908358477836565,\n",
       "  4.7033804201731346,\n",
       "  4.7159375943859292,\n",
       "  4.7285073698571392,\n",
       "  4.7410897460571917,\n",
       "  4.7536847224914753,\n",
       "  4.7662922986999758,\n",
       "  4.7789124742569262,\n",
       "  4.7915452487704497,\n",
       "  4.8041906218822152,\n",
       "  4.8168485932670926,\n",
       "  4.8295191626328027,\n",
       "  4.8422023297195889,\n",
       "  4.8548980942998741,\n",
       "  4.8676064561779295,\n",
       "  4.8803274151895462,\n",
       "  4.8930609712017104,\n",
       "  4.9058071241122754,\n",
       "  4.9185658738496496,\n",
       "  4.9313372203724724,\n",
       "  4.9441211636693003,\n",
       "  4.9569177037583003,\n",
       "  4.9697268406869393,\n",
       "  4.9825485745316742,\n",
       "  4.9953829053976655,\n",
       "  5.0082298334184534,\n",
       "  5.0210893587556766,\n",
       "  5.033961481598789,\n",
       "  5.0468462021647396,\n",
       "  5.0597435206977073,\n",
       "  5.0726534374688059,\n",
       "  5.0855759527758071,\n",
       "  5.0985110669428515,\n",
       "  5.111458780320179,\n",
       "  5.1244190932838469,\n",
       "  5.1373920062354621,\n",
       "  5.150377519601915,\n",
       "  5.1633756338350976,\n",
       "  5.176386349411656,\n",
       "  5.1894096668327157,\n",
       "  5.2024455866236217,\n",
       "  5.2154941093336964,\n",
       "  5.2285552355359579,\n",
       "  5.2416289658268935,\n",
       "  5.2547153008261942,\n",
       "  5.2678142411765041,\n",
       "  5.2809257875431905,\n",
       "  5.2940499406140864,\n",
       "  5.307186701099246,\n",
       "  5.3203360697307263,\n",
       "  5.3334980472623235,\n",
       "  5.3466726344693605,\n",
       "  5.359859832148441,\n",
       "  5.3730596411172238,\n",
       "  5.3862720622141964,\n",
       "  5.3994970962984485,\n",
       "  5.4127347442494376,\n",
       "  5.4259850069667896,\n",
       "  5.4392478853700537,\n",
       "  5.4525233803984952,\n",
       "  5.4658114930108956,\n",
       "  5.4791122241853065,\n",
       "  5.4924255749188635,\n",
       "  5.5057515462275637,\n",
       "  5.5190901391460692,\n",
       "  5.5324413547274851,\n",
       "  5.5458051940431616,\n",
       "  5.5591816581825091,\n",
       "  5.5725707482527609,\n",
       "  5.5859724653788128,\n",
       "  5.5993868107030025,\n",
       "  5.6128137853849207,\n",
       "  5.6262533906012244,\n",
       "  5.6397056275454309,\n",
       "  5.6531704974277455,\n",
       "  5.6666480014748632,\n",
       "  5.6801381409297766,\n",
       "  5.6936409170516109,\n",
       "  5.7071563311154172,\n",
       "  5.7206843844120137,\n",
       "  5.7342250782477908,\n",
       "  5.7477784139445403,\n",
       "  5.7613443928392716,\n",
       "  5.7749230162840464,\n",
       "  5.7885142856458067,\n",
       "  5.8021182023061897,\n",
       "  5.8157347676613655,\n",
       "  5.8293639831218789,\n",
       "  5.8430058501124638,\n",
       "  5.8566603700718955,\n",
       "  5.8703275444528096,\n",
       "  5.8840073747215591,\n",
       "  5.8976998623580306,\n",
       "  5.9114050088555121,\n",
       "  5.9251228157205089,\n",
       "  5.9388532844725948,\n",
       "  5.9525964166442762,\n",
       "  5.9663522137808105,\n",
       "  5.9801206774400724,\n",
       "  5.993901809192387,\n",
       "  6.0076956106204049,\n",
       "  6.0215020833189286,\n",
       "  6.0353212288947722,\n",
       "  6.0491530489666276,\n",
       "  6.0629975451649116,\n",
       "  6.07685471913162,\n",
       "  6.0907245725201911,\n",
       "  6.1046071069953616,\n",
       "  6.1185023242330354,\n",
       "  6.1324102259201334,\n",
       "  6.1463308137544646,\n",
       "  6.1602640894445919,\n",
       "  6.1742100547096976,\n",
       "  6.1881687112794426,\n",
       "  6.2021400608938437,\n",
       "  6.216124105303142,\n",
       "  6.2301208462676643,\n",
       "  6.2441302855577092,\n",
       "  6.258152424953412,\n",
       "  6.2721872662446172,\n",
       "  6.2862348112307496,\n",
       "  6.3002950617207159,\n",
       "  6.3143680195327416,\n",
       "  6.3284536864942886,\n",
       "  6.3425520644419056,\n",
       "  6.3566631552211295,\n",
       "  6.3707869606863508,\n",
       "  6.384923482700712,\n",
       "  6.3990727231359772,\n",
       "  6.413234683872429,\n",
       "  6.4274093667987442,\n",
       "  6.4415967738118853,\n",
       "  6.4557969068169978,\n",
       "  6.4700097677272783,\n",
       "  6.4842353584638843,\n",
       "  6.4984736809558159,\n",
       "  6.5127247371398074,\n",
       "  6.5269885289602234,\n",
       "  6.5412650583689578,\n",
       "  6.555554327325309,\n",
       "  6.5698563377958985,\n",
       "  6.5841710917545617,\n",
       "  6.598498591182234,\n",
       "  6.6128388380668612,\n",
       "  6.627191834403293,\n",
       "  6.6415575821931983,\n",
       "  6.6559360834449306,\n",
       "  6.6703273401734755,\n",
       "  6.6847313544003217,\n",
       "  6.6991481281533796,\n",
       "  6.7135776634668742,\n",
       "  6.7280199623812722,\n",
       "  6.7424750269431568,\n",
       "  6.7569428592051679,\n",
       "  6.7714234612258872,\n",
       "  6.7859168350697558,\n",
       "  6.8004229828069906,\n",
       "  6.8149419065134742,\n",
       "  6.8294736082706953,\n",
       "  6.844018090165636,\n",
       "  6.8585753542906938,\n",
       "  6.8731454027436039,\n",
       "  6.8877282376273365,\n",
       "  6.9023238610500286,\n",
       "  6.9169322751248865,\n",
       "  6.931553481970111,\n",
       "  6.9461874837088242,\n",
       "  6.9608342824689542,\n",
       "  6.9754938803831994,\n",
       "  6.9901662795889106,\n",
       "  7.0048514822280312,\n",
       "  7.0195494904470177,\n",
       "  7.0342603063967513,\n",
       "  7.0489839322324661,\n",
       "  7.0637203701136837,\n",
       "  7.0784696222041177,\n",
       "  7.0932316906716046,\n",
       "  7.1080065776880446,\n",
       "  7.1227942854292996,\n",
       "  7.1375948160751514,\n",
       "  7.1524081718091956,\n",
       "  7.1672343548188042,\n",
       "  7.1820733672950245,\n",
       "  7.1969252114325313,\n",
       "  7.2117898894295376,\n",
       "  7.2266674034877392,\n",
       "  7.2415577558122379,\n",
       "  7.2564609486114842,\n",
       "  7.2713769840971896,\n",
       "  7.2863058644842837,\n",
       "  7.3012475919908297,\n",
       "  7.3162021688379726,\n",
       "  7.3311695972498505,\n",
       "  7.3461498794535665,\n",
       "  7.3611430176790922,\n",
       "  7.3761490141592185,\n",
       "  7.3911678711294941,\n",
       "  7.406199590828157,\n",
       "  7.4212441754960752,\n",
       "  7.4363016273766913,\n",
       "  7.451371948715952,\n",
       "  7.4664551417622596,\n",
       "  7.4815512087663878,\n",
       "  7.4966601519814668,\n",
       "  7.5117819736628846,\n",
       "  7.5269166760682404,\n",
       "  7.5420642614572992,\n",
       "  7.557224732091921,\n",
       "  7.5723980902360166,\n",
       "  7.5875843381554704,\n",
       "  7.6027834781181154,\n",
       "  7.6179955123936551,\n",
       "  7.6332204432536184,\n",
       "  7.6484582729713049,\n",
       "  7.663709003821725,\n",
       "  7.6789726380815617,\n",
       "  7.6942491780291071,\n",
       "  7.7095386259442105,\n",
       "  7.7248409841082317,\n",
       "  7.7401562548039902,\n",
       "  7.7554844403157057,\n",
       "  7.7708255429289643,\n",
       "  7.7861795649306513,\n",
       "  7.8015465086089169,\n",
       "  7.8169263762531145,\n",
       "  7.8323191701537711,\n",
       "  7.8477248926025158,\n",
       "  7.863143545892048,\n",
       "  7.8785751323160946,\n",
       "  7.894019654169349,\n",
       "  7.9094771137474291,\n",
       "  7.9249475133468437,\n",
       "  7.9404308552649354,\n",
       "  7.9559271417998323,\n",
       "  7.9714363752504278,\n",
       "  7.9869585579162967,\n",
       "  8.0024936920976959,\n",
       "  8.0180417800954853,\n",
       "  8.0336028242111066,\n",
       "  8.0491768267465336,\n",
       "  8.0647637900042266,\n",
       "  8.0803637162871009,\n",
       "  8.0959766078984767,\n",
       "  8.1116024671420472,\n",
       "  8.1272412963218255,\n",
       "  8.1428930977421157,\n",
       "  8.1585578737074673,\n",
       "  8.1742356265226412,\n",
       "  8.1899263584925688,\n",
       "  8.2056300719223092,\n",
       "  8.2213467691170141,\n",
       "  8.2370764523818956,\n",
       "  8.2528191240221762,\n",
       "  8.2685747863430716,\n",
       "  8.2843434416497281,\n",
       "  8.3001250922472014,\n",
       "  8.3159197404404317,\n",
       "  8.3317273885341834,\n",
       "  8.3475480388330254,\n",
       "  8.3633816936412906,\n",
       "  8.3792283552630451,\n",
       "  8.3950880260020515,\n",
       "  8.4109607081617384,\n",
       "  8.4268464040451523,\n",
       "  8.4427451159549509,\n",
       "  8.458656846193346,\n",
       "  8.4745815970620733,\n",
       "  8.4905193708623887,\n",
       "  8.5064701698949889,\n",
       "  8.5224339964600109,\n",
       "  8.5384108528570053,\n",
       "  8.5544007413848817,\n",
       "  8.570403664341903,\n",
       "  8.5864196240256163,\n",
       "  8.6024486227328811,\n",
       "  8.6184906627597826,\n",
       "  8.6345457464016349,\n",
       "  8.6506138759529421,\n",
       "  8.6666950537073664,\n",
       "  8.6827892819577048,\n",
       "  8.6988965629958646,\n",
       "  8.7150168991128112,\n",
       "  8.7311502925985778,\n",
       "  8.7472967457422079,\n",
       "  8.7634562608317346,\n",
       "  8.7796288401541638,\n",
       "  8.7958144859954324,\n",
       "  8.81201320064039,\n",
       "  8.8282249863727849,\n",
       "  8.8444498454752019,\n",
       "  8.8606877802290835,\n",
       "  8.876938792914661,\n",
       "  8.893202885810954,\n",
       "  8.9094800611957474,\n",
       "  8.925770321345551,\n",
       "  8.9420736685355759,\n",
       "  8.9583901050397383,\n",
       "  8.9747196331305918,\n",
       "  8.9910622550793349,\n",
       "  9.0074179731557784,\n",
       "  9.0237867896283301,\n",
       "  9.0401687067639482,\n",
       "  9.0565637268281396,\n",
       "  9.072971852084935,\n",
       "  9.0893930847968623,\n",
       "  9.1058274272249218,\n",
       "  9.1222748816285666,\n",
       "  9.138735450265683,\n",
       "  9.1552091353925711,\n",
       "  9.1716959392639197,\n",
       "  9.1881958641327746,\n",
       "  9.2047089122505437,\n",
       "  9.2212350858669527,\n",
       "  9.2377743872300346,\n",
       "  9.2543268185861098,\n",
       "  9.2708923821797597,\n",
       "  9.2874710802538232,\n",
       "  9.3040629150493519,\n",
       "  9.3206678888056089,\n",
       "  9.3372860037600507,\n",
       "  9.3539172621483004,\n",
       "  9.3705616662041233,\n",
       "  9.3872192181594265,\n",
       "  9.403889920244227,\n",
       "  9.420573774686634,\n",
       "  9.4372707837128367,\n",
       "  9.453980949547077,\n",
       "  9.4707042744116503,\n",
       "  9.4874407605268711,\n",
       "  9.5041904101110539,\n",
       "  9.5209532253805182,\n",
       "  9.5377292085495373,\n",
       "  9.5545183618303593,\n",
       "  9.571320687433154,\n",
       "  9.5881361875660396,\n",
       "  9.604964864435015,\n",
       "  9.6218067202439901,\n",
       "  9.6386617571947451,\n",
       "  9.6555299774869159,\n",
       "  9.6724113833179821,\n",
       "  9.6893059768832721,\n",
       "  9.7062137603759062,\n",
       "  9.7231347359868057,\n",
       "  9.7400689059047014,\n",
       "  9.7570162723160596,\n",
       "  9.7739768374051348,\n",
       "  9.7909506033538989,\n",
       "  9.8079375723420679,\n",
       "  9.8249377465470626,\n",
       "  9.8419511281440073,\n",
       "  9.8589777193057166,\n",
       "  9.8760175222026714,\n",
       "  9.8930705390030074,\n",
       "  9.9101367718725264,\n",
       "  9.9272162229746481,\n",
       "  9.9443088944704119,\n",
       "  9.9614147885184803,\n",
       "  9.9785339072751018,\n",
       "  9.995666252894102,\n",
       "  10.012811827526887,\n",
       "  10.029970633322421,\n",
       "  10.047142672427217,\n",
       "  10.064327946985317,\n",
       "  10.081526459138288,\n",
       "  10.098738211025221,\n",
       "  10.115963204782695,\n",
       "  10.133201442544781,\n",
       "  10.150452926443025,\n",
       "  10.167717658606461,\n",
       "  10.184995641161557,\n",
       "  10.202286876232234,\n",
       "  10.219591365939857,\n",
       "  10.236909112403206,\n",
       "  10.254240117738485,\n",
       "  10.271584384059295,\n",
       "  10.288941913476636,\n",
       "  10.306312708098892,\n",
       "  10.323696770031827,\n",
       "  10.341094101378562,\n",
       "  10.358504704239593,\n",
       "  10.375928580712733,\n",
       "  10.393365732893153,\n",
       "  10.410816162873365,\n",
       "  10.428279872743161,\n",
       "  10.445756864589679,\n",
       "  10.463247140497357,\n",
       "  10.480750702547908,\n",
       "  10.498267552820336,\n",
       "  10.51579769339093,\n",
       "  10.533341126333235,\n",
       "  10.550897853718075,\n",
       "  10.568467877613502,\n",
       "  10.586051200084826,\n",
       "  10.603647823194597,\n",
       "  10.621257749002581,\n",
       "  10.638880979565775,\n",
       "  10.656517516938383,\n",
       "  10.674167363171819,\n",
       "  10.691830520314697,\n",
       "  10.709506990412809,\n",
       "  10.727196775509158,\n",
       "  10.744899877643897,\n",
       "  10.762616298854367,\n",
       "  10.780346041175061,\n",
       "  10.798089106637645,\n",
       "  10.815845497270923,\n",
       "  10.83361521510084,\n",
       "  10.851398262150493,\n",
       "  10.869194640440092,\n",
       "  10.887004351986995,\n",
       "  10.904827398805665,\n",
       "  10.922663782907675,\n",
       "  10.940513506301716,\n",
       "  10.958376570993575,\n",
       "  10.976252978986139,\n",
       "  10.994142732279373,\n",
       "  11.012045832870349,\n",
       "  11.029962282753203,\n",
       "  11.047892083919155,\n",
       "  11.06583523835647,\n",
       "  11.083791748050512,\n",
       "  11.10176161498368,\n",
       "  11.119744841135434,\n",
       "  11.137741428482286,\n",
       "  11.155751378997795,\n",
       "  11.173774694652538,\n",
       "  11.191811377414156,\n",
       "  11.209861429247308,\n",
       "  11.227924852113674,\n",
       "  11.246001647971966,\n",
       "  11.264091818777906,\n",
       "  11.282195366484229,\n",
       "  11.300312293040685,\n",
       "  11.318442600394027,\n",
       "  11.336586290488016,\n",
       "  11.354743365263385,\n",
       "  11.372913826657902,\n",
       "  11.391097676606286,\n",
       "  11.409294917040262,\n",
       "  11.427505549888535,\n",
       "  11.445729577076786,\n",
       "  11.463967000527683,\n",
       "  11.482217822160855,\n",
       "  11.500482043892895,\n",
       "  11.518759667637374,\n",
       "  11.53705069530483,\n",
       "  11.555355128802745,\n",
       "  11.573672970035576,\n",
       "  11.592004220904716,\n",
       "  11.610348883308525,\n",
       "  11.628706959142299,\n",
       "  11.647078450298293,\n",
       "  11.665463358665704,\n",
       "  11.683861686130649,\n",
       "  11.70227343457621,\n",
       "  11.720698605882394,\n",
       "  11.739137201926134,\n",
       "  11.757589224581313,\n",
       "  11.77605467571872,\n",
       "  11.794533557206098,\n",
       "  11.813025870908088,\n",
       "  11.831531618686272,\n",
       "  11.850050802399149,\n",
       "  11.868583423902127,\n",
       "  11.88712948504755,\n",
       "  ...],\n",
       " array([ -3.60668398e-01,   7.23061306e-05,  -7.14468236e-03,\n",
       "         -6.24039951e-03,  -4.77145010e-04,  -3.08504353e-03,\n",
       "          4.49444469e-04,  -2.38125455e-02,   3.42157704e-01,\n",
       "         -1.09950486e-04,  -2.84186178e+00,  -2.16216038e-01,\n",
       "          9.55448998e-02,   4.52028139e-02,   2.85042830e+00,\n",
       "         -2.62729702e-04,  -9.95639898e-04,   2.85490239e+00,\n",
       "         -3.54662999e-04,   8.49821857e-04,   3.69247838e-03,\n",
       "          2.18113134e-04,  -5.00574872e-04,  -2.11522348e-01,\n",
       "         -2.18882082e-04,   2.01549413e-04,   3.43135946e-04,\n",
       "         -6.98404056e-05,  -6.58071046e-03,  -1.18306611e-02,\n",
       "          2.84110926e+00]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,final_LS_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building polynomial  0\n",
      "Building polynomial  100000\n",
      "Building polynomial  200000\n"
     ]
    }
   ],
   "source": [
    "degree=4\n",
    "x_powered=build_poly(stdx,degree)\n",
    "lambda_=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_powered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-40d0dafe8064>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mridge_regression_ml_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_powered\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_powered' is not defined"
     ]
    }
   ],
   "source": [
    "loss,w=ridge_regression_ml_function(y,x_powered,lambda_)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation with ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "sample  0\n",
      "sample  50000\n"
     ]
    }
   ],
   "source": [
    "k_indices=build_k_indices(y,4,1)\n",
    "loss_tr, loss_te,final_CV_ridge_w=cross_validation_mean_ridge(y,x_powered,k_indices,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.684850666667 0.68496\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "sample  200000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68479199999999996"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(loss_tr, loss_te)\n",
    "compute_loss_poly(y,x_powered,final_CV_ridge_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding a good lambda : (good lambda is 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda 0.0001 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.000161026202756 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.00025929437974 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.000417531893656 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.00067233575365 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.00108263673387 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.0017433288222 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.00280721620394 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.00452035365636 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.00727895384398 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.0117210229753 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.0188739182214 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.0303919538231 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.0489390091848 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.0788046281567 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.126896100317 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.204335971786 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.329034456231 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.529831690628 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.853167852417 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 1.37382379588 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 2.21221629107 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 3.56224789026 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 5.73615251045 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 9.23670857187 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 14.8735210729 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 23.9502661999 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 38.5662042116 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 62.1016941892 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 100.0 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cross_validation_visualization' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-b6f2437dacb2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mrmse_tr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mrmse_te\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mcross_validation_visualization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrmse_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrmse_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cross_validation_visualization' is not defined"
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "k_fold = 4\n",
    "lambdas = np.logspace(-4, 2, 30)\n",
    "# split data in k fold\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "# define lists to store the loss of training data and test data\n",
    "rmse_tr = []\n",
    "rmse_te = []\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# cross validation: TODO\n",
    "# *************************************************** \n",
    "for lambda_ in lambdas:\n",
    "    print (\"Lambda\",lambda_,\"/\",len(lambdas))\n",
    "    loss_tr=[]\n",
    "    loss_te=[]\n",
    "    for k in np.arange(k_fold):\n",
    "        print (\"k=\",k)\n",
    "        loss=np.zeros(2)\n",
    "        loss[0],loss[1],weight=cross_validation_ridge(y,x_powered,k_indices,k,lambda_)\n",
    "        loss_tr.append(loss[0])\n",
    "        loss_te.append(loss[1])\n",
    "    rmse_tr.append(np.mean(loss_tr))\n",
    "    rmse_te.append(np.mean(loss_te))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAEdCAYAAAA4rdFEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2514ab51cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "# The graph shows us that lambda should be 10^1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_iters=2000\n",
    "gamma=0.000000001\n",
    "tx=build_tx(stdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=0.5\n",
      "Current iteration=1000, the loss=11.905688987684659\n"
     ]
    }
   ],
   "source": [
    "loss,ws=logistic_regression_gradient_descent(y,stdx,max_iters,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_Log_Reg_GD=ws[len(ws)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37.359437193054511,\n",
       " array([ 0.08968769,  0.08824643,  0.35293112,  0.25465957, -0.22021743,\n",
       "        -0.07713731, -0.22205536,  0.566085  ,  0.14736452,  0.32000105,\n",
       "         0.21849436,  0.10867015, -0.22115594,  0.42541678, -0.00209893,\n",
       "        -0.00286459,  0.36716063, -0.00215762,  0.00640889,  0.24134735,\n",
       "         0.00252708,  0.36598435,  0.24940106, -0.04736372, -0.08028741,\n",
       "        -0.08029143, -0.19931744, -0.22138621, -0.2213865 ,  0.19782791]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss[len(loss)-1],final_Log_Reg_GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newton method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tx=build_tx(stdx)\n",
    "max_iter=100\n",
    "gamma=0.001\n",
    "seed=1\n",
    "ratio=20000/250000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=13862.943611201723\n",
      "Current iteration=20, the loss=12473.931929369031\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-cdf2002b6286>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# can't create a 250 000 X 250 000 Hessian matrix, splitting data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mloss_tr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_te\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfinal_newton_split_w\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msplit_data_newton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mratio\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-b98f72f8dc3a>\u001b[0m in \u001b[0;36msplit_data_newton\u001b[1;34m(y, tx, ratio, max_iter, gamma, seed)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mtrain_tx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_tx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffled_tx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msplit_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffled_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msplit_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffled_tx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msplit_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffled_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msplit_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mloss_tr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogistic_regression_newton_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_tx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss_tr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_tx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-035057b5efb2>\u001b[0m in \u001b[0;36mlogistic_regression_newton_method\u001b[1;34m(y, tx, max_iter, gamma)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;31m# get loss and update w.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_by_newton_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;31m# log info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m20\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-9ff7d5618136>\u001b[0m in \u001b[0;36mlearning_by_newton_method\u001b[1;34m(y, tx, w, gamma)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \"\"\"\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# return loss, gradient and hessian:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhessian\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression_newton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;31m# update w:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhessian\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-aaa9cfb9ed8a>\u001b[0m in \u001b[0;36mlogistic_regression_newton\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;34m\"\"\"return the loss, gradient, and hessian.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;31m# return loss, gradient, and hessian:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcalculate_loss_logistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcalculate_gradient_logistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcalculate_hessian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpenalized_logistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-aaa9cfb9ed8a>\u001b[0m in \u001b[0;36mcalculate_hessian\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mS\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# can't create a 250 000 X 250 000 Hessian matrix, splitting data\n",
    "loss_tr,loss_te,final_newton_split_w=split_data_newton(y,tx,ratio,max_iter,gamma,seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_tr[len(loss_tr-1)],loss_te,final_newton_split_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penalized Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tx=build_tx(stdx)\n",
    "max_iter=100\n",
    "gamma=0.000000001\n",
    "seed=1\n",
    "ratio=20000/250000\n",
    "lambda_=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# can't create a 250 000 X 250 000 Hessian matrix, splitting data\n",
    "loss_tr,loss_te,final_penalized_split_w=split_data_penalized_logistic_regression(y,tx,ratio,max_iter,gamma,lambda_,seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../test.csv' # TODO: download test data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building polynomial  0\n",
      "Building polynomial  100000\n",
      "Building polynomial  200000\n",
      "Building polynomial  300000\n",
      "Building polynomial  400000\n",
      "Building polynomial  500000\n"
     ]
    }
   ],
   "source": [
    "test_x_poly=build_poly(tX_test,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "sample  200000\n",
      "sample  250000\n",
      "sample  300000\n",
      "sample  350000\n",
      "sample  400000\n",
      "sample  450000\n",
      "sample  500000\n",
      "sample  550000\n"
     ]
    }
   ],
   "source": [
    "w=final_CV_ridge_w.T   # We will process by degree not by dim\n",
    "y_predicted=np.zeros(tX_test.shape[0])\n",
    "y_predicted_classed=np.ones(tX_test.shape[0])\n",
    "for n in range(0,test_x_poly.shape[0]):\n",
    "    if(n%50000==0):print(\"sample \",n)\n",
    "    for degree in range(0,test_x_poly.shape[1]):\n",
    "        y_predicted[n]+=w[degree] @ test_x_poly[n,degree]\n",
    "    if(y_predicted[n]<0):y_predicted_classed[n]=-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 30)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_classed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '..submission.csv' # TODO: fill in desired name of output file for submission\n",
    "# y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_predicted, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
