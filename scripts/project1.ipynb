{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement ML functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(y, tx, w):\n",
    "    return compute_cost_MSE(y,tx,w)\n",
    "    \n",
    "def compute_cost_MSE(y,tx,w):\n",
    "    e=y-(tx @ w)\n",
    "    return (1/(2*y.shape[0]))*(e.T @ e)\n",
    "def compute_cost_MAE(y,tx,w):\n",
    "    e=y-(tx @ w)\n",
    "    return (1/y.shape[0])*np.absolute(e).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_gradient_MSE(y,tx,w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    e=y-(tx @ w)\n",
    "    return -1/y.shape[0]*(tx.T @ e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def general_gradient_descent(y, tx, initial_w, max_iters, gamma, grad_function, cost_function):\n",
    "    \"\"\"Gradient descent algorithm who work with arbitrary gradient and cost function\n",
    "    grad and cost function should take y,tw and w as parameter and return resÃªctivly the gradient vector and the scalar error\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w.ravel()]\n",
    "    losses = []\n",
    "    w = initial_w.ravel()\n",
    "    for n_iter in range(max_iters):\n",
    "        #compute gradient and loss\n",
    "        gradient=grad_function(y,tx,w)\n",
    "        loss=cost_function(y,tx,w)\n",
    "        if n_iter % 1 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=n_iter, l=loss))\n",
    "        #update w by gradient\n",
    "        w=w-gamma*gradient\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "        #print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "        #     bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses[-1], ws[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def general_stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma, stock_grad_function, cost_function):\n",
    "    \"\"\"Gradient descent algorithm who work with arbitrary gradient and cost function\n",
    "    grad and cost function should take y,tw and w as parameter and return respectivly the gradient vector and the scalar error\"\"\"\n",
    "    \n",
    "    # implement stochastic gradient descent.\n",
    "    ws = [initial_w.ravel()]\n",
    "    losses = []\n",
    "    w = initial_w.ravel()\n",
    "    \n",
    "    minibatchs = batch_iter(y, tx, batch_size, num_batches=math.floor(y.shape[0]/batch_size))\n",
    "    for n_iter in range(0,max_epochs):\n",
    "        \n",
    "        # compute gradient and loss\n",
    "        minibatch=minibatchs.__next__()\n",
    "        gradient=stock_grad_function(minibatch[0],minibatch[1],w)\n",
    "        loss=cost_function(y,tx,w)\n",
    "        \n",
    "        # update w by gradient\n",
    "        w=w-gamma*gradient\n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses[-1], ws[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, gamma, max_iters):\n",
    "    return general_gradient_descent(y,tx,np.zeros((tx.shape[1], 1)).ravel(),max_iters,gamma,compute_gradient_MSE,compute_cost_MSE)\n",
    "\n",
    "#general_gradient_descent(y,tX,np.zeros((tX.shape[1], 1)),20,0.000003,compute_gradient_MSE,compute_cost_MSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, gamma, max_iters):\n",
    "    batch_size = y.shape[0]//2\n",
    "    return general_stochastic_gradient_descent(y,tx,np.zeros((tx.shape[1], 1)),batch_size,max_iters,gamma,compute_gradient_MSE,compute_cost_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    w = np.linalg.solve(tx.T.dot(tx),tx.T.dot(y)) #return best weight\n",
    "    return compute_cost_MSE(y, tx, w), w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lamb):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    \n",
    "    lambp = lamb*(2*tx.shape[0])\n",
    "    #return np.linalg.inv(tx.T.dot(tx)+lambp*np.eye(tx.shape[1])).dot(tx.T).dot(y)\n",
    "    w =  np.linalg.solve(tx.T.dot(tx)+lambp*np.eye(tx.shape[1]),tx.T.dot(y))\n",
    "    return compute_cost_MSE(y, tx, w), w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    z = np.exp(t)\n",
    "    return z/(1+z)\n",
    "\n",
    "def compute_loss_logistic(y, tx, w, lambda_ = 0):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    clip = np.clip(tX @ w, -700, 700)\n",
    "    if lambda_ == 0:\n",
    "        return np.sum(np.log(1+np.exp(clip))-y*(tx @ w))\n",
    "    else:\n",
    "        return np.sum(np.log(1+np.exp(clip))-y*(tx @ w)) + lambda_*np.sum(w*w) #or + lambda_*w.T*w\n",
    "    #return -np.sum(np.log(1+np.exp(tx @ w))-y*(tx @ w))\n",
    "    \n",
    "def compute_gradient_sigmoid(y, tx, w, lambda_ = 0):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    clip = np.clip(tX @ w, -700, 700)\n",
    "    if lambda_ == 0:\n",
    "        return tx.T.dot(sigmoid(clip)-y)\n",
    "    else:\n",
    "        return tx.T.dot(sigmoid(clip)-y) + lambda_*2*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, gamma ,max_iters):\n",
    "    print(\"fuck\")\n",
    "    ok = general_gradient_descent(y,tx,np.zeros((tx.shape[1], 1)),max_iters,gamma,compute_gradient_sigmoid,compute_loss_logistic)\n",
    "    return ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reg_logistic_regression(y, tx, lambda_ , gamma, max_iters):\n",
    "    gradf = partial(compute_gradient_sigmoid,lambda_ = lambda_)\n",
    "    costf = partial(compute_loss_logistic, lambda_ = lambda_)\n",
    "    return general_gradient_descent(y,tx,np.zeros((tx.shape[1], 1)),max_iters,gamma,gradf,costf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuck\n",
      "Current iteration=0, the loss=173286.79513998624\n",
      "Current iteration=1, the loss=-2725323.636153825\n",
      "Current iteration=2, the loss=-3971997.8492310946\n",
      "Current iteration=3, the loss=-5216589.725502855\n",
      "Current iteration=4, the loss=-6459791.38545703\n",
      "Current iteration=5, the loss=-7702040.284123596\n",
      "Current iteration=6, the loss=-8943607.951261211\n",
      "Current iteration=7, the loss=-10184668.000161488\n",
      "Current iteration=8, the loss=-11425336.560482027\n",
      "Current iteration=9, the loss=-12665694.868688703\n",
      "Current iteration=10, the loss=-13905802.001533179\n",
      "Current iteration=11, the loss=-15145702.310699685\n",
      "Current iteration=12, the loss=-16385429.957667599\n",
      "Current iteration=13, the loss=-17625011.804495726\n",
      "Current iteration=14, the loss=-18864469.33292817\n",
      "Current iteration=15, the loss=-20103819.964558963\n",
      "Current iteration=16, the loss=-21343077.99692411\n",
      "Current iteration=17, the loss=-22582255.284399487\n",
      "Current iteration=18, the loss=-23821361.744191263\n",
      "Current iteration=19, the loss=-25060405.73920481\n"
     ]
    }
   ],
   "source": [
    "w=np.zeros((tX.shape[1], 1))#.ravel()\n",
    "#ok = general_gradient_descent(y,tX,w,50,0.00000000003,compute_gradient_sigmoid,compute_loss_logistic)\n",
    "ok2 = logistic_regression(y,tX,0.00000000003,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.69314718  0.69314718  0.69314718 ...,  0.69314718  0.69314718\n",
      "  0.69314718]\n"
     ]
    }
   ],
   "source": [
    "w=np.zeros((tX.shape[1], 1)).ravel()\n",
    "y2 = y.copy()\n",
    "y2[np.where(y==-1)] = 0\n",
    "test = np.clip(tX @ w, -700, 700)\n",
    "zbub = compute_loss_logistic(y2,tX,w,0.03)\n",
    "zbub2 = compute_gradient_sigmoid(y2,tX,w,0.03)\n",
    "tX2 = tX.copy()\n",
    "tX2[np.where(tX2==-999)] = 0\n",
    "e=y-(tX @ w)\n",
    "print(np.log(1+np.exp(test))-y*(tX @ w))\n",
    "#ok = general_gradient_descent(y2,tX2,w,50,0.00000000003,compute_gradient_sigmoid,compute_loss_logistic)\n",
    "ok = least_squares_GD(y2,tX2,0.00000000003,20)\n",
    "#ok = general_gradient_descent(y2,tX,w,20,0.0000003,compute_gradient_MSE,compute_cost_MSE)\n",
    "#ok = general_gradient_descent(y2,tX,w,10,-0.003,compute_gradient_MSE,compute_cost_MSE)\n",
    "#print((tX @ w).shape,zbub,zbub2,ok)\n",
    "#print(txs.shape,y.shape[1])\n",
    "#compute_loss_logistic(y,test,w)\n",
    "\n",
    "#a = compute_loss_logistic(y,tX,w)\n",
    "#print(txs,sigmoid(-700))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# _ ,weights = least_squares(y,tX)\n",
    "losss,weights = least_squares(y,tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.339686809477 [  8.03494352e-05  -7.20202266e-03  -6.05417274e-03  -5.47559078e-04\n",
      "  -1.93874687e-02   4.73451613e-04  -2.60379058e-02   3.25106299e-01\n",
      "  -3.80780000e-05  -2.72788699e+00  -2.21220141e-01   9.50794097e-02\n",
      "   6.40351609e-02   2.73614667e+00  -3.31801090e-04  -9.54325141e-04\n",
      "   2.74090341e+00  -5.34165279e-04   9.73498912e-04   3.69225050e-03\n",
      "   3.54487164e-04  -5.43344618e-04  -3.30448035e-01  -1.40800497e-03\n",
      "   8.31432840e-04   1.02117276e-03  -1.68047418e-03  -5.83664793e-03\n",
      "  -1.11088004e-02   2.72834692e+00]\n"
     ]
    }
   ],
   "source": [
    "print(losss,weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../submission.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    return compute_gradient_MSE(y,tx,w)\n",
    "\n",
    "def compute_gradient_MSE(y,tx,w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    e=y-(tx @ w)\n",
    "    return -1/y.shape[0]*tx.T @ e\n",
    "\n",
    "def gradient_descent(y, tx, initial_w, max_iters, gamma): \n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        #compute gradient and loss\n",
    "        gradient=compute_gradient(y,tx,w)\n",
    "        loss=compute_cost(y,tx,w)\n",
    "        \n",
    "        #update w by gradient\n",
    "        w=w-gamma*gradient\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses[-1], ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient for batch data.\"\"\"\n",
    "\n",
    "    return compute_gradient(y,tx,w)\n",
    "\n",
    "import math\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_epochs, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    \n",
    "    # implement stochastic gradient descent.\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    minibatchs = batch_iter(y, tx, batch_size, num_batches=math.floor(y.shape[0]/batch_size))\n",
    "    for n_iter in range(0,max_epochs):\n",
    "        \n",
    "        # compute gradient and loss\n",
    "        minibatch=minibatchs.__next__()\n",
    "        gradient=compute_stoch_gradient(minibatch[0],minibatch[1],w)\n",
    "        loss=compute_cost(y,tx,w)\n",
    "        \n",
    "        # update w by gradient\n",
    "        w=w-gamma*gradient\n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
