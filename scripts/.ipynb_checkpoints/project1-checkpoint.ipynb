{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from helpers import *\n",
    "from costs import *\n",
    "DATA_TRAIN_PATH = '../train.csv' # TODO: download train data and supply path here \n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stdx, mean_x, std_x=standardize(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., -1., -1., ...,  1., -1., -1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional loss and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return np.exp(t)/(1+np.exp(t))\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    poly=np.ones((x.shape[0],degree+1,x.shape[1]))\n",
    "    for i in range(0,x.shape[0]):\n",
    "        if i%100000==0: print(\"Building polynomial \", i)\n",
    "        for d in range(1, degree+1):\n",
    "            for j in range(0,x.shape[1]):\n",
    "                poly[i][d][j]=np.power(x[i,j],d)\n",
    "    return poly\n",
    "\n",
    "def compute_rmse(y,tx,w):\n",
    "    e=y-(tx @ w)\n",
    "    return math.sqrt(1/y.shape[0]*(e @ e))\n",
    "\n",
    "\n",
    "def calculate_loss_logistic_regression(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    a=0\n",
    "    for n in range(y.shape[0]):    \n",
    "        a+=np.log(1+np.exp(tx[n].T @ w))-y[n]*tx[n].T @ w\n",
    "    return a[0]\n",
    "\n",
    "def build_tx(x):\n",
    "    return np.c_[np.ones((x.shape[0], 1)), x]\n",
    "\n",
    "\n",
    "def compute_loss_poly(train_y,train_tx,weight):\n",
    "    w=weight.T   # We will process by degree not by dim\n",
    "    y_predicted=np.zeros(train_y.shape[0])\n",
    "    y_predicted_classed=np.ones(train_y.shape[0])\n",
    "    for n in range(0,train_tx.shape[0]):\n",
    "        if(n%50000==0):print(\"sample \",n)\n",
    "        for degree in range(0,train_tx.shape[1]):\n",
    "            y_predicted[n]+=w[degree] @ train_tx[n,degree]\n",
    "        if(y_predicted[n]<0):y_predicted_classed[n]=-1\n",
    "    e=train_y-y_predicted_classed\n",
    "    return np.sqrt(1/train_y.shape[0]*(e @ e))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_gradient_MSE(y,tx,w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    e=y-(tx @ w)\n",
    "    return -1/y.shape[0]*tx.T @ e\n",
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient for batch data.\"\"\"\n",
    "    return compute_gradient_MSE(y,tx,w)\n",
    "\n",
    "def compute_gradient_MAE(y,tx,w):\n",
    "    sumX=0\n",
    "    sumY=0\n",
    "    for n in range(0,y.shape[0]):\n",
    "        temp=y[n]-w[0]-w[1]*tx[n,1]\n",
    "        if(temp>0):\n",
    "            sumX=sumX-1\n",
    "            sumY=sumY-tx[n,1]\n",
    "        if(temp<0):\n",
    "            sumX=sumX+1\n",
    "            sumY=sumY+tx[n,1]\n",
    "    return np.array([sumX/y.shape[0],sumY/y.shape[0]])\n",
    "\n",
    "def calculate_gradient_logistic_regression(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return tx.T @ (sigmoid(tx @ w)-y)\n",
    "\n",
    "def calculate_hessian(y, tx, w):\n",
    "    \"\"\"return the hessian of the loss function.\"\"\"\n",
    "    # calculate hessian: \n",
    "    S=np.zeros((y.shape[0],y.shape[0]))\n",
    "    for i in range(0,y.shape[0]):\n",
    "        S[i,i]=sigmoid(tx[i].T @ w)*(1-sigmoid(tx[i].T @ w))\n",
    "    return tx.T @ S @ tx\n",
    "\n",
    "\n",
    "def logistic_regression_newton(y, tx, w):\n",
    "    \"\"\"return the loss, gradient, and hessian.\"\"\"\n",
    "    # return loss, gradient, and hessian:\n",
    "    return calculate_loss_logistic_regression(y,tx,w),calculate_gradient_logistic_regression(y,tx,w),calculate_hessian(y,tx,w)\n",
    "\n",
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss, gradient, and hessian.\"\"\"\n",
    "    # return loss, gradient, and hessian:\n",
    "    loss,gradient,hessian=logistic_regression_newton(y,tx,w)\n",
    "    loss+= lambda_*np.sum(w*w)\n",
    "    gradient+=lambda_*np.sum(2*w)\n",
    "    hessian+=lambda_*2*w.shape[0]\n",
    "    return loss,gradient,hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent (one step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent_logistic_regression(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    # compute the cost:\n",
    "    #loss=calculate_loss_logistic_regression(y,tx,w)\n",
    "    loss=compute_loss(y,tx,w)\n",
    "    # compute the gradient:\n",
    "    gradient=calculate_gradient_logistic_regression(y,tx,w)\n",
    "    # update w:\n",
    "    w=w-gamma*gradient\n",
    "    return loss, w\n",
    "\n",
    "def learning_by_newton_method(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step on Newton's method.\n",
    "    return the loss and updated w.\n",
    "    \"\"\"\n",
    "    # return loss, gradient and hessian:\n",
    "    loss,gradient,hessian = logistic_regression_newton(y,tx,w)\n",
    "    # update w:\n",
    "    w=w-gamma * np.linalg.inv(hessian) @ gradient\n",
    "    return loss, w\n",
    "\n",
    "def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated w.\n",
    "    \"\"\"\n",
    "    # return loss, gradient and hessian:\n",
    "    \n",
    "    loss,gradient,hessian = penalized_logistic_regression(y,tx,w,lambda_)\n",
    "    # update w:\n",
    "    w=w-gamma * np.linalg.inv(hessian) @ gradient\n",
    "    return loss, w\n",
    "\n",
    "\n",
    "# DO GD linear regression with mse\n",
    "def learning_by_GD_mse(y,tx,w,lambda_):\n",
    "    return compute_loss(y,tx,w),w-lambda_*compute_gradient_MSE(y,tx,w)\n",
    "# DO SGD linear regression with mse\n",
    "def learning_by_SGD_mse(y,tx,w,lambda_):\n",
    "    return compute_loss(y,tx,w),w-lambda_*compute_stoch_gradient(y,tx,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    w=np.linalg.inv(tx.T @ tx) @ tx.T @ y\n",
    "    return compute_loss(y,tx,w),w\n",
    "\n",
    "def ridge_regression(y, teta, lamb):\n",
    "    \"\"\"implement ridge regression.\n",
    "    # For each dimension we have the weights\n",
    "    ws=np.zeros((tx.shape[1],tx.shape[2]))\n",
    "    \n",
    "    for dim in range(0,tx.shape[2]):\n",
    "        tx_per_dim=tx[:,dim,:] #one dimension\n",
    "        ws[dim]=(np.linalg.solve((tx_per_dim.T @ tx_per_dim)+lamb*np.identity(tx_per_dim.shape[1]), tx_per_dim.T @ y))\n",
    "    # We calculate the average\n",
    "    return ws\n",
    "    \"\"\"\n",
    "    \n",
    "    teta_t=np.transpose(teta,(2,1,0))\n",
    "    teta_good= np.transpose(teta,(2,0,1))\n",
    "    to_add=(teta_t @ teta_good)\n",
    "    \n",
    "    to_inv=to_add+lamb*2*teta.shape[0]*np.identity(to_add.shape[1])\n",
    "    \n",
    "    #inv=np.linalg.inv(to_inv)\n",
    "    #a=inv @ teta_t\n",
    "    #b=a @ y\n",
    "    b=teta_t @ y\n",
    "    \n",
    "    w=np.linalg.solve(to_inv,b) #/teta.shape[0]\n",
    "    # is of form (Dimension X degree)\n",
    "    return w\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent_mse(y, tx, initial_w, max_iters, gamma): \n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss,w=learning_by_GD_mse(y,tx,w,gamma)\n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "    return losses, ws\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iter, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    minibatchs = batch_iter(y, tx, batch_size, num_batches=np.int(y.shape[0]/batch_size))\n",
    "    num_batches=np.int(y.shape[0]/batch_size)\n",
    "    for n_iter in range(0,np.int(np.min([max_iter,num_batches]))):\n",
    "        # compute gradient and loss\n",
    "        minibatch=minibatchs.__next__()\n",
    "        loss,w=learning_by_SGD_mse(minibatch[0],minibatch[1],w,gamma)\n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "    return losses, ws\n",
    "\n",
    "def logistic_regression_gradient_descent(y, x, max_iter, threshold,gamma):\n",
    "    # init parameters\n",
    "    losses = []\n",
    "    #tx=build_tx(x)\n",
    "    tx=x\n",
    "    w = np.zeros((tx.shape[1]))\n",
    "    ws=[]\n",
    "    ws.append(w)\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        \n",
    "        loss, w = learning_by_gradient_descent_logistic_regression(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 1000 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criteria\n",
    "        losses.append(loss)\n",
    "        ws.append(w)\n",
    "    return losses,ws\n",
    "\n",
    "def logistic_regression_newton_method(y, x,max_iters,threshold,gamma):\n",
    "    # init parameters\n",
    "    losses = []\n",
    "    #tx=build_tx(x)\n",
    "    tx=x\n",
    "    w = np.zeros((tx.shape[1]))\n",
    "    ws=[]\n",
    "    ws.append(w)\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_newton_method(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 500 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criteria\n",
    "        losses.append(loss)\n",
    "        ws.append(w)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    # visualization\n",
    "    visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_newton_method\")\n",
    "    print(\"The loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "\n",
    "def logistic_regression_penalized_gradient_descent(y, x,max_iters,threshold,gamma,lambda_):\n",
    "    # init parameters\n",
    "    losses = []\n",
    "    #tx=build_tx(x)\n",
    "    tx=x\n",
    "    w = np.zeros((tx.shape[1]))\n",
    "    ws=[]\n",
    "    ws.append(w)\n",
    "    \n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "        # log info\n",
    "        if iter % 500 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criteria\n",
    "        losses.append(loss)\n",
    "        ws.append(w)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    # visualization\n",
    "    visualization(y, x, mean_x, std_x, w, \"classification_by_logistic_regression_penalized_gradient_descent\")\n",
    "    print(\"The loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    \n",
    "    \n",
    "def ridge_regression_ml_function(y,x_powered,lambda_):\n",
    "    \"\"\"ridge regression demo.\"\"\"\n",
    "    # form train and test data with polynomial basis function: TODO\n",
    "    # ***************************************************\n",
    "    #train_tx=build_poly(x,degree)\n",
    "    train_tx=x_powered\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # ridge regression with different lambda: TODO\n",
    "    # ***************************************************\n",
    "    weight = ridge_regression(y,train_tx,lambda_)\n",
    "    loss=compute_loss_poly(y,train_tx,weight)\n",
    "    return loss,weight\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation(y, x_poly, k_indices, k, lambda_):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # get k'th subgroup in test, others in train: TODO\n",
    "    # ***************************************************\n",
    "    train_indices=k_indices[[i for i in range(len(k_indices)) if i != k]]\n",
    "    train_tx,train_y=x_poly[np.ravel(train_indices)],y[np.ravel(train_indices)]\n",
    "    test_tx,test_y=x_poly[k_indices[k]],y[k_indices[k]]\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # ridge regression: TODO\n",
    "    # ***************************************************\n",
    "    weight = ridge_regression(train_y,train_tx,lambda_)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # calculate the loss for train and test data: TODO\n",
    "    # ***************************************************\n",
    "    loss_tr=compute_loss_poly(train_y,train_tx,weight)\n",
    "    loss_te=compute_loss_poly(test_y,test_tx,weight)\n",
    "    return loss_tr, loss_te,weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Testing different methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tx=build_tx(x)\n",
    "initial_w=np.ones(tx.shape[1])\n",
    "max_iters=500\n",
    "gamma =0.0000000001\n",
    "losses,ws=gradient_descent_mse(y, tx, initial_w, max_iters, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "losses,ws=gradient_descent_mse(y, tx, ws[len(ws)-1], max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_ws=ws[len(ws)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14488053.728343694,\n",
       " 14472114.754554447,\n",
       " 14456194.016681407,\n",
       " 14440291.493818505,\n",
       " 14424407.16508366,\n",
       " 14408541.009618729,\n",
       " 14392693.00658947,\n",
       " 14376863.13518556,\n",
       " 14361051.37462051,\n",
       " 14345257.704131668,\n",
       " 14329482.102980206,\n",
       " 14313724.550451051,\n",
       " 14297985.025852902,\n",
       " 14282263.508518172,\n",
       " 14266559.977802962,\n",
       " 14250874.413087063,\n",
       " 14235206.793773899,\n",
       " 14219557.099290501,\n",
       " 14203925.309087509,\n",
       " 14188311.402639106,\n",
       " 14172715.359443014,\n",
       " 14157137.159020472,\n",
       " 14141576.780916186,\n",
       " 14126034.204698326,\n",
       " 14110509.409958487,\n",
       " 14095002.376311658,\n",
       " 14079513.083396202,\n",
       " 14064041.510873837,\n",
       " 14048587.638429591,\n",
       " 14033151.445771797,\n",
       " 14017732.912632039,\n",
       " 14002332.018765138,\n",
       " 13986948.743949156,\n",
       " 13971583.067985311,\n",
       " 13956234.970697992,\n",
       " 13940904.431934722,\n",
       " 13925591.43156613,\n",
       " 13910295.949485919,\n",
       " 13895017.965610858,\n",
       " 13879757.459880732,\n",
       " 13864514.412258331,\n",
       " 13849288.802729417,\n",
       " 13834080.611302707,\n",
       " 13818889.818009825,\n",
       " 13803716.402905311,\n",
       " 13788560.346066553,\n",
       " 13773421.627593799,\n",
       " 13758300.2276101,\n",
       " 13743196.126261309,\n",
       " 13728109.303716039,\n",
       " 13713039.740165638,\n",
       " 13697987.415824169,\n",
       " 13682952.31092838,\n",
       " 13667934.405737681,\n",
       " 13652933.680534123,\n",
       " 13637950.115622349,\n",
       " 13622983.691329591,\n",
       " 13608034.388005653,\n",
       " 13593102.186022855,\n",
       " 13578187.065776002,\n",
       " 13563289.007682418,\n",
       " 13548407.99218186,\n",
       " 13533543.999736508,\n",
       " 13518697.01083095,\n",
       " 13503867.005972153,\n",
       " 13489053.965689426,\n",
       " 13474257.870534411,\n",
       " 13459478.701081038,\n",
       " 13444716.437925534,\n",
       " 13429971.061686344,\n",
       " 13415242.553004161,\n",
       " 13400530.89254185,\n",
       " 13385836.06098447,\n",
       " 13371158.039039219,\n",
       " 13356496.807435414,\n",
       " 13341852.34692446,\n",
       " 13327224.638279846,\n",
       " 13312613.662297098,\n",
       " 13298019.399793766,\n",
       " 13283441.831609383,\n",
       " 13268880.938605476,\n",
       " 13254336.701665487,\n",
       " 13239809.101694781,\n",
       " 13225298.119620642,\n",
       " 13210803.736392194,\n",
       " 13196325.932980424,\n",
       " 13181864.69037812,\n",
       " 13167419.989599872,\n",
       " 13152991.811682047,\n",
       " 13138580.137682743,\n",
       " 13124184.948681775,\n",
       " 13109806.225780657,\n",
       " 13095443.950102575,\n",
       " 13081098.102792358,\n",
       " 13066768.665016448,\n",
       " 13052455.61796288,\n",
       " 13038158.942841265,\n",
       " 13023878.620882764,\n",
       " 13009614.633340046,\n",
       " 12995366.961487276,\n",
       " 12981135.586620091,\n",
       " 12966920.49005558,\n",
       " 12952721.653132262,\n",
       " 12938539.057210028,\n",
       " 12924372.683670156,\n",
       " 12910222.513915271,\n",
       " 12896088.529369321,\n",
       " 12881970.711477557,\n",
       " 12867869.041706489,\n",
       " 12853783.501543902,\n",
       " 12839714.072498783,\n",
       " 12825660.736101337,\n",
       " 12811623.473902935,\n",
       " 12797602.267476112,\n",
       " 12783597.098414518,\n",
       " 12769607.948332919,\n",
       " 12755634.798867146,\n",
       " 12741677.631674115,\n",
       " 12727736.428431746,\n",
       " 12713811.170838974,\n",
       " 12699901.840615727,\n",
       " 12686008.419502877,\n",
       " 12672130.889262246,\n",
       " 12658269.23167656,\n",
       " 12644423.428549435,\n",
       " 12630593.461705344,\n",
       " 12616779.312989617,\n",
       " 12602980.964268377,\n",
       " 12589198.397428565,\n",
       " 12575431.594377866,\n",
       " 12561680.537044711,\n",
       " 12547945.207378272,\n",
       " 12534225.5873484,\n",
       " 12520521.658945616,\n",
       " 12506833.404181125,\n",
       " 12493160.8050867,\n",
       " 12479503.843714762,\n",
       " 12465862.502138294,\n",
       " 12452236.76245084,\n",
       " 12438626.606766459,\n",
       " 12425032.017219733,\n",
       " 12411452.975965738,\n",
       " 12397889.465179972,\n",
       " 12384341.467058415,\n",
       " 12370808.963817438,\n",
       " 12357291.937693793,\n",
       " 12343790.370944623,\n",
       " 12330304.245847402,\n",
       " 12316833.54469992,\n",
       " 12303378.249820279,\n",
       " 12289938.343546826,\n",
       " 12276513.808238203,\n",
       " 12263104.626273226,\n",
       " 12249710.780050967,\n",
       " 12236332.251990639,\n",
       " 12222969.024531636,\n",
       " 12209621.080133483,\n",
       " 12196288.401275801,\n",
       " 12182970.970458312,\n",
       " 12169668.770200813,\n",
       " 12156381.783043124,\n",
       " 12143109.991545105,\n",
       " 12129853.378286593,\n",
       " 12116611.925867401,\n",
       " 12103385.616907304,\n",
       " 12090174.434046004,\n",
       " 12076978.359943101,\n",
       " 12063797.377278076,\n",
       " 12050631.468750272,\n",
       " 12037480.617078874,\n",
       " 12024344.805002876,\n",
       " 12011224.015281064,\n",
       " 11998118.230691994,\n",
       " 11985027.434033949,\n",
       " 11971951.608124975,\n",
       " 11958890.735802779,\n",
       " 11945844.799924772,\n",
       " 11932813.783368004,\n",
       " 11919797.669029169,\n",
       " 11906796.439824559,\n",
       " 11893810.078690073,\n",
       " 11880838.56858116,\n",
       " 11867881.892472811,\n",
       " 11854940.033359544,\n",
       " 11842012.974255379,\n",
       " 11829100.6981938,\n",
       " 11816203.188227756,\n",
       " 11803320.427429616,\n",
       " 11790452.398891171,\n",
       " 11777599.085723579,\n",
       " 11764760.471057396,\n",
       " 11751936.53804248,\n",
       " 11739127.269848038,\n",
       " 11726332.64966256,\n",
       " 11713552.660693821,\n",
       " 11700787.286168842,\n",
       " 11688036.509333877,\n",
       " 11675300.313454399,\n",
       " 11662578.681815056,\n",
       " 11649871.597719658,\n",
       " 11637179.044491185,\n",
       " 11624501.00547171,\n",
       " 11611837.464022415,\n",
       " 11599188.403523562,\n",
       " 11586553.80737447,\n",
       " 11573933.65899349,\n",
       " 11561327.941817978,\n",
       " 11548736.639304303,\n",
       " 11536159.734927768,\n",
       " 11523597.212182647,\n",
       " 11511049.054582141,\n",
       " 11498515.245658344,\n",
       " 11485995.768962232,\n",
       " 11473490.608063651,\n",
       " 11460999.746551273,\n",
       " 11448523.16803259,\n",
       " 11436060.856133899,\n",
       " 11423612.794500258,\n",
       " 11411178.966795484,\n",
       " 11398759.356702127,\n",
       " 11386353.94792144,\n",
       " 11373962.724173367,\n",
       " 11361585.669196516,\n",
       " 11349222.766748145,\n",
       " 11336874.000604134,\n",
       " 11324539.354558961,\n",
       " 11312218.812425688,\n",
       " 11299912.358035941,\n",
       " 11287619.975239875,\n",
       " 11275341.647906162,\n",
       " 11263077.359921983,\n",
       " 11250827.095192984,\n",
       " 11238590.837643262,\n",
       " 11226368.571215356,\n",
       " 11214160.279870201,\n",
       " 11201965.947587138,\n",
       " 11189785.55836387,\n",
       " 11177619.09621645,\n",
       " 11165466.54517925,\n",
       " 11153327.889304956,\n",
       " 11141203.112664541,\n",
       " 11129092.199347228,\n",
       " 11116995.133460505,\n",
       " 11104911.899130063,\n",
       " 11092842.4804998,\n",
       " 11080786.861731796,\n",
       " 11068745.027006289,\n",
       " 11056716.960521651,\n",
       " 11044702.646494394,\n",
       " 11032702.069159087,\n",
       " 11020715.212768415,\n",
       " 11008742.061593093,\n",
       " 10996782.59992188,\n",
       " 10984836.812061558,\n",
       " 10972904.682336884,\n",
       " 10960986.195090599,\n",
       " 10949081.334683398,\n",
       " 10937190.0854939,\n",
       " 10925312.431918642,\n",
       " 10913448.358372044,\n",
       " 10901597.849286409,\n",
       " 10889760.889111873,\n",
       " 10877937.462316416,\n",
       " 10866127.553385818,\n",
       " 10854331.146823652,\n",
       " 10842548.227151256,\n",
       " 10830778.778907709,\n",
       " 10819022.786649833,\n",
       " 10807280.234952148,\n",
       " 10795551.108406855,\n",
       " 10783835.391623829,\n",
       " 10772133.069230592,\n",
       " 10760444.125872279,\n",
       " 10748768.546211654,\n",
       " 10737106.314929049,\n",
       " 10725457.416722354,\n",
       " 10713821.836307023,\n",
       " 10702199.558416028,\n",
       " 10690590.567799844,\n",
       " 10678994.84922643,\n",
       " 10667412.387481205,\n",
       " 10655843.167367045,\n",
       " 10644287.173704246,\n",
       " 10632744.391330507,\n",
       " 10621214.805100907,\n",
       " 10609698.399887901,\n",
       " 10598195.160581278,\n",
       " 10586705.072088154,\n",
       " 10575228.119332964,\n",
       " 10563764.287257407,\n",
       " 10552313.560820464,\n",
       " 10540875.92499835,\n",
       " 10529451.364784511,\n",
       " 10518039.865189606,\n",
       " 10506641.41124147,\n",
       " 10495255.987985114,\n",
       " 10483883.580482678,\n",
       " 10472524.173813455,\n",
       " 10461177.75307383,\n",
       " 10449844.30337728,\n",
       " 10438523.80985434,\n",
       " 10427216.257652612,\n",
       " 10415921.631936722,\n",
       " 10404639.917888299,\n",
       " 10393371.100705964,\n",
       " 10382115.165605318,\n",
       " 10370872.097818902,\n",
       " 10359641.882596195,\n",
       " 10348424.505203588,\n",
       " 10337219.950924365,\n",
       " 10326028.205058686,\n",
       " 10314849.252923559,\n",
       " 10303683.079852834,\n",
       " 10292529.671197169,\n",
       " 10281389.012324031,\n",
       " 10270261.088617655,\n",
       " 10259145.885479035,\n",
       " 10248043.388325905,\n",
       " 10236953.582592722,\n",
       " 10225876.453730633,\n",
       " 10214811.987207484,\n",
       " 10203760.168507772,\n",
       " 10192720.983132625,\n",
       " 10181694.416599823,\n",
       " 10170680.454443729,\n",
       " 10159679.082215302,\n",
       " 10148690.285482068,\n",
       " 10137714.049828088,\n",
       " 10126750.36085397,\n",
       " 10115799.204176828,\n",
       " 10104860.565430256,\n",
       " 10093934.430264326,\n",
       " 10083020.784345573,\n",
       " 10072119.613356953,\n",
       " 10061230.902997844,\n",
       " 10050354.638984015,\n",
       " 10039490.807047622,\n",
       " 10028639.392937178,\n",
       " 10017800.38241753,\n",
       " 10006973.761269851,\n",
       " 9996159.5152916238,\n",
       " 9985357.6302965935,\n",
       " 9974568.0921148006,\n",
       " 9963790.8865925111,\n",
       " 9953025.9995922297,\n",
       " 9942273.4169926606,\n",
       " 9931533.1246887129,\n",
       " 9920805.1085914597,\n",
       " 9910089.3546281271,\n",
       " 9899385.848742079,\n",
       " 9888694.5768927969,\n",
       " 9878015.5250558648,\n",
       " 9867348.6792229339,\n",
       " 9856694.025401732,\n",
       " 9846051.5496160202,\n",
       " 9835421.2379055955,\n",
       " 9824803.0763262417,\n",
       " 9814197.0509497523,\n",
       " 9803603.1478638761,\n",
       " 9793021.3531723209,\n",
       " 9782451.6529947221,\n",
       " 9771894.0334666371,\n",
       " 9761348.4807395134,\n",
       " 9750814.9809806813,\n",
       " 9740293.5203733239,\n",
       " 9729784.0851164795,\n",
       " 9719286.6614249945,\n",
       " 9708801.2355295345,\n",
       " 9698327.7936765477,\n",
       " 9687866.3221282475,\n",
       " 9677416.8071626034,\n",
       " 9666979.2350733168,\n",
       " 9656553.5921698157,\n",
       " 9646139.8647771999,\n",
       " 9635738.0392362699,\n",
       " 9625348.1019034833,\n",
       " 9614970.0391509328,\n",
       " 9604603.8373663481,\n",
       " 9594249.4829530586,\n",
       " 9583906.9623299818,\n",
       " 9573576.2619316187,\n",
       " 9563257.3682080116,\n",
       " 9552950.2676247433,\n",
       " 9542654.9466629159,\n",
       " 9532371.3918191269,\n",
       " 9522099.5896054693,\n",
       " 9511839.5265494864,\n",
       " 9501591.1891941708,\n",
       " 9491354.5640979595,\n",
       " 9481129.6378346793,\n",
       " 9470916.3969935663,\n",
       " 9460714.8281792346,\n",
       " 9450524.9180116355,\n",
       " 9440346.6531261001,\n",
       " 9430180.0201732386,\n",
       " 9420025.0058189984,\n",
       " 9409881.5967446119,\n",
       " 9399749.7796465643,\n",
       " 9389629.5412366148,\n",
       " 9379520.8682417497,\n",
       " 9369423.747404173,\n",
       " 9359338.1654812936,\n",
       " 9349264.1092456952,\n",
       " 9339201.5654851422,\n",
       " 9329150.5210025329,\n",
       " 9319110.9626159091,\n",
       " 9309082.8771584202,\n",
       " 9299066.2514783144,\n",
       " 9289061.0724389181,\n",
       " 9279067.3269186188,\n",
       " 9269085.001810858,\n",
       " 9259114.0840240866,\n",
       " 9249154.560481783,\n",
       " 9239206.4181224164,\n",
       " 9229269.6438994221,\n",
       " 9219344.2247812059,\n",
       " 9209430.1477511022,\n",
       " 9199527.3998073842,\n",
       " 9189635.9679632261,\n",
       " 9179755.8392466903,\n",
       " 9169887.0007007197,\n",
       " 9160029.4393831007,\n",
       " 9150183.1423664689,\n",
       " 9140348.0967382807,\n",
       " 9130524.2896007989,\n",
       " 9120711.7080710717,\n",
       " 9110910.3392809238,\n",
       " 9101120.1703769211,\n",
       " 9091341.1885203775,\n",
       " 9081573.3808873314,\n",
       " 9071816.7346685138,\n",
       " 9062071.2370693479,\n",
       " 9052336.8753099293,\n",
       " 9042613.6366250012,\n",
       " 9032901.5082639493,\n",
       " 9023200.4774907716,\n",
       " 9013510.5315840766,\n",
       " 9003831.6578370575,\n",
       " 8994163.843557464,\n",
       " 8984507.0760676209,\n",
       " 8974861.3427043706,\n",
       " 8965226.6308190748,\n",
       " 8955602.9277776126,\n",
       " 8945990.2209603451,\n",
       " 8936388.4977620877,\n",
       " 8926797.745592121,\n",
       " 8917217.9518741667,\n",
       " 8907649.1040463485,\n",
       " 8898091.1895612106,\n",
       " 8888544.1958856769,\n",
       " 8879008.1105010435,\n",
       " 8869482.9209029544,\n",
       " 8859968.6146013923,\n",
       " 8850465.179120671,\n",
       " 8840972.6019993965,\n",
       " 8831490.8707904648,\n",
       " 8822019.9730610475,\n",
       " 8812559.8963925708,\n",
       " 8803110.6283806954,\n",
       " 8793672.1566353068,\n",
       " 8784244.4687805027,\n",
       " 8774827.5524545591,\n",
       " 8765421.3953099363,\n",
       " 8756025.9850132428,\n",
       " 8746641.3092452418,\n",
       " 8737267.3557008002,\n",
       " 8727904.112088915,\n",
       " 8718551.566132661,\n",
       " 8709209.7055692002,\n",
       " 8699878.5181497466,\n",
       " 8690557.991639562,\n",
       " 8681248.1138179358,\n",
       " 8671948.872478174,\n",
       " 8662660.2554275673,\n",
       " 8653382.2504873928,\n",
       " 8644114.8454928976,\n",
       " 8634858.0282932632,\n",
       " 8625611.7867516149,\n",
       " 8616376.1087449901,\n",
       " 8607150.9821643196,\n",
       " 8597936.394914424,\n",
       " 8588732.334913997,\n",
       " 8579538.7900955733,\n",
       " 8570355.748405531,\n",
       " 8561183.1978040691,\n",
       " 8552021.1262651868,\n",
       " 8542869.5217766762,\n",
       " 8533728.3723400999,\n",
       " 8524597.6659707725,\n",
       " 8515477.3906977624,\n",
       " 8506367.5345638581,\n",
       " 8497268.0856255461,\n",
       " 8488179.0319530256,\n",
       " 8479100.3616301604,\n",
       " 8470032.0627544895,\n",
       " 8460974.1234371867,\n",
       " 8451926.5318030659,\n",
       " 8442889.2759905513,\n",
       " 8433862.3441516664,\n",
       " 8424845.7244520281]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_ws_GD_mse=ws[len(ws)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient descent mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP Pace\\Documents\\GitHub\\MLO-project1\\scripts\\helpers.py:49: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[23894990.032122701,\n",
       " 25657670.379684858,\n",
       " 24903585.608309388,\n",
       " 25234665.7760607,\n",
       " 24525918.98082418,\n",
       " 27229180.305197425,\n",
       " 23893760.627589416,\n",
       " 24317171.430089876,\n",
       " 25497332.62087208,\n",
       " 26178514.923334233,\n",
       " 25613957.040389273,\n",
       " 25010799.759458899,\n",
       " 25230163.600142904,\n",
       " 26332749.419346664,\n",
       " 24382442.741232626,\n",
       " 26004544.561279036,\n",
       " 26061209.528096363,\n",
       " 24923551.815746401,\n",
       " 25802124.765765898,\n",
       " 25543702.273635849,\n",
       " 24021057.447276451,\n",
       " 23815810.734768759,\n",
       " 23675600.698433485,\n",
       " 25335564.124801941,\n",
       " 24833837.876984511,\n",
       " 24740637.745754316,\n",
       " 24471431.066504985,\n",
       " 24037185.460804623,\n",
       " 25885466.069239751,\n",
       " 23851224.447146829,\n",
       " 24714946.832639635,\n",
       " 24120284.354127016,\n",
       " 24303575.60932152,\n",
       " 24326974.839675538,\n",
       " 24737329.557919759,\n",
       " 24613234.881603666,\n",
       " 25259961.700384002,\n",
       " 24159630.617820475,\n",
       " 24329055.960784219,\n",
       " 23614106.493234016,\n",
       " 24038981.479804873,\n",
       " 23784971.585734118,\n",
       " 24220097.822157312,\n",
       " 23060491.250042986,\n",
       " 25183330.911386531,\n",
       " 22962251.439977624,\n",
       " 24362607.715491761,\n",
       " 24938742.710835841,\n",
       " 23581457.589651275,\n",
       " 23487355.836204264,\n",
       " 24427667.739528611,\n",
       " 22446131.438302234,\n",
       " 22324432.478570458,\n",
       " 24692760.044055596,\n",
       " 22351976.909161843,\n",
       " 21570036.161486145,\n",
       " 23104854.004934527,\n",
       " 23190671.480066586,\n",
       " 23898947.858024973,\n",
       " 24057606.09590197,\n",
       " 24914981.134760238,\n",
       " 24590554.770298451,\n",
       " 23304637.590321399,\n",
       " 23425792.647818778,\n",
       " 23803611.233290598,\n",
       " 23206020.754375424,\n",
       " 22941347.028208144,\n",
       " 22597584.638399176,\n",
       " 23379517.134552926,\n",
       " 22622199.081470076,\n",
       " 23770424.57571663,\n",
       " 23659017.690284319,\n",
       " 22427728.459286261,\n",
       " 22771861.01148269,\n",
       " 22979694.556351312,\n",
       " 22048761.126966491,\n",
       " 24031622.582884703,\n",
       " 23805956.613857858,\n",
       " 24082472.324725997,\n",
       " 23974713.492114142,\n",
       " 22334202.24568196,\n",
       " 21909720.513158012,\n",
       " 24406089.27706746,\n",
       " 22364360.927535545,\n",
       " 24216558.78390127,\n",
       " 23716190.032997482,\n",
       " 23679357.8715485,\n",
       " 23830182.75077536,\n",
       " 22670164.942645717,\n",
       " 23347288.878759697,\n",
       " 22448410.226235136,\n",
       " 24051595.337298166,\n",
       " 22457818.047299247,\n",
       " 23638078.192023262,\n",
       " 22910330.528796867,\n",
       " 22426622.641801447,\n",
       " 22670668.373700015,\n",
       " 21844477.010226056,\n",
       " 23370754.942803428,\n",
       " 21290966.623490795,\n",
       " 22186798.7618571,\n",
       " 22602497.518322725,\n",
       " 22649774.156439651,\n",
       " 22274862.938267983,\n",
       " 22758387.52939311,\n",
       " 21747244.532039642,\n",
       " 23016629.346040383,\n",
       " 21650694.720368415,\n",
       " 22704083.149559993,\n",
       " 20591128.977332328,\n",
       " 21197257.691066995,\n",
       " 22232246.155722789,\n",
       " 22441808.669714928,\n",
       " 22850851.859306447,\n",
       " 22532531.78040231,\n",
       " 22979089.931077488,\n",
       " 21548440.659933336,\n",
       " 20777760.933425054,\n",
       " 22819011.317926779,\n",
       " 21488615.769774124,\n",
       " 22284479.320999868,\n",
       " 22277791.165724367,\n",
       " 22713786.022424586,\n",
       " 23033619.505205598,\n",
       " 21674799.964272272,\n",
       " 21763025.058083434,\n",
       " 21176416.525925469,\n",
       " 22667969.251934938,\n",
       " 22520789.956698444,\n",
       " 20753545.14960717,\n",
       " 21677125.647146139,\n",
       " 21325911.599532876,\n",
       " 20708352.396199293,\n",
       " 21699541.219356824,\n",
       " 20839866.780270278,\n",
       " 21813981.289225157,\n",
       " 21807142.365919806,\n",
       " 21038755.23630755,\n",
       " 21344385.922906112,\n",
       " 21655391.415518988,\n",
       " 22092024.127526328,\n",
       " 20829405.804250322,\n",
       " 21680645.422985464,\n",
       " 21313081.317522433,\n",
       " 21208931.81026867,\n",
       " 20371697.734310538,\n",
       " 20371167.262416359,\n",
       " 23022619.186069991,\n",
       " 21683635.389562342,\n",
       " 22421600.155124087,\n",
       " 21399761.033416733,\n",
       " 21800919.220261712,\n",
       " 21775533.081784703,\n",
       " 21494608.913952574,\n",
       " 21038071.592320111,\n",
       " 20856493.833961561,\n",
       " 20707043.495946296,\n",
       " 21033389.461923074,\n",
       " 20667208.544712387,\n",
       " 21126716.865662713,\n",
       " 20739508.18562435,\n",
       " 21353766.214989975,\n",
       " 21287513.828991883,\n",
       " 20739775.498799976,\n",
       " 20924741.925517891,\n",
       " 20351393.156619225,\n",
       " 20515173.628410179,\n",
       " 19842221.292760268,\n",
       " 20725163.228692878,\n",
       " 20402418.734502126,\n",
       " 20535794.83594631,\n",
       " 19737370.476839583,\n",
       " 20569942.403062668,\n",
       " 21834868.71438678,\n",
       " 21283834.128242936,\n",
       " 21230760.548527297,\n",
       " 20220682.298196092,\n",
       " 20553829.989465877,\n",
       " 20934475.192700107,\n",
       " 19628036.553870842,\n",
       " 21087207.6529532,\n",
       " 21027842.186149217,\n",
       " 20579125.985734947,\n",
       " 21201892.027071111,\n",
       " 21341693.298488356,\n",
       " 19340909.856371194,\n",
       " 21846134.338241853,\n",
       " 20496973.77169409,\n",
       " 19050591.245252572,\n",
       " 20996431.652923759,\n",
       " 20863132.631949104,\n",
       " 20623957.453703824,\n",
       " 21139453.799203724,\n",
       " 21365139.160302665,\n",
       " 21674784.761106033,\n",
       " 20092540.134960387,\n",
       " 19911519.64096345,\n",
       " 20146658.332004074,\n",
       " 19917148.149005365,\n",
       " 20346129.735162202,\n",
       " 20378975.402481552,\n",
       " 21320233.179651327,\n",
       " 19694082.712823201,\n",
       " 20486405.216333304,\n",
       " 19725750.030620471,\n",
       " 18953827.54772146,\n",
       " 20463327.83442444,\n",
       " 19979955.850239586,\n",
       " 20050370.895669639,\n",
       " 20827776.717611205,\n",
       " 20011586.151042704,\n",
       " 19367726.226016279,\n",
       " 20907344.668895613,\n",
       " 20262453.101773132,\n",
       " 19350771.187953178,\n",
       " 19968411.296535797,\n",
       " 20480441.66893493,\n",
       " 19300111.992845628,\n",
       " 20457930.168633983,\n",
       " 19264844.361762717,\n",
       " 20495874.041906793,\n",
       " 20478782.709226973,\n",
       " 19525089.012247555,\n",
       " 19677651.075919945,\n",
       " 19395606.199468501,\n",
       " 18624685.933550198,\n",
       " 18582123.023166042,\n",
       " 18649310.42284536,\n",
       " 20030843.967505891,\n",
       " 19499035.804324295,\n",
       " 19359891.157636542,\n",
       " 18375111.214232378,\n",
       " 18461799.609918259,\n",
       " 19537420.539500661,\n",
       " 19198248.856554046,\n",
       " 18466603.26180587,\n",
       " 20238626.271185186,\n",
       " 18910668.805284973,\n",
       " 18949654.851792,\n",
       " 18870709.793587931,\n",
       " 19287341.502800453,\n",
       " 18925685.175442897,\n",
       " 19259267.993916009,\n",
       " 19377444.495051637,\n",
       " 20637596.528996143,\n",
       " 18421627.861343399,\n",
       " 19438092.748493034,\n",
       " 19741691.789534118,\n",
       " 19737578.055093337,\n",
       " 19459585.487587012,\n",
       " 18493677.182946377,\n",
       " 18118463.097019531,\n",
       " 19637611.52310501,\n",
       " 19161524.085698478,\n",
       " 18645792.393718056,\n",
       " 18620394.730192903,\n",
       " 18580554.918515302,\n",
       " 18705110.986939702,\n",
       " 19790352.449642789,\n",
       " 18426217.235553019,\n",
       " 18043909.30039943,\n",
       " 19883167.636301924,\n",
       " 19285995.017911162,\n",
       " 19405248.712855529,\n",
       " 17657537.511597291,\n",
       " 19226938.804713085,\n",
       " 18867185.497597288,\n",
       " 18464266.503507428,\n",
       " 19332656.285578653,\n",
       " 17398096.081860259,\n",
       " 18138467.356751047,\n",
       " 19099097.247469407,\n",
       " 18969985.434097204,\n",
       " 18111954.294814058,\n",
       " 17848530.366608262,\n",
       " 18241415.61254847,\n",
       " 20297294.075916968,\n",
       " 18334075.016478375,\n",
       " 19686357.767436568,\n",
       " 18862211.447272316,\n",
       " 17570585.669673547,\n",
       " 17257329.697980434,\n",
       " 18864008.747951575,\n",
       " 19156141.118265785,\n",
       " 17377149.222940043,\n",
       " 18300300.382989377,\n",
       " 18266552.890458327,\n",
       " 18973645.091006339,\n",
       " 18165946.793780692,\n",
       " 18687826.668700598,\n",
       " 18192249.598036762,\n",
       " 18813559.458645508,\n",
       " 18718420.809328429,\n",
       " 17743103.536678977,\n",
       " 18711321.223643288,\n",
       " 18556660.5697482,\n",
       " 18695301.484554362,\n",
       " 17844612.027375553,\n",
       " 18581238.651543826,\n",
       " 18768248.870836116,\n",
       " 17726223.222010009,\n",
       " 18766837.068636373,\n",
       " 18024423.322768185,\n",
       " 18810219.080480676,\n",
       " 18339048.366168961,\n",
       " 17916445.59215482,\n",
       " 19193601.14727477,\n",
       " 18012572.252752922,\n",
       " 19134260.714368939,\n",
       " 18349907.019380029,\n",
       " 16841938.121327933,\n",
       " 17880207.174580146,\n",
       " 18111533.910247441,\n",
       " 17555813.366461396,\n",
       " 17320173.957955368,\n",
       " 18639525.995702293,\n",
       " 17326898.569638349,\n",
       " 18255813.28196238,\n",
       " 18851300.922577348,\n",
       " 18183560.989816479,\n",
       " 17139513.756608728,\n",
       " 18010610.682255086,\n",
       " 17910221.641667314,\n",
       " 17510179.696656473,\n",
       " 17981689.557577617,\n",
       " 17191425.040136445,\n",
       " 17322199.900846355,\n",
       " 16634673.718742585,\n",
       " 17899242.496401716,\n",
       " 17360426.856132463,\n",
       " 16811299.742804784,\n",
       " 16789066.510608103,\n",
       " 17849306.316232968,\n",
       " 16280877.734405167,\n",
       " 17902554.2165953,\n",
       " 16339186.595214387,\n",
       " 17703438.049144462,\n",
       " 17788407.895660378,\n",
       " 17541466.682454836,\n",
       " 17722794.403547361,\n",
       " 17554992.185805418,\n",
       " 17297224.807563342,\n",
       " 18651124.528869499,\n",
       " 18629992.930227585,\n",
       " 16367369.102851389,\n",
       " 16724581.68157723,\n",
       " 16557308.706214897,\n",
       " 17253162.645105757,\n",
       " 17004278.316739868,\n",
       " 16103636.800871946,\n",
       " 17425936.299394734,\n",
       " 18275569.820550963,\n",
       " 16118220.466024006,\n",
       " 16658041.18200339,\n",
       " 15216469.960593253,\n",
       " 17134680.461848274,\n",
       " 16867996.691742789,\n",
       " 17014885.652890436,\n",
       " 16714696.399710266,\n",
       " 17344076.419076771,\n",
       " 16668230.673820829,\n",
       " 16404257.9998501,\n",
       " 17378876.949523509,\n",
       " 17022900.482614242,\n",
       " 17128266.886637423,\n",
       " 17302082.610169314,\n",
       " 17095687.069891538,\n",
       " 16952013.468032129,\n",
       " 16892328.799940191,\n",
       " 17207292.696061164,\n",
       " 17000751.971485786,\n",
       " 16594627.897394756,\n",
       " 15313960.833230741,\n",
       " 16743027.404627997,\n",
       " 16920449.175282497,\n",
       " 16976556.527639538,\n",
       " 17315360.924031951,\n",
       " 16484986.820414426,\n",
       " 16814271.21925284,\n",
       " 15497067.736702384,\n",
       " 16120078.9698513,\n",
       " 16233392.248023972,\n",
       " 16750099.898038611,\n",
       " 15989623.01316788,\n",
       " 16030035.917242775,\n",
       " 16479861.209225498,\n",
       " 17503609.701542377,\n",
       " 16225515.338942144,\n",
       " 16414744.187078079,\n",
       " 16804013.423344556,\n",
       " 16665095.784794709,\n",
       " 15702307.094948823,\n",
       " 16317664.254247366,\n",
       " 16190393.805610774,\n",
       " 15204590.933015728,\n",
       " 15433500.643515436,\n",
       " 16905015.139993217,\n",
       " 16388145.698978595,\n",
       " 16307935.090974776,\n",
       " 16168528.570752224,\n",
       " 16596096.753842808,\n",
       " 17258060.726628222,\n",
       " 16869357.03606052,\n",
       " 16088133.656799218,\n",
       " 15524334.912043713,\n",
       " 15250780.125109535,\n",
       " 16337409.659660345,\n",
       " 15479511.735425992,\n",
       " 15597342.570867244,\n",
       " 15303706.042798676,\n",
       " 16008919.955126366,\n",
       " 16555502.633903569,\n",
       " 17104035.543100439,\n",
       " 16392069.945462925,\n",
       " 16000223.971983029,\n",
       " 15723383.215260662,\n",
       " 14910430.940367946,\n",
       " 16298784.588566242,\n",
       " 16431146.305999309,\n",
       " 15496262.37392321,\n",
       " 14691434.436921297,\n",
       " 15991406.789319497,\n",
       " 16154617.615089485,\n",
       " 15765242.375014178,\n",
       " 15473041.34529178,\n",
       " 16266974.086506339,\n",
       " 15922943.38289311,\n",
       " 15660028.614085134,\n",
       " 17218916.487943199,\n",
       " 15145728.980607076,\n",
       " 16524774.375358472,\n",
       " 15277604.501445632,\n",
       " 14966764.807191171,\n",
       " 14996217.458253834,\n",
       " 16117954.963332361,\n",
       " 15936606.323319182,\n",
       " 15004729.159213392,\n",
       " 16417216.711921183,\n",
       " 14966671.762551043,\n",
       " 14767740.794738732,\n",
       " 15394739.661021942,\n",
       " 15402601.290039228,\n",
       " 14768738.416371761,\n",
       " 16276353.964173254,\n",
       " 14945685.340991333,\n",
       " 15270947.124836437,\n",
       " 15466782.673688831,\n",
       " 15712499.916557228,\n",
       " 15996838.712354684,\n",
       " 15455677.267761268,\n",
       " 15494884.934846744,\n",
       " 15140352.322614701,\n",
       " 16207742.172142372,\n",
       " 15138053.204677664,\n",
       " 15094613.668467363,\n",
       " 14633894.960692883,\n",
       " 15311652.560173554,\n",
       " 14424107.514724586,\n",
       " 15815136.724074794,\n",
       " 15933317.011346605,\n",
       " 14545970.533878788,\n",
       " 14711415.976309845,\n",
       " 14603904.788825322,\n",
       " 15053893.893704835,\n",
       " 14966272.803266358,\n",
       " 15335444.330105251,\n",
       " 14213294.754222481,\n",
       " 15045177.818529902,\n",
       " 15323532.626558553,\n",
       " 14921133.643006638,\n",
       " 14774595.489455543,\n",
       " 14985675.777251875,\n",
       " 14692983.190267278,\n",
       " 15949352.446717402,\n",
       " 14614461.507035526,\n",
       " 14262207.485945171,\n",
       " 15719829.452630971,\n",
       " 15011620.98467987,\n",
       " 15189369.1745726,\n",
       " 15003588.575747021,\n",
       " 14761899.574260455,\n",
       " 15374166.050599782,\n",
       " 14362868.233400609,\n",
       " 14912600.821361445,\n",
       " 15080993.934031906,\n",
       " 14925968.09096868,\n",
       " 14281965.326665929,\n",
       " 14259439.28629563,\n",
       " 14840677.750460014,\n",
       " 14421121.376163531,\n",
       " 15311381.293070029,\n",
       " 15074903.284732535,\n",
       " 15341346.794756036,\n",
       " 14570972.571354721,\n",
       " 13477656.384939203,\n",
       " 14568939.372487828,\n",
       " 13806515.75729404,\n",
       " 13902838.72687825,\n",
       " 14320550.806296555,\n",
       " 15112336.04166678]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx=build_tx(x)\n",
    "initial_w=np.ones(tx.shape[1])\n",
    "max_iters=500\n",
    "gamma =0.0000000001\n",
    "losses,ws=stochastic_gradient_descent(y, tx, initial_w, x.shape[0]/500, max_iters, gamma)\n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP Pace\\Documents\\GitHub\\MLO-project1\\scripts\\helpers.py:49: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[15470325.799969975,\n",
       " 14192777.675960518,\n",
       " 14148318.615577459,\n",
       " 14644722.370317277,\n",
       " 14177013.37818668,\n",
       " 14224459.190269576,\n",
       " 14761279.623222332,\n",
       " 13611409.755071739,\n",
       " 13502754.034042632,\n",
       " 15237639.627170384,\n",
       " 13926176.243662611,\n",
       " 14542572.543201121,\n",
       " 13957064.319552619,\n",
       " 13880225.016062455,\n",
       " 13910363.347619755,\n",
       " 15186592.676472278,\n",
       " 13881582.246449016,\n",
       " 13947945.852802416,\n",
       " 13872199.614315523,\n",
       " 14367897.760229748,\n",
       " 13856253.859321656,\n",
       " 14761950.421071969,\n",
       " 13919219.574412497,\n",
       " 14729860.641825484,\n",
       " 14210024.852006398,\n",
       " 13778283.875981096,\n",
       " 13691994.745634751,\n",
       " 14535996.35092843,\n",
       " 14215235.930889813,\n",
       " 13843562.591168698,\n",
       " 13400305.446432216,\n",
       " 14485039.231476421,\n",
       " 13329416.289762039,\n",
       " 13717326.846429242,\n",
       " 13362587.573835114,\n",
       " 14562105.927615216,\n",
       " 13783517.860072106,\n",
       " 13829064.807183206,\n",
       " 14541790.070842456,\n",
       " 14504029.376524162,\n",
       " 14654703.311700558,\n",
       " 14011434.564725077,\n",
       " 14673018.549876319,\n",
       " 13116613.641577514,\n",
       " 14223056.422506645,\n",
       " 14371131.516222648,\n",
       " 13771208.975631241,\n",
       " 13876993.545018306,\n",
       " 13955313.385015011,\n",
       " 13625673.230253929,\n",
       " 13843331.924862474,\n",
       " 14605759.074524701,\n",
       " 14065244.985639339,\n",
       " 14023183.51211278,\n",
       " 13197281.358703766,\n",
       " 13007791.29247061,\n",
       " 12848650.879845113,\n",
       " 13273218.957148749,\n",
       " 13401220.939046849,\n",
       " 13867700.542596191,\n",
       " 13844403.488576544,\n",
       " 12864764.375721302,\n",
       " 13114245.71289525,\n",
       " 13535516.486308077,\n",
       " 13482903.5332538,\n",
       " 12459694.86981258,\n",
       " 12602153.375087131,\n",
       " 13315337.822008964,\n",
       " 12949601.491067443,\n",
       " 13747121.30995461,\n",
       " 13709965.786869152,\n",
       " 13600601.810060441,\n",
       " 13726995.7641312,\n",
       " 12610808.135891475,\n",
       " 14234973.288782913,\n",
       " 13553617.277122268,\n",
       " 13141224.886510048,\n",
       " 13156078.0953105,\n",
       " 14316301.320177549,\n",
       " 13347196.088556172,\n",
       " 13130334.471381161,\n",
       " 12891646.94053974,\n",
       " 13705152.816329872,\n",
       " 13222346.1057948,\n",
       " 13312410.565047525,\n",
       " 13078525.883740451,\n",
       " 13396925.947775822,\n",
       " 12917636.300549002,\n",
       " 13384903.313007016,\n",
       " 13353294.744376609,\n",
       " 12825667.908472367,\n",
       " 12726129.071446486,\n",
       " 13139361.72393382,\n",
       " 13388691.819100991,\n",
       " 13010999.174358068,\n",
       " 12773913.503228981,\n",
       " 13076133.659916103,\n",
       " 12609733.394943623,\n",
       " 13443440.073573392,\n",
       " 13071257.207955033,\n",
       " 12353772.917711893,\n",
       " 13101162.277967677,\n",
       " 12642898.829279732,\n",
       " 12963460.886341874,\n",
       " 12806037.781976353,\n",
       " 12877842.393718377,\n",
       " 13057618.074267767,\n",
       " 12751592.852471501,\n",
       " 13302125.308842849,\n",
       " 13437519.811153986,\n",
       " 12268589.871679861,\n",
       " 12659586.292960854,\n",
       " 13073277.24735602,\n",
       " 13501729.399871411,\n",
       " 12877706.345353752,\n",
       " 13233025.259858081,\n",
       " 13402923.972050231,\n",
       " 12870862.340027856,\n",
       " 12935905.684663616,\n",
       " 12427674.952252621,\n",
       " 12214420.064275611,\n",
       " 12288265.142565146,\n",
       " 12273525.335378014,\n",
       " 12937140.8065328,\n",
       " 13478737.563848969,\n",
       " 12695952.636200124,\n",
       " 12536563.044986596,\n",
       " 12018173.764792506,\n",
       " 13282333.365770297,\n",
       " 12783333.912496909,\n",
       " 11987660.20056849,\n",
       " 11744498.792261034,\n",
       " 11826303.699239075,\n",
       " 12295706.611937508,\n",
       " 12276579.960560607,\n",
       " 12550105.490372529,\n",
       " 12920284.4394408,\n",
       " 12738223.223070875,\n",
       " 12945920.296665424,\n",
       " 12818292.443581086,\n",
       " 12218460.373002449,\n",
       " 12322583.704237523,\n",
       " 11661720.404447734,\n",
       " 12870366.226619693,\n",
       " 12319518.654063923,\n",
       " 11592636.501673551,\n",
       " 12638851.44979555,\n",
       " 13023748.067656312,\n",
       " 12481732.694486346,\n",
       " 12331052.863843281,\n",
       " 12519330.354903145,\n",
       " 12798261.153799353,\n",
       " 12194253.483655976,\n",
       " 12210963.488833416,\n",
       " 12199501.594729584,\n",
       " 12309974.478562325,\n",
       " 11746474.526639896,\n",
       " 12195291.056164825,\n",
       " 12497080.696725773,\n",
       " 12546883.22375913,\n",
       " 12126710.465062829,\n",
       " 12060743.561106399,\n",
       " 11784497.754122159,\n",
       " 12178407.940004395,\n",
       " 11372719.849001555,\n",
       " 12425119.977315594,\n",
       " 12772273.564253082,\n",
       " 12424240.331126867,\n",
       " 11994182.65339355,\n",
       " 11191101.234818624,\n",
       " 12013261.822486732,\n",
       " 11811272.617200995,\n",
       " 12254760.399940308,\n",
       " 11849608.063322421,\n",
       " 11820090.891251022,\n",
       " 12042707.999263538,\n",
       " 12096359.576354628,\n",
       " 12285961.747310512,\n",
       " 12014032.497708274,\n",
       " 11815591.067965463,\n",
       " 12415299.596594157,\n",
       " 11324574.414567197,\n",
       " 12145728.730145207,\n",
       " 11735652.389764832,\n",
       " 12523270.309453225,\n",
       " 11680241.517312348,\n",
       " 11908321.06869298,\n",
       " 12500828.407241737,\n",
       " 12904841.137163861,\n",
       " 11560006.933239788,\n",
       " 11822198.275192207,\n",
       " 11677211.241506796,\n",
       " 11724861.245492745,\n",
       " 11126778.679493472,\n",
       " 12320047.759479769,\n",
       " 11274353.044194411,\n",
       " 11746460.85744066,\n",
       " 11041710.220933303,\n",
       " 11387683.732597828,\n",
       " 12451942.778650308,\n",
       " 11490560.182764065,\n",
       " 12083539.948237479,\n",
       " 11435064.031828728,\n",
       " 11469738.129533546,\n",
       " 11689654.663673416,\n",
       " 11512102.777484803,\n",
       " 11112321.5823834,\n",
       " 11720305.996895812,\n",
       " 11378101.104496999,\n",
       " 10967297.385903588,\n",
       " 12761386.70858464,\n",
       " 12335607.356116204,\n",
       " 11695324.730250526,\n",
       " 11733073.811449097,\n",
       " 11826679.764124999,\n",
       " 10861543.251369487,\n",
       " 11467809.429090241,\n",
       " 10853342.965339977,\n",
       " 11456246.417915057,\n",
       " 11112637.010972913,\n",
       " 11102310.930536933,\n",
       " 11734313.258873332,\n",
       " 11537134.850624219,\n",
       " 12059090.309447259,\n",
       " 10886489.589824788,\n",
       " 11486406.771075454,\n",
       " 10987149.260903064,\n",
       " 12126286.657353874,\n",
       " 11321547.473174503,\n",
       " 11302310.346954949,\n",
       " 11428077.693485711,\n",
       " 11356914.882047879,\n",
       " 11145483.4434154,\n",
       " 10882476.629278328,\n",
       " 11133251.576584423,\n",
       " 10838309.684303207,\n",
       " 11542282.110170951,\n",
       " 11615362.433974996,\n",
       " 11481721.117061242,\n",
       " 11173458.428050075,\n",
       " 11226126.422327323,\n",
       " 11250951.613520788,\n",
       " 11000081.606903475,\n",
       " 10943857.456854051,\n",
       " 10446351.443390617,\n",
       " 11551185.43481168,\n",
       " 11010486.43863586,\n",
       " 11064080.77458646,\n",
       " 11307268.457039507,\n",
       " 10333839.037401367,\n",
       " 11256356.251572898,\n",
       " 10783810.718009539,\n",
       " 10573199.139426872,\n",
       " 11061750.315523755,\n",
       " 10700150.926401237,\n",
       " 11175487.82872697,\n",
       " 11250752.632243412,\n",
       " 10793988.938412437,\n",
       " 11754892.569170082,\n",
       " 10486685.633003503,\n",
       " 11039280.235451709,\n",
       " 10948525.441465572,\n",
       " 10940082.656040618,\n",
       " 10843748.723099628,\n",
       " 11343811.489820022,\n",
       " 11131657.019690707,\n",
       " 10625000.844323626,\n",
       " 11199483.018356066,\n",
       " 11353261.805292744,\n",
       " 10239735.868441002,\n",
       " 10362182.410609486,\n",
       " 10817875.200362941,\n",
       " 10558966.054045305,\n",
       " 11525567.958551206,\n",
       " 10566699.009586345,\n",
       " 10628403.259833153,\n",
       " 10713037.167610742,\n",
       " 10793654.679457158,\n",
       " 10589917.811320087,\n",
       " 9841194.7833538782,\n",
       " 10669163.5242365,\n",
       " 10626133.554785065,\n",
       " 10660596.404899115,\n",
       " 10753718.306904655,\n",
       " 11122469.215797734,\n",
       " 10490241.341675999,\n",
       " 10825707.619963603,\n",
       " 10917530.141609296,\n",
       " 10473944.060332304,\n",
       " 9993484.749077877,\n",
       " 10511732.71019686,\n",
       " 10415707.8570274,\n",
       " 11439433.331140297,\n",
       " 10201885.948302832,\n",
       " 11048245.343502766,\n",
       " 10284049.071270591,\n",
       " 10390817.728085162,\n",
       " 10610810.243717246,\n",
       " 9899385.023495838,\n",
       " 10179480.843857501,\n",
       " 9473965.7528329622,\n",
       " 10339076.178931488,\n",
       " 10198441.63978746,\n",
       " 10755976.707101563,\n",
       " 9748103.315235164,\n",
       " 9898413.1794750765,\n",
       " 10471084.74011538,\n",
       " 10249806.523991749,\n",
       " 10719167.204702327,\n",
       " 11097301.118410632,\n",
       " 10670902.110752827,\n",
       " 9956325.7033583932,\n",
       " 9806363.7267726716,\n",
       " 11149625.797739472,\n",
       " 10244280.143247735,\n",
       " 10359729.458514236,\n",
       " 10386895.015500106,\n",
       " 10197100.995247154,\n",
       " 10058364.567394024,\n",
       " 10385165.023289753,\n",
       " 9815377.4296463504,\n",
       " 9736602.9691925272,\n",
       " 9890521.0175553393,\n",
       " 10118516.531299278,\n",
       " 9915133.2010027841,\n",
       " 10487853.683613468,\n",
       " 10205109.867139507,\n",
       " 10315602.056490669,\n",
       " 9828574.2176753394,\n",
       " 10076816.012106419,\n",
       " 10505402.501552735,\n",
       " 10070444.483670086,\n",
       " 11075907.78636683,\n",
       " 10381959.268107308,\n",
       " 9658639.4200959969,\n",
       " 9921002.6470579077,\n",
       " 10020004.974272266,\n",
       " 10797614.659214782,\n",
       " 9645004.5286000706,\n",
       " 10024969.857374908,\n",
       " 9546035.8742875941,\n",
       " 9444188.8229154963,\n",
       " 10509394.979314648,\n",
       " 9667138.3268071376,\n",
       " 9569543.667308975,\n",
       " 10758977.488484338,\n",
       " 10249727.171658419,\n",
       " 10057183.997926563,\n",
       " 9945664.8051837422,\n",
       " 9850138.5128000844,\n",
       " 10722185.166678257,\n",
       " 10540185.700382808,\n",
       " 9814470.2703274768,\n",
       " 9300946.7705444079,\n",
       " 9264146.6837478634,\n",
       " 9922827.8504403457,\n",
       " 10067834.160852106,\n",
       " 9482843.088789029,\n",
       " 9097710.3967199512,\n",
       " 9985064.3454521559,\n",
       " 9353932.574116677,\n",
       " 9487354.5045777094,\n",
       " 9810954.3191301692,\n",
       " 9137967.0835799444,\n",
       " 10391506.349691136,\n",
       " 9722692.2729113419,\n",
       " 9470262.5460228771,\n",
       " 10290230.546185967,\n",
       " 9488069.5324857645,\n",
       " 9162825.2739753891,\n",
       " 9081374.2884656731,\n",
       " 9601049.841816796,\n",
       " 9759730.2356951982,\n",
       " 10199227.066929864,\n",
       " 9274925.6796769667,\n",
       " 10071482.63972313,\n",
       " 9669360.3362522591,\n",
       " 10031322.067957988,\n",
       " 9722623.0827702601,\n",
       " 9904701.1496748049,\n",
       " 9406846.1593053322,\n",
       " 9379088.2517776489,\n",
       " 9022626.7153513394,\n",
       " 9293493.7916750479,\n",
       " 9585769.878951095,\n",
       " 9359362.4059765171,\n",
       " 9579306.2817409094,\n",
       " 9461787.7943706699,\n",
       " 9285092.1284438893,\n",
       " 10309937.563447408,\n",
       " 9782221.4328984562,\n",
       " 9239922.622144457,\n",
       " 9536778.2240596078,\n",
       " 9894795.8789723963,\n",
       " 9217396.0993279107,\n",
       " 9425642.6121752132,\n",
       " 9472138.7957782783,\n",
       " 8916853.1897023097,\n",
       " 9250231.4588924665,\n",
       " 9862796.0834717639,\n",
       " 9265378.9089023173,\n",
       " 9590420.1848935671,\n",
       " 9479266.2818955202,\n",
       " 9101266.1777053904,\n",
       " 8897909.2499195896,\n",
       " 9605626.0716646891,\n",
       " 9512514.7425366975,\n",
       " 9197690.5836819429,\n",
       " 9727807.0859197713,\n",
       " 9164437.2054889426,\n",
       " 9292502.3243892901,\n",
       " 9063729.0004000962,\n",
       " 9239372.6400380675,\n",
       " 9597989.8094924651,\n",
       " 9913373.6825072058,\n",
       " 9233636.2152875401,\n",
       " 8629856.2884715777,\n",
       " 9223829.5691814981,\n",
       " 9317721.4850609396,\n",
       " 9299680.2357507385,\n",
       " 8651826.8035854734,\n",
       " 9536005.6676743329,\n",
       " 9095984.2128588222,\n",
       " 8976771.0501117352,\n",
       " 9140400.6698823404,\n",
       " 9403199.5578495786,\n",
       " 8924473.472973207,\n",
       " 9076317.4065216258,\n",
       " 9590445.6721825004,\n",
       " 9643830.0182973687,\n",
       " 8479537.7849297114,\n",
       " 9023485.7728583328,\n",
       " 9141921.5924583785,\n",
       " 8672382.4170679878,\n",
       " 8695742.2024926376,\n",
       " 9044302.8764389306,\n",
       " 8437380.6756244749,\n",
       " 8538827.2145447284,\n",
       " 8833035.7293530926,\n",
       " 8816774.0851143748,\n",
       " 8945204.8425608706,\n",
       " 8804514.2888515666,\n",
       " 8542867.0644894037,\n",
       " 9274451.6720077321,\n",
       " 8678697.596818883,\n",
       " 8905207.8807783872,\n",
       " 9171857.7069236152,\n",
       " 9127443.6814845353,\n",
       " 9009487.2617256138,\n",
       " 8564460.8712038212,\n",
       " 9160333.7349529527,\n",
       " 8671388.7812742423,\n",
       " 8813519.1615546551,\n",
       " 8079717.4233420026,\n",
       " 8713388.007033933,\n",
       " 8740317.8362825606,\n",
       " 9008312.1330932211,\n",
       " 8719528.4478075337,\n",
       " 8737863.2480184045,\n",
       " 8477907.4950200636,\n",
       " 8857565.0307358522,\n",
       " 9014172.8897119109,\n",
       " 8605666.5518061351,\n",
       " 8490642.8919533622,\n",
       " 8720922.7363245133,\n",
       " 8706303.8522471804,\n",
       " 8483102.5227748659,\n",
       " 8770187.8887726106,\n",
       " 8864353.9787005577,\n",
       " 8449716.7456008643,\n",
       " 8796790.1116782539,\n",
       " 8417036.7756832093,\n",
       " 8280132.3709092233,\n",
       " 8561241.8051568456,\n",
       " 8417251.389183864,\n",
       " 8481374.0053957254,\n",
       " 8811312.8333605919,\n",
       " 8773877.2815888748,\n",
       " 8361692.0971460193,\n",
       " 8280394.6102967365,\n",
       " 8349722.6552358195,\n",
       " 8741147.0117682684,\n",
       " 8596018.5666308515,\n",
       " 8393615.8727502674,\n",
       " 8565000.9690007251,\n",
       " 8343038.2668632111,\n",
       " 8260142.5764083555,\n",
       " 8719560.8073313404,\n",
       " 8765496.4088931773,\n",
       " 8438684.4366521593,\n",
       " 8342795.5794917112,\n",
       " 8252072.2971935906,\n",
       " 8248747.3038297789,\n",
       " 8104219.4075796241,\n",
       " 8320920.5958427675,\n",
       " 8640597.5174628273,\n",
       " 8439095.1604215838,\n",
       " 8949665.8790522199,\n",
       " 8251014.8915150352,\n",
       " 8804291.3330936152]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses,ws=stochastic_gradient_descent(y, tx, ws[len(ws)-1], x.shape[0]/500, max_iters, gamma)\n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_SGD_gradient_ws = ws[len(ws)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss,final_LS_w=least_squares(y,tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33944559851782757"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -3.60668398e-01,   7.23061306e-05,  -7.14468236e-03,\n",
       "        -6.24039951e-03,  -4.77145010e-04,  -3.08504353e-03,\n",
       "         4.49444469e-04,  -2.38125455e-02,   3.42157704e-01,\n",
       "        -1.09950486e-04,  -2.84186178e+00,  -2.16216038e-01,\n",
       "         9.55448998e-02,   4.52028139e-02,   2.85042830e+00,\n",
       "        -2.62729702e-04,  -9.95639898e-04,   2.85490239e+00,\n",
       "        -3.54662999e-04,   8.49821857e-04,   3.69247838e-03,\n",
       "         2.18113134e-04,  -5.00574872e-04,  -2.11522348e-01,\n",
       "        -2.18882082e-04,   2.01549413e-04,   3.43135946e-04,\n",
       "        -6.98404056e-05,  -6.58071046e-03,  -1.18306611e-02,\n",
       "         2.84110926e+00])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_LS_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building polynomial  0\n",
      "Building polynomial  100000\n",
      "Building polynomial  200000\n"
     ]
    }
   ],
   "source": [
    "degree=4\n",
    "x_powered=build_poly(stdx,degree)\n",
    "#split data !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 5, 30)\n",
      "y,train_tx,weight (250000,) (250000, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "sample  200000\n",
      "1.17077751943\n"
     ]
    }
   ],
   "source": [
    "lambda_=0.01\n",
    "np.zeros((30,5))\n",
    "stdx\n",
    "loss,w=ridge_regression_ml_function(y,x_powered,lambda_)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation with ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "sample  0\n",
      "sample  50000\n"
     ]
    }
   ],
   "source": [
    "k_indices=build_k_indices(y,4,1)\n",
    "loss_tr, loss_te,weight=cross_validation(y,x_powered,k_indices,0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.17236626814 1.16457030702\n"
     ]
    }
   ],
   "source": [
    "print(loss_tr, loss_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda 0.0001 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.000161026202756 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.00025929437974 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.000417531893656 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.00067233575365 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.00108263673387 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.0017433288222 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.00280721620394 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.00452035365636 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.00727895384398 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.0117210229753 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.0188739182214 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.0303919538231 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.0489390091848 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.0788046281567 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.126896100317 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.204335971786 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.329034456231 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.529831690628 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 0.853167852417 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 1.37382379588 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 2.21221629107 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 3.56224789026 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 5.73615251045 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 9.23670857187 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 14.8735210729 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 23.9502661999 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 38.5662042116 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 62.1016941892 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "Lambda 100.0 / 30\n",
      "k= 0\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 1\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 2\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "k= 3\n",
      "(187500, 5, 30) (187500,)\n",
      "(187500, 5, 30)\n",
      "y,train_tx,weight (187500,) (187500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n",
      "sample  100000\n",
      "sample  150000\n",
      "y,train_tx,weight (62500,) (62500, 5, 30) (30, 5)\n",
      "sample  0\n",
      "sample  50000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cross_validation_visualization' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-b6f2437dacb2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mrmse_tr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mrmse_te\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mcross_validation_visualization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrmse_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrmse_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cross_validation_visualization' is not defined"
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "k_fold = 4\n",
    "lambdas = np.logspace(-4, 2, 30)\n",
    "# split data in k fold\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "# define lists to store the loss of training data and test data\n",
    "rmse_tr = []\n",
    "rmse_te = []\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# cross validation: TODO\n",
    "# *************************************************** \n",
    "for lambda_ in lambdas:\n",
    "    print (\"Lambda\",lambda_,\"/\",len(lambdas))\n",
    "    loss_tr=[]\n",
    "    loss_te=[]\n",
    "    for k in np.arange(k_fold):\n",
    "        print (\"k=\",k)\n",
    "        loss=np.zeros(2)\n",
    "        loss[0],loss[1],weight=cross_validation(y,x_powered,k_indices,k,lambda_)\n",
    "        loss_tr.append(loss[0])\n",
    "        loss_te.append(loss[1])\n",
    "    rmse_tr.append(np.mean(loss_tr))\n",
    "    rmse_te.append(np.mean(loss_te))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAEdCAYAAAA4rdFEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2514ab51cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "# The graph shows us that lambda should be 10^1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=0.5\n",
      "Current iteration=1000, the loss=0.4178719682095945\n"
     ]
    }
   ],
   "source": [
    "max_iters=2000\n",
    "threshold=0.0000000000000001\n",
    "gamma=0.000000001\n",
    "sx, mean_x, std_x=standardize(x)\n",
    "\n",
    "loss,ws=logistic_regression_gradient_descent(y,sx,max_iters,threshold,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 31)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5,\n",
       " 0.49978034751905664,\n",
       " 0.49956113907306915,\n",
       " 0.49934237392289693,\n",
       " 0.499124051330602,\n",
       " 0.49890617055945025,\n",
       " 0.49868873087390775,\n",
       " 0.49847173153964064,\n",
       " 0.49825517182351325,\n",
       " 0.49803905099358597,\n",
       " 0.49782336831911467,\n",
       " 0.49760812307054875,\n",
       " 0.49739331451953028,\n",
       " 0.49717894193889123,\n",
       " 0.49696500460265319,\n",
       " 0.49675150178602551,\n",
       " 0.49653843276540377,\n",
       " 0.49632579681836797,\n",
       " 0.49611359322368176,\n",
       " 0.4959018212612904,\n",
       " 0.49569048021231987,\n",
       " 0.49547956935907439,\n",
       " 0.49526908798503555,\n",
       " 0.49505903537486151,\n",
       " 0.49484941081438388,\n",
       " 0.49464021359060778,\n",
       " 0.49443144299170944,\n",
       " 0.49422309830703515,\n",
       " 0.49401517882709967,\n",
       " 0.49380768384358481,\n",
       " 0.49360061264933741,\n",
       " 0.49339396453836892,\n",
       " 0.4931877388058527,\n",
       " 0.49298193474812368,\n",
       " 0.49277655166267575,\n",
       " 0.49257158884816138,\n",
       " 0.49236704560438949,\n",
       " 0.49216292123232402,\n",
       " 0.4919592150340823,\n",
       " 0.49175592631293402,\n",
       " 0.49155305437329971,\n",
       " 0.49135059852074875,\n",
       " 0.49114855806199809,\n",
       " 0.49094693230491154,\n",
       " 0.49074572055849691,\n",
       " 0.49054492213290513,\n",
       " 0.4903445363394297,\n",
       " 0.49014456249050409,\n",
       " 0.48994499989969986,\n",
       " 0.48974584788172665,\n",
       " 0.48954710575242971,\n",
       " 0.48934877282878836,\n",
       " 0.4891508484289156,\n",
       " 0.48895333187205442,\n",
       " 0.48875622247857919,\n",
       " 0.48855951956999161,\n",
       " 0.48836322246892083,\n",
       " 0.48816733049912153,\n",
       " 0.48797184298547164,\n",
       " 0.48777675925397257,\n",
       " 0.48758207863174635,\n",
       " 0.48738780044703411,\n",
       " 0.48719392402919576,\n",
       " 0.48700044870870729,\n",
       " 0.48680737381716011,\n",
       " 0.48661469868725898,\n",
       " 0.48642242265282087,\n",
       " 0.48623054504877383,\n",
       " 0.48603906521115436,\n",
       " 0.48584798247710709,\n",
       " 0.48565729618488301,\n",
       " 0.4854670056738376,\n",
       " 0.48527711028443005,\n",
       " 0.48508760935822093,\n",
       " 0.48489850223787129,\n",
       " 0.48470978826714084,\n",
       " 0.48452146679088698,\n",
       " 0.48433353715506333,\n",
       " 0.48414599870671698,\n",
       " 0.48395885079398887,\n",
       " 0.48377209276611133,\n",
       " 0.48358572397340599,\n",
       " 0.4833997437672839,\n",
       " 0.48321415150024294,\n",
       " 0.48302894652586648,\n",
       " 0.48284412819882189,\n",
       " 0.48265969587485957,\n",
       " 0.48247564891081079,\n",
       " 0.48229198666458695,\n",
       " 0.48210870849517734,\n",
       " 0.48192581376264831,\n",
       " 0.48174330182814129,\n",
       " 0.48156117205387167,\n",
       " 0.48137942380312754,\n",
       " 0.48119805644026747,\n",
       " 0.48101706933071964,\n",
       " 0.48083646184098039,\n",
       " 0.48065623333861257,\n",
       " 0.48047638319224406,\n",
       " 0.480296910771566,\n",
       " 0.4801178154473324,\n",
       " 0.47993909659135753,\n",
       " 0.47976075357651438,\n",
       " 0.47958278577673485,\n",
       " 0.47940519256700637,\n",
       " 0.4792279733233713,\n",
       " 0.47905112742292588,\n",
       " 0.4788746542438172,\n",
       " 0.47869855316524401,\n",
       " 0.47852282356745329,\n",
       " 0.47834746483174007,\n",
       " 0.47817247634044535,\n",
       " 0.47799785747695434,\n",
       " 0.47782360762569598,\n",
       " 0.4776497261721408,\n",
       " 0.47747621250279976,\n",
       " 0.47730306600522227,\n",
       " 0.47713028606799529,\n",
       " 0.47695787208074186,\n",
       " 0.47678582343411935,\n",
       " 0.47661413951981824,\n",
       " 0.47644281973056074,\n",
       " 0.47627186346009881,\n",
       " 0.47610127010321324,\n",
       " 0.47593103905571199,\n",
       " 0.47576116971442939,\n",
       " 0.47559166147722326,\n",
       " 0.47542251374297512,\n",
       " 0.47525372591158738,\n",
       " 0.47508529738398259,\n",
       " 0.4749172275621017,\n",
       " 0.47474951584890368,\n",
       " 0.47458216164836198,\n",
       " 0.47441516436546494,\n",
       " 0.47424852340621387,\n",
       " 0.47408223817762085,\n",
       " 0.47391630808770863,\n",
       " 0.4737507325455077,\n",
       " 0.47358551096105655,\n",
       " 0.47342064274539802,\n",
       " 0.47325612731058037,\n",
       " 0.47309196406965409,\n",
       " 0.4729281524366708,\n",
       " 0.4727646918266824,\n",
       " 0.47260158165573901,\n",
       " 0.47243882134088772,\n",
       " 0.4722764103001717,\n",
       " 0.47211434795262758,\n",
       " 0.47195263371828516,\n",
       " 0.47179126701816582,\n",
       " 0.47163024727427999,\n",
       " 0.47146957390962774,\n",
       " 0.47130924634819521,\n",
       " 0.47114926401495416,\n",
       " 0.47098962633586133,\n",
       " 0.47083033273785574,\n",
       " 0.47067138264885749,\n",
       " 0.47051277549776704,\n",
       " 0.47035451071446355,\n",
       " 0.47019658772980255,\n",
       " 0.47003900597561576,\n",
       " 0.4698817648847094,\n",
       " 0.46972486389086193,\n",
       " 0.46956830242882341,\n",
       " 0.46941207993431439,\n",
       " 0.46925619584402345,\n",
       " 0.46910064959560666,\n",
       " 0.46894544062768562,\n",
       " 0.46879056837984678,\n",
       " 0.46863603229263912,\n",
       " 0.4684818318075733,\n",
       " 0.46832796636712037,\n",
       " 0.46817443541470966,\n",
       " 0.4680212383947282,\n",
       " 0.4678683747525188,\n",
       " 0.46771584393437904,\n",
       " 0.46756364538755918,\n",
       " 0.46741177856026184,\n",
       " 0.46726024290163948,\n",
       " 0.46710903786179331,\n",
       " 0.46695816289177294,\n",
       " 0.46680761744357324,\n",
       " 0.46665740097013425,\n",
       " 0.46650751292533921,\n",
       " 0.46635795276401371,\n",
       " 0.46620871994192303,\n",
       " 0.46605981391577256,\n",
       " 0.46591123414320484,\n",
       " 0.46576298008279937,\n",
       " 0.46561505119406993,\n",
       " 0.46546744693746434,\n",
       " 0.465320166774363,\n",
       " 0.46517321016707641,\n",
       " 0.46502657657884483,\n",
       " 0.46488026547383693,\n",
       " 0.46473427631714748,\n",
       " 0.46458860857479711,\n",
       " 0.46444326171373007,\n",
       " 0.46429823520181318,\n",
       " 0.46415352850783442,\n",
       " 0.46400914110150177,\n",
       " 0.46386507245344144,\n",
       " 0.46372132203519678,\n",
       " 0.46357788931922678,\n",
       " 0.4634347737789048,\n",
       " 0.46329197488851687,\n",
       " 0.46314949212326112,\n",
       " 0.46300732495924496,\n",
       " 0.46286547287348573,\n",
       " 0.46272393534390738,\n",
       " 0.46258271184934047,\n",
       " 0.46244180186951994,\n",
       " 0.46230120488508425,\n",
       " 0.46216092037757395,\n",
       " 0.46202094782943004,\n",
       " 0.46188128672399287,\n",
       " 0.46174193654550066,\n",
       " 0.46160289677908845,\n",
       " 0.46146416691078623,\n",
       " 0.46132574642751784,\n",
       " 0.46118763481709985,\n",
       " 0.46104983156823948,\n",
       " 0.46091233617053468,\n",
       " 0.4607751481144709,\n",
       " 0.46063826689142084,\n",
       " 0.46050169199364316,\n",
       " 0.46036542291428129,\n",
       " 0.46022945914736119,\n",
       " 0.46009380018779039,\n",
       " 0.45995844553135745,\n",
       " 0.45982339467472916,\n",
       " 0.4596886471154506,\n",
       " 0.45955420235194294,\n",
       " 0.45942005988350243,\n",
       " 0.45928621921029883,\n",
       " 0.45915267983337454,\n",
       " 0.45901944125464267,\n",
       " 0.4588865029768861,\n",
       " 0.45875386450375594,\n",
       " 0.45862152533977052,\n",
       " 0.45848948499031339,\n",
       " 0.45835774296163312,\n",
       " 0.45822629876084048,\n",
       " 0.45809515189590888,\n",
       " 0.45796430187567128,\n",
       " 0.45783374820981976,\n",
       " 0.45770349040890473,\n",
       " 0.45757352798433215,\n",
       " 0.45744386044836383,\n",
       " 0.45731448731411489,\n",
       " 0.45718540809555258,\n",
       " 0.45705662230749605,\n",
       " 0.45692812946561362,\n",
       " 0.45679992908642258,\n",
       " 0.45667202068728679,\n",
       " 0.45654440378641681,\n",
       " 0.45641707790286662,\n",
       " 0.4562900425565346,\n",
       " 0.45616329726816035,\n",
       " 0.45603684155932422,\n",
       " 0.45591067495244619,\n",
       " 0.45578479697078378,\n",
       " 0.45565920713843178,\n",
       " 0.45553390498031987,\n",
       " 0.45540889002221213,\n",
       " 0.45528416179070536,\n",
       " 0.45515971981322811,\n",
       " 0.45503556361803882,\n",
       " 0.4549116927342251,\n",
       " 0.45478810669170205,\n",
       " 0.45466480502121109,\n",
       " 0.45454178725431904,\n",
       " 0.45441905292341583,\n",
       " 0.4542966015617147,\n",
       " 0.45417443270324959,\n",
       " 0.45405254588287386,\n",
       " 0.4539309406362606,\n",
       " 0.45380961649989954,\n",
       " 0.45368857301109633,\n",
       " 0.45356780970797178,\n",
       " 0.45344732612946032,\n",
       " 0.45332712181530793,\n",
       " 0.45320719630607226,\n",
       " 0.45308754914312022,\n",
       " 0.45296817986862709,\n",
       " 0.45284908802557544,\n",
       " 0.45273027315775372,\n",
       " 0.4526117348097547,\n",
       " 0.45249347252697469,\n",
       " 0.45237548585561205,\n",
       " 0.45225777434266573,\n",
       " 0.45214033753593452,\n",
       " 0.45202317498401506,\n",
       " 0.4519062862363013,\n",
       " 0.4517896708429826,\n",
       " 0.4516733283550432,\n",
       " 0.45155725832426036,\n",
       " 0.45144146030320292,\n",
       " 0.45132593384523118,\n",
       " 0.45121067850449426,\n",
       " 0.45109569383592962,\n",
       " 0.45098097939526205,\n",
       " 0.45086653473900129,\n",
       " 0.45075235942444208,\n",
       " 0.45063845300966276,\n",
       " 0.4505248150535226,\n",
       " 0.45041144511566189,\n",
       " 0.45029834275650099,\n",
       " 0.45018550753723796,\n",
       " 0.45007293901984796,\n",
       " 0.44996063676708176,\n",
       " 0.44984860034246504,\n",
       " 0.44973682931029635,\n",
       " 0.44962532323564647,\n",
       " 0.44951408168435675,\n",
       " 0.44940310422303853,\n",
       " 0.44929239041907137,\n",
       " 0.44918193984060156,\n",
       " 0.44907175205654187,\n",
       " 0.44896182663656903,\n",
       " 0.44885216315112403,\n",
       " 0.44874276117140965,\n",
       " 0.44863362026938924,\n",
       " 0.4485247400177868,\n",
       " 0.44841611999008418,\n",
       " 0.4483077597605209,\n",
       " 0.44819965890409263,\n",
       " 0.44809181699654971,\n",
       " 0.44798423361439649,\n",
       " 0.4478769083348893,\n",
       " 0.44776984073603593,\n",
       " 0.44766303039659505,\n",
       " 0.44755647689607303,\n",
       " 0.44745017981472412,\n",
       " 0.4473441387335495,\n",
       " 0.44723835323429523,\n",
       " 0.44713282289945155,\n",
       " 0.44702754731225119,\n",
       " 0.44692252605666916,\n",
       " 0.44681775871742024,\n",
       " 0.44671324487995867,\n",
       " 0.44660898413047717,\n",
       " 0.44650497605590445,\n",
       " 0.44640122024390583,\n",
       " 0.44629771628288012,\n",
       " 0.44619446376196015,\n",
       " 0.44609146227101065,\n",
       " 0.44598871140062735,\n",
       " 0.4458862107421348,\n",
       " 0.44578395988758757,\n",
       " 0.44568195842976654,\n",
       " 0.44558020596217884,\n",
       " 0.4454787020790571,\n",
       " 0.44537744637535726,\n",
       " 0.44527643844675813,\n",
       " 0.44517567788965973,\n",
       " 0.44507516430118266,\n",
       " 0.44497489727916656,\n",
       " 0.444874876422169,\n",
       " 0.44477510132946407,\n",
       " 0.44467557160104182,\n",
       " 0.44457628683760653,\n",
       " 0.4444772466405757,\n",
       " 0.44437845061207892,\n",
       " 0.44427989835495735,\n",
       " 0.44418158947276082,\n",
       " 0.44408352356974851,\n",
       " 0.44398570025088696,\n",
       " 0.44388811912184911,\n",
       " 0.44379077978901244,\n",
       " 0.4436936818594594,\n",
       " 0.44359682494097452,\n",
       " 0.44350020864204426,\n",
       " 0.44340383257185539,\n",
       " 0.44330769634029465,\n",
       " 0.4432117995579466,\n",
       " 0.44311614183609283,\n",
       " 0.44302072278671101,\n",
       " 0.44292554202247381,\n",
       " 0.44283059915674722,\n",
       " 0.44273589380359002,\n",
       " 0.44264142557775221,\n",
       " 0.44254719409467425,\n",
       " 0.44245319897048541,\n",
       " 0.44235943982200332,\n",
       " 0.44226591626673223,\n",
       " 0.44217262792286216,\n",
       " 0.44207957440926776,\n",
       " 0.44198675534550658,\n",
       " 0.44189417035181988,\n",
       " 0.44180181904912874,\n",
       " 0.44170970105903495,\n",
       " 0.44161781600381939,\n",
       " 0.44152616350644014,\n",
       " 0.44143474319053277,\n",
       " 0.44134355468040803,\n",
       " 0.44125259760105134,\n",
       " 0.44116187157812126,\n",
       " 0.44107137623794873,\n",
       " 0.44098111120753614,\n",
       " 0.44089107611455525,\n",
       " 0.44080127058734714,\n",
       " 0.44071169425492113,\n",
       " 0.44062234674695233,\n",
       " 0.44053322769378211,\n",
       " 0.44044433672641631,\n",
       " 0.44035567347652393,\n",
       " 0.44026723757643627,\n",
       " 0.44017902865914588,\n",
       " 0.44009104635830576,\n",
       " 0.44000329030822743,\n",
       " 0.4399157601438804,\n",
       " 0.43982845550089106,\n",
       " 0.43974137601554197,\n",
       " 0.4396545213247694,\n",
       " 0.43956789106616384,\n",
       " 0.43948148487796823,\n",
       " 0.43939530239907648,\n",
       " 0.43930934326903315,\n",
       " 0.43922360712803149,\n",
       " 0.43913809361691386,\n",
       " 0.4390528023771687,\n",
       " 0.43896773305093056,\n",
       " 0.43888288528097924,\n",
       " 0.43879825871073802,\n",
       " 0.43871385298427329,\n",
       " 0.43862966774629253,\n",
       " 0.43854570264214432,\n",
       " 0.43846195731781629,\n",
       " 0.43837843141993482,\n",
       " 0.43829512459576375,\n",
       " 0.43821203649320301,\n",
       " 0.43812916676078767,\n",
       " 0.4380465150476871,\n",
       " 0.43796408100370421,\n",
       " 0.43788186427927289,\n",
       " 0.43779986452545938,\n",
       " 0.4377180813939589,\n",
       " 0.43763651453709568,\n",
       " 0.43755516360782204,\n",
       " 0.43747402825971715,\n",
       " 0.43739310814698534,\n",
       " 0.43731240292445611,\n",
       " 0.43723191224758279,\n",
       " 0.43715163577244037,\n",
       " 0.43707157315572631,\n",
       " 0.43699172405475795,\n",
       " 0.43691208812747256,\n",
       " 0.43683266503242524,\n",
       " 0.43675345442878888,\n",
       " 0.43667445597635246,\n",
       " 0.43659566933552035,\n",
       " 0.43651709416731116,\n",
       " 0.43643873013335666,\n",
       " 0.43636057689590085,\n",
       " 0.43628263411779872,\n",
       " 0.43620490146251556,\n",
       " 0.43612737859412565,\n",
       " 0.43605006517731115,\n",
       " 0.43597296087736159,\n",
       " 0.43589606536017234,\n",
       " 0.43581937829224388,\n",
       " 0.43574289934068011,\n",
       " 0.43566662817318857,\n",
       " 0.43559056445807837,\n",
       " 0.43551470786425955,\n",
       " 0.43543905806124189,\n",
       " 0.43536361471913426,\n",
       " 0.43528837750864369,\n",
       " 0.4352133461010731,\n",
       " 0.435138520168322,\n",
       " 0.4350638993828847,\n",
       " 0.43498948341784877,\n",
       " 0.43491527194689544,\n",
       " 0.43484126464429712,\n",
       " 0.4347674611849171,\n",
       " 0.43469386124420839,\n",
       " 0.43462046449821312,\n",
       " 0.43454727062356102,\n",
       " 0.43447427929746868,\n",
       " 0.43440149019773833,\n",
       " 0.43432890300275734,\n",
       " 0.43425651739149629,\n",
       " 0.43418433304350951,\n",
       " 0.43411234963893197,\n",
       " 0.43404056685848069,\n",
       " 0.43396898438345172,\n",
       " 0.4338976018957203,\n",
       " 0.43382641907773944,\n",
       " 0.43375543561253926,\n",
       " 0.43368465118372507,\n",
       " 0.43361406547547843,\n",
       " 0.43354367817255368,\n",
       " 0.4334734889602786,\n",
       " 0.43340349752455298,\n",
       " 0.43333370355184753,\n",
       " 0.43326410672920296,\n",
       " 0.43319470674422944,\n",
       " 0.43312550328510452,\n",
       " 0.43305649604057372,\n",
       " 0.43298768469994814,\n",
       " 0.43291906895310439,\n",
       " 0.43285064849048305,\n",
       " 0.43278242300308828,\n",
       " 0.4327143921824862,\n",
       " 0.43264655572080452,\n",
       " 0.43257891331073151,\n",
       " 0.43251146464551427,\n",
       " 0.43244420941895911,\n",
       " 0.4323771473254292,\n",
       " 0.43231027805984501,\n",
       " 0.43224360131768141,\n",
       " 0.43217711679496934,\n",
       " 0.43211082418829261,\n",
       " 0.43204472319478804,\n",
       " 0.43197881351214384,\n",
       " 0.43191309483859996,\n",
       " 0.43184756687294551,\n",
       " 0.43178222931451882,\n",
       " 0.43171708186320623,\n",
       " 0.43165212421944138,\n",
       " 0.43158735608420351,\n",
       " 0.43152277715901788,\n",
       " 0.4314583871459533,\n",
       " 0.43139418574762234,\n",
       " 0.43133017266718005,\n",
       " 0.4312663476083225,\n",
       " 0.43120271027528695,\n",
       " 0.4311392603728495,\n",
       " 0.43107599760632587,\n",
       " 0.43101292168156863,\n",
       " 0.43095003230496787,\n",
       " 0.43088732918344935,\n",
       " 0.43082481202447365,\n",
       " 0.43076248053603561,\n",
       " 0.43070033442666333,\n",
       " 0.43063837340541694,\n",
       " 0.43057659718188818,\n",
       " 0.43051500546619881,\n",
       " 0.43045359796900018,\n",
       " 0.43039237440147238,\n",
       " 0.43033133447532307,\n",
       " 0.43027047790278689,\n",
       " 0.43020980439662365,\n",
       " 0.43014931367011888,\n",
       " 0.43008900543708189,\n",
       " 0.43002887941184492,\n",
       " 0.4299689353092625,\n",
       " 0.42990917284471047,\n",
       " 0.42984959173408532,\n",
       " 0.42979019169380306,\n",
       " 0.42973097244079772,\n",
       " 0.4296719336925221,\n",
       " 0.42961307516694464,\n",
       " 0.42955439658255073,\n",
       " 0.42949589765834018,\n",
       " 0.42943757811382738,\n",
       " 0.42937943766903941,\n",
       " 0.42932147604451654,\n",
       " 0.42926369296130984,\n",
       " 0.42920608814098105,\n",
       " 0.42914866130560225,\n",
       " 0.42909141217775343,\n",
       " 0.42903434048052341,\n",
       " 0.42897744593750725,\n",
       " 0.42892072827280719,\n",
       " 0.42886418721102981,\n",
       " 0.42880782247728677,\n",
       " 0.42875163379719289,\n",
       " 0.42869562089686619,\n",
       " 0.42863978350292581,\n",
       " 0.42858412134249263,\n",
       " 0.42852863414318682,\n",
       " 0.4284733216331284,\n",
       " 0.42841818354093519,\n",
       " 0.42836321959572304,\n",
       " 0.42830842952710407,\n",
       " 0.42825381306518612,\n",
       " 0.42819936994057245,\n",
       " 0.42814509988435934,\n",
       " 0.42809100262813693,\n",
       " 0.42803707790398804,\n",
       " 0.42798332544448603,\n",
       " 0.42792974498269515,\n",
       " 0.42787633625217003,\n",
       " 0.42782309898695331,\n",
       " 0.42777003292157617,\n",
       " 0.42771713779105702,\n",
       " 0.42766441333090027,\n",
       " 0.42761185927709611,\n",
       " 0.42755947536611916,\n",
       " 0.42750726133492817,\n",
       " 0.42745521692096466,\n",
       " 0.42740334186215218,\n",
       " 0.42735163589689562,\n",
       " 0.42730009876408021,\n",
       " 0.42724873020307141,\n",
       " 0.42719752995371241,\n",
       " 0.42714649775632507,\n",
       " 0.42709563335170819,\n",
       " 0.42704493648113673,\n",
       " 0.42699440688636109,\n",
       " 0.42694404430960642,\n",
       " 0.42689384849357159,\n",
       " 0.42684381918142794,\n",
       " 0.42679395611682003,\n",
       " 0.42674425904386243,\n",
       " 0.4266947277071409,\n",
       " 0.42664536185171098,\n",
       " 0.42659616122309663,\n",
       " 0.42654712556728991,\n",
       " 0.42649825463075003,\n",
       " 0.42644954816040292,\n",
       " 0.42640100590363972,\n",
       " 0.42635262760831621,\n",
       " 0.4263044130227524,\n",
       " 0.42625636189573124,\n",
       " 0.42620847397649819,\n",
       " 0.42616074901475975,\n",
       " 0.42611318676068344,\n",
       " 0.42606578696489694,\n",
       " 0.42601854937848621,\n",
       " 0.42597147375299627,\n",
       " 0.42592455984042887,\n",
       " 0.42587780739324338,\n",
       " 0.42583121616435382,\n",
       " 0.42578478590713026,\n",
       " 0.42573851637539667,\n",
       " 0.42569240732343061,\n",
       " 0.42564645850596178,\n",
       " 0.42560066967817256,\n",
       " 0.42555504059569571,\n",
       " 0.42550957101461456,\n",
       " 0.42546426069146231,\n",
       " 0.42541910938322031,\n",
       " 0.4253741168473178,\n",
       " 0.42532928284163179,\n",
       " 0.42528460712448501,\n",
       " 0.42524008945464614,\n",
       " 0.42519572959132812,\n",
       " 0.42515152729418848,\n",
       " 0.42510748232332779,\n",
       " 0.42506359443928882,\n",
       " 0.4250198634030562,\n",
       " 0.42497628897605577,\n",
       " 0.42493287092015264,\n",
       " 0.42488960899765194,\n",
       " 0.424846502971297,\n",
       " 0.42480355260426955,\n",
       " 0.42476075766018712,\n",
       " 0.42471811790310487,\n",
       " 0.42467563309751255,\n",
       " 0.42463330300833502,\n",
       " 0.42459112740093069,\n",
       " 0.42454910604109158,\n",
       " 0.42450723869504198,\n",
       " 0.42446552512943775,\n",
       " 0.42442396511136543,\n",
       " 0.42438255840834249,\n",
       " 0.42434130478831461,\n",
       " 0.42430020401965735,\n",
       " 0.42425925587117314,\n",
       " 0.4242184601120918,\n",
       " 0.42417781651206976,\n",
       " 0.42413732484118905,\n",
       " 0.42409698486995606,\n",
       " 0.42405679636930155,\n",
       " 0.42401675911057995,\n",
       " 0.42397687286556773,\n",
       " 0.42393713740646372,\n",
       " 0.42389755250588756,\n",
       " 0.42385811793687939,\n",
       " 0.42381883347289889,\n",
       " 0.42377969888782441,\n",
       " 0.42374071395595314,\n",
       " 0.42370187845199891,\n",
       " 0.42366319215109266,\n",
       " 0.42362465482878103,\n",
       " 0.42358626626102608,\n",
       " 0.42354802622420407,\n",
       " 0.42350993449510493,\n",
       " 0.42347199085093229,\n",
       " 0.42343419506930119,\n",
       " 0.42339654692823853,\n",
       " 0.42335904620618203,\n",
       " 0.42332169268197956,\n",
       " 0.42328448613488828,\n",
       " 0.4232474263445738,\n",
       " 0.42321051309110974,\n",
       " 0.42317374615497716,\n",
       " 0.42313712531706343,\n",
       " 0.42310065035866118,\n",
       " 0.4230643210614688,\n",
       " 0.42302813720758858,\n",
       " 0.42299209857952647,\n",
       " 0.42295620496019148,\n",
       " 0.4229204561328942,\n",
       " 0.42288485188134717,\n",
       " 0.42284939198966365,\n",
       " 0.42281407624235695,\n",
       " 0.42277890442433935,\n",
       " 0.42274387632092181,\n",
       " 0.42270899171781362,\n",
       " 0.42267425040112061,\n",
       " 0.42263965215734578,\n",
       " 0.42260519677338709,\n",
       " 0.42257088403653853,\n",
       " 0.42253671373448765,\n",
       " 0.42250268565531612,\n",
       " 0.42246879958749844,\n",
       " 0.42243505531990133,\n",
       " 0.42240145264178353,\n",
       " 0.42236799134279379,\n",
       " 0.42233467121297191,\n",
       " 0.42230149204274681,\n",
       " 0.42226845362293597,\n",
       " 0.42223555574474564,\n",
       " 0.42220279819976891,\n",
       " 0.42217018077998597,\n",
       " 0.42213770327776257,\n",
       " 0.42210536548585043,\n",
       " 0.42207316719738569,\n",
       " 0.42204110820588819,\n",
       " 0.42200918830526163,\n",
       " 0.42197740728979155,\n",
       " 0.42194576495414654,\n",
       " 0.42191426109337549,\n",
       " 0.42188289550290808,\n",
       " 0.4218516679785545,\n",
       " 0.42182057831650321,\n",
       " 0.42178962631332201,\n",
       " 0.42175881176595614,\n",
       " 0.42172813447172841,\n",
       " 0.4216975942283378,\n",
       " 0.4216671908338594,\n",
       " 0.42163692408674297,\n",
       " 0.42160679378581389,\n",
       " 0.42157679973027051,\n",
       " 0.42154694171968426,\n",
       " 0.42151721955399996,\n",
       " 0.42148763303353332,\n",
       " 0.421458181958972,\n",
       " 0.4214288661313737,\n",
       " 0.42139968535216621,\n",
       " 0.42137063942314662,\n",
       " 0.4213417281464808,\n",
       " 0.42131295132470142,\n",
       " 0.42128430876070971,\n",
       " 0.42125580025777287,\n",
       " 0.42122742561952409,\n",
       " 0.4211991846499617,\n",
       " 0.4211710771534492,\n",
       " 0.42114310293471341,\n",
       " 0.42111526179884473,\n",
       " 0.42108755355129629,\n",
       " 0.42105997799788347,\n",
       " 0.42103253494478249,\n",
       " 0.42100522419853087,\n",
       " 0.42097804556602564,\n",
       " 0.42095099885452386,\n",
       " 0.42092408387164099,\n",
       " 0.4208973004253509,\n",
       " 0.42087064832398513,\n",
       " 0.42084412737623123,\n",
       " 0.42081773739113415,\n",
       " 0.42079147817809387,\n",
       " 0.42076534954686534,\n",
       " 0.42073935130755802,\n",
       " 0.42071348327063474,\n",
       " 0.42068774524691188,\n",
       " 0.42066213704755812,\n",
       " 0.42063665848409354,\n",
       " 0.42061130936839008,\n",
       " 0.42058608951266963,\n",
       " 0.42056099872950442,\n",
       " 0.42053603683181556,\n",
       " 0.42051120363287353,\n",
       " 0.42048649894629586,\n",
       " 0.42046192258604842,\n",
       " 0.42043747436644363,\n",
       " 0.42041315410213986,\n",
       " 0.42038896160814115,\n",
       " 0.42036489669979643,\n",
       " 0.42034095919279924,\n",
       " 0.42031714890318655,\n",
       " 0.42029346564733849,\n",
       " 0.42026990924197771,\n",
       " 0.42024647950416855,\n",
       " 0.420223176251317,\n",
       " 0.42019999930116936,\n",
       " 0.42017694847181197,\n",
       " 0.42015402358167064,\n",
       " 0.42013122444951007,\n",
       " 0.42010855089443322,\n",
       " 0.42008600273588037,\n",
       " 0.42006357979362879,\n",
       " 0.42004128188779277,\n",
       " 0.42001910883882149,\n",
       " 0.41999706046749996,\n",
       " 0.41997513659494745,\n",
       " 0.41995333704261723,\n",
       " 0.41993166163229623,\n",
       " 0.41991011018610386,\n",
       " 0.41988868252649147,\n",
       " 0.41986737847624289,\n",
       " 0.41984619785847216,\n",
       " 0.41982514049662384,\n",
       " 0.41980420621447245,\n",
       " 0.41978339483612165,\n",
       " 0.4197627061860037,\n",
       " 0.41974214008887895,\n",
       " 0.41972169636983525,\n",
       " 0.41970137485428716,\n",
       " 0.41968117536797545,\n",
       " 0.41966109773696675,\n",
       " 0.41964114178765272,\n",
       " 0.41962130734674952,\n",
       " 0.41960159424129734,\n",
       " 0.4195820022986595,\n",
       " 0.41956253134652199,\n",
       " 0.41954318121289347,\n",
       " 0.41952395172610391,\n",
       " 0.41950484271480382,\n",
       " 0.41948585400796523,\n",
       " 0.41946698543487931,\n",
       " 0.41944823682515642,\n",
       " 0.41942960800872614,\n",
       " 0.41941109881583577,\n",
       " 0.41939270907705023,\n",
       " 0.41937443862325186,\n",
       " 0.41935628728563895,\n",
       " 0.41933825489572596,\n",
       " 0.41932034128534229,\n",
       " 0.4193025462866326,\n",
       " 0.41928486973205509,\n",
       " 0.41926731145438195,\n",
       " 0.41924987128669822,\n",
       " 0.41923254906240193,\n",
       " 0.41921534461520205,\n",
       " 0.41919825777911962,\n",
       " 0.41918128838848645,\n",
       " 0.41916443627794397,\n",
       " 0.41914770128244372,\n",
       " 0.41913108323724685,\n",
       " 0.4191145819779219,\n",
       " 0.41909819734034609,\n",
       " 0.4190819291607043,\n",
       " 0.41906577727548777,\n",
       " 0.41904974152149416,\n",
       " 0.41903382173582704,\n",
       " 0.41901801775589542,\n",
       " 0.41900232941941257,\n",
       " 0.41898675656439605,\n",
       " 0.41897129902916685,\n",
       " 0.41895595665234953,\n",
       " 0.41894072927287029,\n",
       " 0.41892561672995804,\n",
       " 0.41891061886314257,\n",
       " 0.41889573551225462,\n",
       " 0.41888096651742579,\n",
       " 0.41886631171908639,\n",
       " 0.41885177095796666,\n",
       " 0.41883734407509571,\n",
       " 0.41882303091179995,\n",
       " 0.41880883130970453,\n",
       " 0.41879474511073028,\n",
       " 0.41878077215709619,\n",
       " 0.41876691229131574,\n",
       " 0.41875316535619861,\n",
       " 0.41873953119484963,\n",
       " 0.41872600965066736,\n",
       " 0.41871260056734483,\n",
       " 0.41869930378886799,\n",
       " 0.41868611915951554,\n",
       " 0.418673046523859,\n",
       " 0.41866008572676117,\n",
       " 0.41864723661337605,\n",
       " 0.41863449902914873,\n",
       " 0.41862187281981389,\n",
       " 0.41860935783139647,\n",
       " 0.41859695391020985,\n",
       " 0.41858466090285668,\n",
       " 0.41857247865622732,\n",
       " 0.41856040701749975,\n",
       " 0.41854844583413864,\n",
       " 0.41853659495389617,\n",
       " 0.4185248542248094,\n",
       " 0.4185132234952012,\n",
       " 0.41850170261367997,\n",
       " 0.41849029142913791,\n",
       " 0.41847898979075121,\n",
       " 0.4184677975479798,\n",
       " 0.41845671455056632,\n",
       " 0.41844574064853568,\n",
       " 0.41843487569219501,\n",
       " 0.41842411953213265,\n",
       " 0.41841347201921752,\n",
       " 0.41840293300459958,\n",
       " 0.41839250233970782,\n",
       " 0.41838217987625109,\n",
       " 0.41837196546621702,\n",
       " 0.41836185896187139,\n",
       " 0.4183518602157581,\n",
       " 0.4183419690806981,\n",
       " 0.41833218540978928,\n",
       " 0.41832250905640572,\n",
       " 0.4183129398741976,\n",
       " 0.41830347771709003,\n",
       " 0.41829412243928366,\n",
       " 0.41828487389525243,\n",
       " 0.41827573193974488,\n",
       " 0.41826669642778286,\n",
       " 0.41825776721466035,\n",
       " 0.41824894415594449,\n",
       " 0.41824022710747422,\n",
       " 0.41823161592535901,\n",
       " 0.41822311046598021,\n",
       " 0.41821471058598897,\n",
       " 0.41820641614230614,\n",
       " 0.41819822699212267,\n",
       " 0.41819014299289797,\n",
       " 0.41818216400235964,\n",
       " 0.41817428987850397,\n",
       " 0.4181665204795938,\n",
       " 0.41815885566415945,\n",
       " 0.4181512952909977,\n",
       " 0.41814383921917109,\n",
       " 0.41813648730800812,\n",
       " 0.41812923941710178,\n",
       " 0.41812209540630962,\n",
       " 0.41811505513575409,\n",
       " 0.4181081184658203,\n",
       " 0.41810128525715679,\n",
       " 0.41809455537067475,\n",
       " 0.41808792866754774,\n",
       " 0.41808140500921065,\n",
       " 0.41807498425735962,\n",
       " 0.41806866627395156,\n",
       " 0.41806245092120425,\n",
       " 0.41805633806159409,\n",
       " 0.41805032755785787,\n",
       " 0.41804441927299058,\n",
       " 0.41803861307024581,\n",
       " 0.41803290881313482,\n",
       " 0.41802730636542712,\n",
       " 0.41802180559114793,\n",
       " 0.41801640635458009,\n",
       " 0.41801110852026185,\n",
       " 0.41800591195298736,\n",
       " 0.41800081651780552,\n",
       " 0.4179958220800204,\n",
       " 0.4179909285051896,\n",
       " 0.41798613565912496,\n",
       " 0.41798144340789162,\n",
       " 0.41797685161780707,\n",
       " 0.41797236015544115,\n",
       " 0.41796796888761639,\n",
       " 0.4179636776814058,\n",
       " 0.41795948640413416,\n",
       " 0.41795539492337574,\n",
       " 0.41795140310695589,\n",
       " 0.41794751082294895,\n",
       " 0.41794371793967872,\n",
       " 0.41794002432571759,\n",
       " 0.41793642984988622,\n",
       " 0.41793293438125267,\n",
       " 0.41792953778913328,\n",
       " 0.41792623994309036,\n",
       " 0.41792304071293301,\n",
       " 0.41791993996871635,\n",
       " 0.41791693758074111,\n",
       " 0.41791403341955313,\n",
       " 0.41791122735594249,\n",
       " 0.41790851926094408,\n",
       " 0.41790590900583624,\n",
       " 0.41790339646214059,\n",
       " 0.41790098150162192,\n",
       " 0.41789866399628683,\n",
       " 0.41789644381838492,\n",
       " 0.41789432084040623,\n",
       " 0.41789229493508312,\n",
       " 0.41789036597538726,\n",
       " 0.41788853383453184,\n",
       " 0.41788679838596882,\n",
       " 0.41788515950339056,\n",
       " 0.41788361706072724,\n",
       " 0.41788217093214841,\n",
       " 0.41788082099206103,\n",
       " 0.41787956711511076,\n",
       " 0.41787840917617897,\n",
       " 0.41787734705038476,\n",
       " 0.41787638061308341,\n",
       " 0.41787550973986615,\n",
       " 0.4178747343065593,\n",
       " 0.4178740541892248,\n",
       " 0.41787346926415869,\n",
       " 0.4178729794078917,\n",
       " 0.41787258449718784,\n",
       " 0.41787228440904445,\n",
       " 0.41787207902069246,\n",
       " ...]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "build_poly() missing 1 required positional argument: 'degree'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-4413c84700c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_loss_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: build_poly() missing 1 required positional argument: 'degree'"
     ]
    }
   ],
   "source": [
    "\n",
    "test_loss=compute_loss_poly(_,build_poly(tX_test,degree),w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newton method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., -1., -1., ...,  1., -1., -1.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../test.csv' # TODO: download test data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
